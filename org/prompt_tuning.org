#+TITLE: 大模型如何适配下游任务
https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora
* 背景
** 主要内容
** 如何让大模型适配下游的任务
*** 大模型做全量微调
- 全部的参数做fine-tuning
- 缺点：
  + 训练代价大：全量模型load
  + 每个子任务需要一个独立的模型副本
*** 冻结大模型，微调delta参数
- 做法：新增加少量微调参数，训练中冻结大模型的全部参数，
- 效果：
  1. 已经超越了fine-tuning
  2. 同时解决了模型对discrete prompt的稳定性问题
- 同时，概念：incontext learning
- 例子：
  + 添加adapters： lora
  + soft prompt
    + Prompt Tuning:
       The Power of Scale for Parameter-Efficient Prompt Tuning, 2021
    + P-tuning:
        GPT Understands, Too, 2023
    + prefix tuning:
       Prefix-tuning: Optimizing continuous prompts for generation.
     
** 大模型的prompt
*** 常见的prompt类型
- few-shot prompts
- zero-shot prompts
*** in-context learning
*** discrete prompt 缺点
- 不稳定
- token数量变大，inference的代价
* Prompt Tuning:
The Power of Scale for Parameter-Efficient Prompt Tuning, 2021
** motivation

** how
- 添加k个虚拟的token
- virtual tokens + real tokens
- 新增加的参数：virtual token embeddings
** prompt Tuning的效果
- scaling的效果，效果超过fine-tuning
** code
#+begin_src python
import torch
from torch import nn
from transformers import GPT2Model, GPT2Tokenizer

class PromptTuningModel(nn.Module):
    def __init__(self, pretrained_model_name, prompt_length):
        super().__init__()
        self.transformer = GPT2Model.from_pretrained(pretrained_model_name)
        self.prompt_length = prompt_length
        # 初始化提示向量，这些是我们将要学习的唯一参数
        self.prompt_embeddings = nn.Parameter(torch.randn(prompt_length, self.transformer.config.n_embd))
        # 冻结预训练模型的所有参数
        for param in self.transformer.parameters():
            param.requires_grad = False

    def forward(self, input_ids):
        batch_size = input_ids.shape[0]
        # 扩展提示向量以匹配输入批次大小
        prompt = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        # 获取模型的原始输入嵌入
        inputs_embeds = self.transformer.get_input_embeddings()(input_ids)
        # 将提示向量前置于输入嵌入向量之前
        full_input_embeds = torch.cat((prompt, inputs_embeds), dim=1)
        # 通过模型传递组合的嵌入向量
        outputs = self.transformer(inputs_embeds=full_input_embeds)
        return outputs

# 示例：初始化模型和分词器
pretrained_model_name = 'gpt2-medium'
tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name)
model = PromptTuningModel(pretrained_model_name, prompt_length=20)

# 示例输入
input_text = "The quick brown fox jumps over the lazy dog"
input_ids = tokenizer(input_text, return_tensors='pt').input_ids

# 通过模型前向传递
output = model(input_ids)
#+end_src

* P-tuning
** title: GPT Understands, Too
加入了lstm prefix encoder
** title: 
Prefix-Tuning: Optimizing Continuous Prompts for Generation
* P-tuning v1
** title：The Power of Scale for Parameter-Efficient Prompt Tuning
* 几种tuning的方法
