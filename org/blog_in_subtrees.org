#+hugo_base_dir: ../
#+HUGO_SECTION: ./
#+HUGO_CUSTOM_FRONT_MATTER: :toc true
#+hugo_weight: auto
#+hugo_auto_set_lastmod: t
#+options: toc:t
#+author: 连义江

* 大模型
关于大模型的文章都在这里
** DONE transformer入门                                             :hugo:org:
:PROPERTIES:
:EXPORT_FILE_NAME: about-transformer
:EXPORT_DATE: 2024-02-01
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_TAGS: transformer org
:EXPORT_HUGO_CATEGORIES: llm
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :foo bar :baz zoo :alpha 1 :beta "two words" :gamma 10
:END:
*** transformer背景
**** 主要内容
- 参考
  + 2017. (引用 106576)
     Attention Is All You Need
  + 2020. (引用 975)
     Fast Autoregressive Transformers with Linear Attention
  + [[https://github.com/karpathy/minGPT/tree/master/mingpt][mingpt by karpathy]]
- 主要内容
  + transformer 的设计推演
  + transformer 的代码讲解
  + transformer的参数和运算量
  + linear attention
**** 矩阵知识
***** why
- 原文直接从整个矩阵作用出发
   $$\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\
   \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i)
   \end{aligned}$$
- 从行向量的角度更容易理解
***** 矩阵和行向量
- 矩阵
   $X\in R^{N\times F}$ 
   $X=\begin{pmatrix}
     X_{11}, X_{12},\ldots, X_{1F} \\
     X_{21}, X_{22},\ldots, X_{2F} \\
     \vdots\\
     X_{N1}, X_{N2},\ldots, X_{NF} 
     \end{pmatrix}$
- 行向量
   $X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}$
- 分块矩阵
   $X=\begin{pmatrix}
   X_1\\
   X_2\\
   \vdots\\
   X_N
   \end{pmatrix}$
****** 例子
- $N$ 个token，$F$ 是embedding的维度
- 每行对应于一个token的embedding 行向量
$tokens=\begin{pmatrix}
   hello \\
   world \\
   pad \\
   pad \\
   pad 
   \end{pmatrix} 
   \rightarrow X=\begin{pmatrix}
   [0.59, 0.20, 0.04, 0.96] \\
   [0.96, 0.30, 0.16, 0.63] \\
   [0.02, 0.19, 0.34, 0.25] \\
   [0.02, 0.19, 0.34, 0.25] \\
   [0.02, 0.19, 0.34, 0.25] 
   \end{pmatrix}$

***** 矩阵相乘和算子作用
- 定义线性算子 $\mathcal{A}$
  + 可以作用到行向量  $\mathcal{A}(X_i) = X_{i} A$
  + 也可以作用到矩阵  $\mathcal{A}(X) = XA$
- 右乘矩阵等于对每个行向量逐个施加行变换
  $XA=\begin{pmatrix}
  X_1\\
  X_2\\
  \vdots\\
  X_N
  \end{pmatrix}A=
  \begin{pmatrix}
  X_1 A\\
  X_2 A\\
  \vdots\\
  X_N A
  \end{pmatrix}=
  \begin{pmatrix}
  \mathcal{A}(X_1) \\
  \mathcal{A}(X_2) \\
  \vdots\\
  \mathcal{A}(X_N) 
  \end{pmatrix}=\mathcal{A}(X)$
- 代码对应于 nn.Linear
#+begin_src python -n :results output
  import torch
  import torch.nn as nn
  F = 6
  linear = nn.Linear(in_features=F, out_features=F)
  X_i = torch.rand(1, 6)
  X = torch.rand(3, 6)
  print(linear(X_i).shape)
  print(linear(X).shape)
#+end_src   
- 算子是对矩阵乘法的一种物理理解
  + 旋转矩阵
     
     $R(\theta)=\begin{pmatrix}
        cos\theta& sin\theta\\
        -sin\theta& cos\theta
        \end{pmatrix}$
  + 缩放变换
     
     $R(\lambda_1,\lambda_2)=\begin{pmatrix} \lambda_1 & \\
        & \lambda_2  \end{pmatrix}$


***** transformer中的$QK^{T}V$

- $S=QK^T$ = 行向量两两计算点积相似性
   $\begin{pmatrix}
   Q_{1}\\
   Q_{2}\\
   \vdots\\
   Q_N
   \end{pmatrix}
   \begin{pmatrix}
   K_{1}^T, K_2^T,\ldots,K_N^T
   \end{pmatrix}=(Q_{i}K_j^T)_{ij}=S$

- $SV$ = 对行向量做加权求和
   $\begin{pmatrix}
   S_{11},S_{12},\ldots, S_{1N}\\
   S_{21},S_{22},\ldots, S_{2N}\\
   \vdots\\
   S_{N1},S_{N2},\ldots, S_{NN}\\
   \end{pmatrix}
   \begin{pmatrix}
   Q_{1}\\
   Q_{2}\\
   \vdots\\
   Q_N
   \end{pmatrix}
   =(Q_{i}K_j^T)_{ij}=S$

***** 代码
- pytorch/tensorflow中的代码都是按照作用于行向量来组织的
- nn.Linear 作用于行向量
- nn.Embedding 按照行向量来组织数据
#+begin_src python -n :results output
  import torch
  import torch.nn as nn
  N = 3
  F = 8
  embed = nn.Embedding(30, F)
  idx = torch.tensor([1,2,3])
  X = embed(idx)
  print(X.shape)
#+end_src

***** 注
- 左乘以一个矩阵相当于对每个列向量来施加变化
- 论文：一般会有行/列向量两种表示方式
- 代码：基本都是行向量来作为数据组织的标准
- 本文:
  + 向量都按照行向量的形式来组织
  + 按照作用于单个行向量的方式来讲解transformer
**** encoder-decoder
- 大部分的s2s 的任务建模为 encoder-decoder的结构
  + 机器翻译，语音识别，文本摘要，问答系统等
- encoder
  + 把token序列$(x_{1}, x_2,\ldots, x_N)$ 转化为语义向量序列 $(Y_{1}, Y_2, \ldots, Y_N)$
  + 一般组织为多层的网络的形式
    + 第一层：基础语义向量序列
        $(x_{1}, x_2,\ldots, x_N)\rightarrow (X_{1}, X_2,\ldots, X_N)$
    + 其它层：高阶语义向量序列
        $(X_{1}, X_2,\ldots, X_N)\rightarrow (Y_{1}, Y_2,\ldots, Y_N)$
- decoder
   基于$(Y_{1}, Y_2, \ldots, Y_N)$ 自回归式的逐个token解码

focus到 encoder部分来理解transformer
**** 低阶到高阶语义向量的转换
寻找算子 $\mathcal{T}$ 将低阶的语义向量序列变换为高阶的语义向量序列
  $\mathcal{T}\begin{pmatrix}
   X_1\\
   X_2\\
   \vdots\\
   X_N
   \end{pmatrix}
   \rightarrow\begin{pmatrix}
   Y_1\\
   Y_2\\
   \vdots\\
   Y_N
   \end{pmatrix}$
- 输入: $X$ 低阶语义向量序列，输出: $Y$ 高阶语义向量序列
- 意义
  + $Y_{i}=f(X_{1}, X_2, \ldots, X_{N})$
  + 对低阶语义向量做加工组合处理和抽象，变换为一个高阶的语义向量序列
  + 高阶语义向量考虑了 /上下文/ 的语义向量表达
- motivation
  + 1957. Firth
     #+begin_quote
        a word is characterized by the company it keeps.
     #+end_quote
     例子：
     #+begin_quote
        The *enigmatic* smile on Mona Lisa's face has intrigued art enthusiasts for centuries, leaving them to speculate about its true meaning.
     #+end_quote
- 用矩阵变换表达 $Y=\mathcal{T}(X)$
  + $X \in R^{N\times F}$, $Y=\mathcal{T}(X): \quad R^{N\times F}\rightarrow R^{N\times F}$
  + 这个算子天然可以复合嵌套，形成多层的网络结构
     $Y=\mathcal{T}_{L}\circ \mathcal{T}_{L-1}\circ \ldots \circ \mathcal{T}_{1}(X)$

**** 核心的问题
****** 问题
如何设计 $Y_{i}=f(X_{1}, X_2, \ldots, X_{N})$
- $Y_{1}, \ldots, Y_N$ 能否并行得到
- $Y_{i}$ 能否高效的建立起远程的依赖
****** RNN
#+DOWNLOADED: screenshot @ 2024-01-18 14:03:26
#+ATTR_HTML: :width 600px :align middle
[[file:/images/2024-01-18_14-03-26_screenshot.png]]

- 递归语义序列 $Y_{0}\rightarrow Y_1 \rightarrow \ldots \rightarrow Y_{N}$
- $Y_{i}=tanh(X_{i}W + Y_{i-1}U)$
- 串行
- 单方向的依赖关系，间接
****** CNN
#+DOWNLOADED: screenshot @ 2024-01-18 14:04:23
#+ATTR_HTML: :width 600px :align middle
[[file:/images/2024-01-18_14-04-23_screenshot.png]]

- $Y_{i}=(X_{i-1},X_i, X_{i+1}) W$ 假设窗口宽度是3
- 并行
- 长距离依赖？
   + 一层卷积只能依赖于当前窗口内，不能对窗口外的形成依赖。
****** transformer思路
设计$Y_{i}=f(X_{1}, X_2, \ldots, X_{N})$，使得
- 使得 $Y_{1},\ldots, Y_N$ 可以做并行计算
- 同时解决长距离依赖的问题
#+DOWNLOADED: screenshot @ 2024-01-18 14:13:40
#+ATTR_HTML: :width 400px :align middle
[[file:images/2024-01-18_14-13-40_screenshot.png]]


$Y=\mathcal{F}\circ \mathcal{A}(X)$ 做两次矩阵的变换
- $Y=\mathcal{A}(X)$    MultiHead Attention
  + 高阶的语义等于对 /全部/ 的低阶语义向量基于 /相似性(Attention)/ 做 /加权平均/
  + $$\begin{aligned}\mathcal{A}(X_i) &=  \frac{\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\sum_{j=1}^N sim(X_i,X_j)} \end{aligned}$$
  + attention = 相似性
    
- $Y'=\mathcal{F}(Y)$  Position-wise Feedforward
  + 再施加若干非线性变换

* Footnotes
* COMMENT Local Variables                                           :ARCHIVE:
# Local Variables:
# org-hugo-footer: "\n\n[//]: # \"Exported with love from a post written in Org mode\"\n[//]: # \"- https://github.com/kaushalmodi/ox-hugo\""
# End:
