#+TITLE: 图像生成之VAE
* 背景
** 主要内容
- 是理解diffusion model的一个基础
- 应用场景，活跃在图像生成的各个领域
  - dall-e2
  - imagen
  - sora
- 数学基础回顾
- 参考：
  + https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models
  + https://medium.com/@kemalpiro/step-by-step-visual-introduction-to-diffusion-models-235942d2f15c
** 关于概率密度的估计
*** 机器学习中常见的任务
- 分类
- 回归
*** 我们之前熟悉的概率建模
- 基于特征来推断目标事件发生的概率。P(label|特征）
- ctr 里面：基于用户和item的特征信息来推测用户点击item的概率

   $P(y=1|x)=\sigma(f_{\theta}(x))$

- NLP 中的相关性模型：基于用户输入的query和doc特征来推测两者相关的概率
   $P(y=1|(q,t))=\sigma(f_{\theta}(q,t))$

- 我们很少需要对特征的分布直接建模
  + 用户特征的分布？
  + query,doc 的特征分布？
** 概率密度估计
机器学习中另外一个大的topic是概率密度的估计：
*** 一种简单的方式：
假设概率分布的pdf，然后做参数估计
  - 假设这个分布是$\mathcal{N}(\mu,\Sigma)$，来推断$\mu, \Sigma$
*** 另一种
基于自身$X$ 的序列结构，做条件概率展开
  - language model:
    + token粒度，每个token有离散的值
    + $P(x_{1}, x_{2},\ldots, x_{n})= P(x_{1})P(x_{2}|x_{1})\ldots P(x_{n}|x_{1,\ldots,n-1})$
    + autoregressive
    + decoder only GPT model
  - image model?
    + pixelRNN
    + 效率的问题: 1920*1080=207,3600
*** 用途
- 建模出来一个概率密度函数pdf $P_{\theta}(x)$ 
  + 知道什么样的x概率低，知道什么样的x概率高
- 基于概率采样，我们可以生成更多的类似的，不在样本集合中的 X
- 图片：
   比如登录的图片认证，生成若干旋转的文字，角度等等
- conditional 的时候会产生大的价值
  + 文本：LLM GPT做的就是概率密度估计的事情，可以逐个token来生成
  + 图片：更进一步，做可以控制的图像生成，如果加一个conditional，P(x) P(x|c), 可以做文本到图像的生成
** 图像生成的两种思路
*** 自回归的方式
**** 像素级别的自回归
- PixelRNN/PixelCNN
**** patch级别的自回归

** 关于生成模型的直接建模
*** 如果直接建模
- 使用范围局限：
  + 基于这个$P_{\theta}(x)$ 采样
  + 无法做更多的复杂生成的事情，无法对对生成做细粒度操控
  + 比如，
    + 我想加入一段文字的描述来生成
    + 我想生成某种特定类型，特定的风格的图片
*** 直接建模的难点？
- 图像本省的高维特性
- sample 的效率：高维空间中的sample效率会很低
- 你的建模需要能建模出来像素之间的依赖性
- 直接来建模pdf是一个非常困难的事情
** 符号的标记
用大写字母来表示随机变量 $X$ 
* Generative Model
** 换个思路建模密度
*** 先验分布+确定性函数来建模
$P(X)=P(Z)P(X|Z)$
- prior z:
  + 没有需要学习的参数
  + 在一个低维度的latent space中采样
- 确定性函数：
  + h(z)

** 理论的依据
*** 高斯分布+CDF逆变换可以拟合任意的分布
- 假设
  + 随机变量 $N\sim \mathcal{N}[0,1]$, 对应的CDF 是$\Psi$
  + 那么$Y=\Psi(N)\sim \text{Uniform}[0,1]$
  + 目标随机变量$X$ 对应的分布的CDF是 $F(x)=P(X\le x)$
- 那么随机变量 $X=F^{-1}(Y)$ 分布满足$F$
- 高维中依然是成立，但是这个函数表达是不知道的，我们可以通过模型学习得到。
**** 均匀分布+CDF逆变换可以拟合任意的分布
- 假设
  + 随机变量 $U\sim \text{Uniform}[0,1]$
  + 目标随机变量对应的CDF是 $F(x)=P(X\le x)$
- 结论：随机变量 $X=F^{-1}(U)$ 分布满足$F$
- 证明:

   $P(X\le x)=P(F^{-1}(U)\le x)=P(U\le F(x))=F(x)$
**** 高斯分布到均匀分布
- 假设
  + 随机变量 $N\sim \mathcal{N}[0,1]$, 对应的CDF 是$\Psi$
  + 那么$Y=\Psi(N)\sim \text{Uniform}[0,1]$
- 证明:
   $P(Y\le y)=P(\Psi(N)\le y )=P(N\le \Psi^{-1}(y))=\Psi(\Psi^{-1}(y))=y$
*** 在生成式模型中运用：
sample $X$ 可以分两步走
- 先sample $Z\sim \mathcal{N}(0,1)$
- 然后再基于一个复杂的确定函数变换（交给DNN学习）得到 $f(Z)$ 变换得到$X$
- 随机变量 $X=f(Z)$ 就是对整体的sample建模
*** 为什么不用均匀分布做先验？而使用高斯？
- 高斯分布在整个空间上有定义，计算KL 不会有除以0的问题发生
- 高斯分布有很多很好的性质可以使用
** Generative Model
*** 一般的模型的概率建模方式
- 参数化单个样本$X$的概率 $P_{\theta}(X)$，
- 得到若干的样本$X_1, X_2, \ldots, X_N$
- 然后做概率的最大似然估计 $\prod_{i} P_{\theta}(X_i)$
*** 生成式模型的概率建模方式
**** latent variable
- 我们看到的世界可能是高维空间到低维子空间的一个投影
- 每个样本$X$ 对应一个latent variable $Z$
- 不可见的
**** 上帝的视角
- 首先采样$Z$ 基于 $P(Z)$
- 然后再采样$X$ 基于 $P(X|Z)$
**** 建模
**** 一些假设
- $P(X|Z)$ 是参数化的
- $P(Z)$ 是无参数的

** 生成模型的优化
*** 优化的目标：
$P(X)=\int_Z P(X|Z)P(Z) dZ$
*** 积分的存在，导致无法直接优化
*** 需要使用蒙特卡洛方法
$P(X)= E\limits_{Z\sim P(Z)} P(X|Z)$
- sample $Z_1, Z_2, \ldots, Z_n$
- $P(X) \approx \dfrac{1}{n} \sum_{i} P(X|Z_i)$
- 再做梯度下降
- 问题：
  + 维度灾难的问题： n需要极其大来得到一个准确的概率估计
  + sample的过程中 $P(X|Z_i)的概率大多为0，对参数的更新没有贡献
    + 所以我们要更加有效的 Z, 比如使用  $Q(Z|X)$ 来sample Z
    + $Q(Z|X)$ 能给出来容易生成$X$ 的Z 来。
    + 计算 $E_{Z\sim Q} P(X|Z)$
* 数学基础回顾
** 概率论的公式回顾
*** 先验
*** 后验
*** 似然性
*** 联合概率
** 蒙特卡洛方法和维度灾难
*** 定义
*** 一个简单的例子：计算pi
*** 大数定律：均值可以
换句话说，随着样本大小 n 无限增加，样本均值几乎肯定会等于总体均值 μ。
*** 缺点：在高维空间中效率非常的低
*** 本质上：在高维空间里面，你的采样到的大部分的点都不是你想要的
** 高斯分布的性质
*** 定义
*** 性质
- 高斯随机变量线性变换之后还是高斯分布
- 先验gauss，似然高斯，后验依然是高斯
- KL divergence
   $D\left[ \mathcal{N}(\mu_0, \Sigma_0) \parallel \mathcal{N}(\mu_1, \Sigma_1) \right] = \frac{1}{2} \left( \text{tr} \left( \Sigma_1^{-1}\Sigma_0 \right) + (\mu_1 - \mu_0)^T \Sigma_1^{-1} (\mu_1 - \mu_0) - k + \log \left( \dfrac{\det\Sigma_1}{\det\Sigma_0} \right) \right)$
- KL
   $D\left[ \mathcal{N}(\mu(X), \Sigma(X)) \parallel \mathcal{N}(0, I) \right] = \frac{1}{2} \left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \det (\Sigma(X)) \right)$
*** linear gaussian

** TODO reparameterization trick
问题：假设我们要对$\theta$ 求导
*** 如果pdf中没有$\theta$ 
\[
\mathrm{E}_{p(z)} \left[ f_\theta(z) \right]
\]
where \(p\) is a density. Provided we can differentiate \(f_\theta(z)\), we can easily compute the gradient:
\[
\nabla_\theta \mathrm{E}_{p(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p(z)f_\theta(z)dz
\]
\[
= \int p(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \mathrm{E}_{p(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]
*** 如果pdf中依赖于 $\theta$ 

\[
\nabla_\theta \mathrm{E}_{p_\theta(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p_\theta(z)f_\theta(z)dz
\]
\[
= \int \nabla_\theta \left[ p_\theta(z)f_\theta(z) \right] dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \int p_\theta(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \mathrm{E}_{p_\theta(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]

这时候不再可以写成为期望的形式了，这个时候MC方法就走不下去了。
*** 重参数化
$\epsilon \sim p(\epsilon)$
$z = g_\theta(\epsilon, x)$
$\mathrm{E}_{p_\theta(z)}[f(z^{(i)})] = \mathrm{E}_{p(\epsilon)}[f(g_\theta(\epsilon, x^{(i)}))]$
$\begin{aligned}
\nabla_\theta \mathrm{E}_{p_\theta(z)}[f(z^{(i)})]
&= \nabla_\theta \mathrm{E}_{p(\epsilon)}[f(g_\theta(\epsilon, x^{(i)}))] \\
&= \mathrm{E}_{p(\epsilon)}[\nabla_\theta f(g_\theta(\epsilon, x^{(i)}))] \\
&\approx \frac{1}{L} \sum_{l=1}^L \nabla_\theta f(g_\theta(\epsilon^{(l)}, x^{(i)}))
\end{aligned}$
*** 类比
可以类比于强化学习中的 policy gradient 求导
   $J(\theta)= E_{\tau\sim \pi_{\theta}(\tau)} r(\tau)$
   
   $\begin{aligned}\nabla_{\theta}J(\theta) = & \int \nabla_{\theta}\pi_{\theta}(\tau)r(\tau)d\tau \\
     = & \int \pi_{\theta}(\tau) \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau)d\tau \\
     = & E_{\tau \sim \pi_{\theta}(\tau)}\left[ \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau) \right]
     \end{aligned}$

* VAE
** core of variational autoencoder
对于任意的$Q(Z|X)$， 有
$\log P(X) - D_{KL}[Q(Z|X) \| P(Z|X)] = \mathbb{E}_{Z \sim Q}[\log P(X|Z)] - D_{KL}[Q(Z|X) \| P(Z)]$
*** proof:
$\begin{aligned}
\log P(X) &= E_{Q(Z|X)} \log P(X) \\
&=E_{Q(Z|X)} \log \dfrac{P(X,Z)}{P(Z|X)} \\
&=E_{Q(Z|X)} \log \dfrac{P(X,Z)}{P(Z|X)} \dfrac{Q(Z|X)}{Q(Z|X)} \\
& = E_{Q(Z|X)} \log \dfrac{P(X,Z)}{Q(Z|X)} + E_{Q(Z|X)} \log \dfrac{Q(Z|X)}{P(Z|X)}\\
& = E_{Q(Z|X)} \log \dfrac{P(X,Z)}{Q(Z|X)} + D_{KL}[Q(Z|X)||P(Z|X)]
\end{aligned}$
*** ELBO
由于 KL Divergence 非负，所以
$\begin{aligned}
\log P(X) & \ge  E_{Q(Z|X)} \log \dfrac{P(X,Z)}{Q(Z|X)}
\end{aligned}$
*** 理解
- 变分：$Q_{\phi}(Z|X)$ 参数化
- 左边是优化的目标似然性
- 右边第一项是ELBO
- 右边第一项是KL divergence
- 我们希望不断去优化$Q_{\phi}(Z|X)$ 提升ELBO，
- 当$Q_{\phi}(Z|X)= P(Z|X)$的时候，结束。
*** 继续拆分ELBO
$\begin{aligned}
E_{Q(Z|X)} \log \dfrac{P(X,Z)}{Q(Z|X)} &= E_{Q(Z|X)}\log \dfrac{P(X|Z)P(Z)}{Q(Z|X)} \\
&= E_{Q(Z|X)}\log P(X|Z) + E_{Q(Z|X)}\log \dfrac{P(Z)}{Q(Z|X)} \\
& = E_{Q(Z|X)}\log P(X|Z) - D_{KL}[Q(Z|X) \| P(Z)]
\end{aligned}$
- 第一项:
  + 有了encoding,decoding的意思
  + 可以使用MC的方法来优化
- 第二项:
   + 正则的一个效果
*** 最后
$\log P(X) - D_{KL}[Q(Z|X) \| P(Z|X)] = \mathbb{E}_{Z \sim Q}[\log P(X|Z)] - D_{KL}[Q(Z|X) \| P(Z)]$
** 参数化
- $Q(Z|X)=\mathcal{N}(Z|\mu(X;\theta), \Sigma(X;\theta))$ Gauss
- $P(Z)$ gauss
**** 第二项
两个gauss分布之间的KL散度可以显式的计算出来
** 重参数化=积分的变量替换
- sample
  + sample $Z_{i}$ from $Q_{\phi}(Z|X)$
  + $\dfrac{1}{n}\sum log P_{\theta}(X_i|Z_{i})$
- 问题
  + 采样依赖于参数$\phi$
  + 如何对$\phi$ 求导
     
- $E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)$ 的梯度计算问题
  + $\nabla_{\phi} E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)$
  + 期望依赖的随机变量依赖于参数，无法对参数求导
- 重参数化 = 数学积分中的变量替换
  + $Z\sim Q(Z|X)=\mathcal{N}(Z|\mu(X;\theta), \Sigma(X;\theta))$ 
  + $Z=\mu(X)+\Sigma^{1/2}(X)*\varepsilon$
  + $E_{Q(Z|X)}\log p(X|Z)=E_{\varepsilon}\log p(X|Z(\varepsilon))$
** 建立新的优化目标
我们来计算一个新的东西：$E$
** VAE 图示
* VAE代码实现
** VAE代码实现
** ELBO backbone
*** 背景
$P(x)=\int P(x|z) p(z)$
直接做主要的困难来源于MC在高维空间中的sample的效率问题
*** 引入Q分布
1. 聚焦有意义的 z 值
2. Q(z∣X)：为此，我们引入了一个新的函数 
Q(z∣X)，这是一个编码器网络，它可以基于观察到的数据 
X 提供一个关于 z 值的分布。这个分布专门针对那些可能产生 
X 的 z 值。变分方法使得这个分布可以通过学习数据来逼近真实的后验分布 
P(z∣X)。
*** formula
$\log P(X) - D_{KL}[Q(z|X) \| P(z|X)] = \mathbb{E}_{z \sim Q}[\log P(X|z)] - D_{KL}[Q(z|X) \| P(z)]$

* others
** 建模
*** 建模
- latent variable space $\mathcal{Z}$, pdf $p(z)$
- $z\in\mathcal{Z}$ 是一个随机变量
- deterministic function: $f:\mathcal{Z} \times {\Theta} \rightarrow \mathcal{X}$
- $f(z,\theta)$ 是一个随机变量
  + 希望$f$ 描述了这个数据的分布
- 优化的难点，在于随机变量的一个引入。
*** 建模两步
- sample z from $P(z)$
- sample x from $P(z|x)$
*
