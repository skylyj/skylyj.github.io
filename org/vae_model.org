#+TITLE: 图像生成之VAE
* 背景
** 主要内容
- 是理解diffusion model的一个基础
- 应用场景，活跃在图像生成的各个领域
  - dall-e2
  - imagen
  - sora
- 数学基础回顾
- 参考：
  + https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models
  + https://medium.com/@kemalpiro/step-by-step-visual-introduction-to-diffusion-models-235942d2f15c

** 关于概率密度的估计
*** 机器学习的几个常见的topic
- 分类
- 回归
- 概率密度估计
*** 我们熟悉的概率建模
- 基于特征来推断目标事件发生的概率。P(label|特征）
- ctr 里面：基于用户和item的特征信息来推测用户点击item的概率

   $P(y=1|x)=\sigma(f_{\theta}(x))$

- NLP 中的相关性模型：基于用户输入的query和doc特征来推测两者相关的概率
   $P(y=1|(q,t))=\sigma(f_{\theta}(q,t))$

- 我们很少需要对特征的分布直接建模
  + 用户特征的分布？
  + query,doc 的特征分布？
** 概率密度估计
*** 目标
- 做什么：
  + 对样本$X$ 的概率分布$P(X)$ 做估计
  + 知道什么样的样本出现的概率高，什么样的样本出现的概率低
- $X$ 是什么？原始的输入。
  + 文本：一句话     
    - ✅ X=I have a dog.
    - ❌ X=Dog have a i.
  + 图像: 一个图，灰度的图的话，一个矩阵
     $X=\begin{pmatrix}
     1 & 1 & 3 & 255 \\
     1 & 1 & 3 & 255 \\
     1 & 1 & 3 & 255 \\
     \end{pmatrix}$
*** 概率密度估计有什么用？
- 对图像的likilihood给出一个客观的判断
  + 知道什么样的x概率低，知道什么样的x概率高
- 基于概率采样生成
  + 我们可以生成更多的类似的，不在样本集合中的 X
- 例子：
  + 图片：比如登录的图片认证，生成若干旋转的文字，角度等等
  + 文本：基于LM生成文章
  + conditional 的时候会产生更大的价值
    + 文本：LLM GPT做的就是概率密度估计的事情，可以逐个token来生成
    + 图片：控制的图像生成，P(x|c), 文本到图像的生成
** 不确定性描述
我看到的都是一些实实在在的东西，一个图像怎么用概率来描述？
- 可以认为这个世界上的一些都是不确定的，都是上帝在采样丢色子的结果。
  - 比如相关性性判断，表面上是确定性的，依然可以表达为概率形式
     P(相关|query, 广告)
  - 一张图像，我们可以认为是采样的结果，
    + 一张灰度图像可以看做是一个n*n维度的向量，
    + 按照像素值来组织
    + 整体是一个gauss分布，
    + 简单的可以认为是各向同性的，像素之间相互独立。
** 如何来建模概率密度
*** 直接做参数估计
- 假设概率分布的pdf，然后做参数估计
- 假设这个分布是$\mathcal{N}(\mu,\Sigma)$，来推断$\mu, \Sigma$
*** 基于序列结构做条件概率展开
- language model:
    + $P(x_{1}, x_{2},\ldots, x_{n})= P(x_{1})P(x_{2}|x_{1})\ldots P(x_{n}|x_{1,\ldots,n-1})$
    + autoregressive
- image model?
    + pixelRNN
    + 效率的问题: 1920*1080=207,3600
** 生成模型直接建模
*** 如果直接建模$P_{\theta}(x)$, 使用范围局限
- 基于这个$P_{\theta}(x)$ 采样
- 无法做更多的复杂生成的事情，无法对对生成做细粒度操控
- 比如，
    + 我想加入一段文字的描述来生成
    + 我想生成某种特定类型，特定的风格的图片
*** 直接建模的难点？
- 图像本省的高维特性
- sample 的效率：高维空间中的sample效率会很低
- 你的建模需要能建模出来像素之间的依赖性
- 直接来建模pdf是一个非常困难的事情

* Generative Model
** 换个思路建模密度
*** 先验分布+确定性函数来建模
$P(X)=P(Z)P(X|Z)$
- prior z:
  + 没有需要学习的参数
  + 在一个低维度的latent space中采样
- 确定性函数：
  + h(z)
** 理论的依据
*** 高斯分布+CDF逆变换可以拟合任意的分布
- 假设
  + 随机变量 $N\sim \mathcal{N}[0,1]$, 对应的CDF 是$\Psi$
  + 那么$Y=\Psi(N)\sim \text{Uniform}[0,1]$
  + 目标随机变量$X$ 对应的分布的CDF是 $F(x)=P(X\le x)$
- 那么随机变量 $X=F^{-1}(Y)$ 分布满足$F$
- 高维中依然是成立，但是这个函数表达是不知道的，我们可以通过模型学习得到。
**** 均匀分布+CDF逆变换可以拟合任意的分布
- 假设
  + 随机变量 $U\sim \text{Uniform}[0,1]$
  + 目标随机变量对应的CDF是 $F(x)=P(X\le x)$
- 结论：随机变量 $X=F^{-1}(U)$ 分布满足$F$
- 证明:

   $P(X\le x)=P(F^{-1}(U)\le x)=P(U\le F(x))=F(x)$
**** 高斯分布到均匀分布
- 假设
  + 随机变量 $N\sim \mathcal{N}[0,1]$, 对应的CDF 是$\Psi$
  + 那么$Y=\Psi(N)\sim \text{Uniform}[0,1]$
- 证明:
   $P(Y\le y)=P(\Psi(N)\le y )=P(N\le \Psi^{-1}(y))=\Psi(\Psi^{-1}(y))=y$
*** 在生成式模型中运用：
sample $X$ 可以分两步走
- 先sample $Z\sim \mathcal{N}(0,1)$
- 然后再基于一个复杂的确定函数变换（交给DNN学习）得到 $f(Z)$ 变换得到$X$
- 随机变量 $X=f(Z)$ 就是对整体的sample建模
*** 为什么不用均匀分布做先验？而使用高斯？
- 高斯分布在整个空间上有定义，计算KL 不会有除以0的问题发生
- 高斯分布有很多很好的性质可以使用
** latent variables
*** motivation
- 我们看到的世界可能是高维空间到低维子空间的一个投影
- 我们观察获取到的信息$X$ 本身是不完整的
  + 或者说，我们无法观测到完整的信息
  + 盲人摸象
- 我们把在我们观测之外的这些特征可以记作 latent variables
*** 一些假设
- 每个样本$X$ 对应一个latent variable $Z$
- 完整的样本是$(X, Z)$
- 直接去优化 $P_{\theta}(X)$ 是困难的, 有积分的存在
   + $P(X)=\int_Z P_{\theta}(X,Z)=\int_{Z}P_{\theta}(X|Z)P_{\theta}(Z)dZ$
- 但是知道了$Z$ 后，$P(X,Z)$ 或者 $P(X|Z)$ 是容易优化的
*** 生成模型中的latent variables
- 生成的概念： 观测值是基于隐变量的值来生成的。
  + $Z\rightarrow X$ 
  + 先sample $Z$
  + 再基于 $P(X|Z)$ sample得到 $X$
- $P(X,Z)=P(Z)P(X|Z)$
*** 一个例子：高斯混合模型
$P(X)=\sum_{Z}P(Z)P(X|Z)=\sum\limits_{k=1}^{K} \pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})$
- sample过程
  + 先根据先验$P(Z)$ 决定在哪个群落点，
  + 再根据局部的似然 $P(X|Z)$ 采样，(根据这个群的均值，方差采样)
- 直接优化 $\log P(X)$ 非常困难
- 但是 $P(X,Z)=\prod\limits_{k=1}^K \pi_k^{Z_k}\mathcal{N}(X|\mu_{k}, \Sigma_{k})^{Z_k}$, 其中 $Z=(Z_1, Z_2, \ldots, Z_{k})$ one-hot 形式
  + $\log P(X,Z)=\sum\limits_{k=1}^{K}Z_k[log\pi_k+\log\mathcal{N}(X|\mu_{k}, \Sigma_{k})]$ 容易优化
#+DOWNLOADED: screenshot @ 2024-03-29 09:31:36
#+ATTR_HTML: :width 400px :align middle
[[file:images/2024-03-29_09-31-36_screenshot.png]]

** 生成式模型的概率建模
*** 一般的模型的概率建模方式
- 参数化单个样本$X$ 的概率 $P_{\theta}(X)$，
- 得到若干的样本$X_1, X_2, \ldots, X_N$
- 然后做概率的最大似然估计 $\prod_{i} P_{\theta}(X_i)$
*** 生成式模型的概率建模方式
- $P(X)=\int_Z P_{\theta}(X,Z)=\int_{Z}P_{\theta}(X|Z)P_{\theta}(Z)dZ$
- $P(X|Z)$ 参数化为 $P_{\theta}(X|Z)$
  + 比如 $P(X|Z)=\mathcal{N}(\mu(Z;\theta),\Sigma(Z;\theta))$
- $P(Z)$ 可以有参数，也可以没有参数
   + 在VAE/diffusion model 中，$P(Z)=\mathcal{N}(0,I)$
** 生成模型的优化
*** 优化的目标：
$P(X)=\int_Z P_{\theta}(X|Z)P(Z) dZ$
- 积分的存在，导致无法直接优化
*** 使用蒙特卡洛方法
对于一个给定的样本 $X$,$P(X)= E\limits_{Z\sim P(Z)} P(X|Z)$
- sample $Z_1, Z_2, \ldots, Z_n$ from $P(Z)$
- $P(X) \approx \dfrac{1}{n} \sum_{i} P_{\theta}(X|Z_i)$
- 再针对$\theta$ 做梯度下降
*** 问题：
- 维度灾难的问题： $n$ 需要极其大来得到一个准确的概率估计
- sample的过程中 $P(X|Z_i)$ 的概率大多为0，对参数的更新没有贡献
  + 所以我们要更加有效的 Z, 比如使用  $Q(Z|X)$ 来sample $Z$
  + $Q(Z|X)$ 能给出来容易生成$X$ 的Z 来。
  + 计算 $E_{Z\sim Q} P(X|Z)$
* 数学基础回顾
** 蒙特卡洛方法和维度灾难
*** 什么是MC
#+begin_quote
 The underlying concept is to use randomness to solve problems that might be deterministic in principle. 
#+end_quote
*** 计算期望
$E_{X\sim p(x)} f(X)$
- sample $\{X_{i}\}_{i=1}^{n} \sim p(x)$
- $\dfrac{1}{n}\sum_{i=1}^{n}f(X_{i})\rightarrow \mathrm{E}(f(X))$
*** 背后的依据：大数定律
如果$\{X_i\}_{i=1}^{n}$ 独立同分布，那么 $\dfrac{1}{n}\sum_{i=1}^{n}X_{i}\rightarrow \mathrm{E}(X)$

如果$\{X_i\}_{i=1}^{n}$ 独立同部分, 那么 $\{f(X_i)\}_{i=1}^{n}$ 也是独立同分布的，
且$\dfrac{1}{n}\sum_{i=1}^{n}f(X_{i})\rightarrow \mathrm{E}(f(X))$
*** 缺点：在高维空间中效率非常的低
- 在高维空间里面，你的采样到的大部分的点都不是你想要的
*** 举一个例子：计算$\pi$
- $n$ 维的球体的体积
  + $V_n=\dfrac{\pi^{\frac{n}{2}}R^{n}}{\Gamma(\frac{n}{2}+1)}$
  + $\Gamma(n+1)=n!$
- 计算方法
  + 基于单位球体的体积公式反向推导
     $\pi = (\dfrac{V_n\Gamma(\frac{n}{2}+1)}{R^n})^{\frac{2}{n}}$
  + sample n个[-1,1] 之间的均匀随机变量
  + 计算单位球中的个数比例得到单位球体的体积
  + 基于上述公式推导出$\pi$
- 维度灾难
  + $V_{n}\rightarrow 0, as\quad n\rightarrow \infty$
  + n维单位球体的体积趋向于0
  + sample失效
** 高斯分布的性质
*** 定义
$X\sim \mathcal{N}(\mu,\Sigma)$
*** 性质
- 高斯分布的线性组合还是高斯分布
- 先验gauss，似然高斯，后验依然是高斯
- KL divergence
   $D\left[ \mathcal{N}(\mu_0, \Sigma_0) \parallel \mathcal{N}(\mu_1, \Sigma_1) \right] = \frac{1}{2} \left( \text{tr} \left( \Sigma_1^{-1}\Sigma_0 \right) + (\mu_1 - \mu_0)^T \Sigma_1^{-1} (\mu_1 - \mu_0) - k + \log \left( \dfrac{\det\Sigma_1}{\det\Sigma_0} \right) \right)$
- KL
   $D\left[ \mathcal{N}(\mu(X), \Sigma(X)) \parallel \mathcal{N}(0, I) \right] = \frac{1}{2} \left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \det (\Sigma(X)) \right)$

** 重参数化
*** 问题
- $\nabla_{\theta}\mathrm{E}_{p(z)} \left[ f_\theta(z) \right]$
- $\nabla_{\theta}\mathrm{E}_{p_{\theta}(z)} \left[ f_\theta(z) \right]$
*** pdf不含有参数
\[
\nabla_\theta \mathrm{E}_{p(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p(z)f_\theta(z)dz
\]
\[
= \int p(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \mathrm{E}_{p(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]
求导穿过了期望
*** pdf中含有参数
\[
\nabla_\theta \mathrm{E}_{p_\theta(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p_\theta(z)f_\theta(z)dz
\]
\[
= \int \nabla_\theta \left[ p_\theta(z)f_\theta(z) \right] dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \int p_\theta(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \mathrm{E}_{p_\theta(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]
- 多出来一个左端项，不好处理
- 进一步，如果我们基于MC来表达期望的话
  + sample $\{Z_{i}\}_{i=1}^{n}\sim p(Z)$
  + 得到 $\dfrac{1}{n}\sum_{i}f_\theta(Z_{i})$
  + 如果sample 的分布 $p(Z)$ 依赖于$\theta$, 将无法求导
*** 重参数化=积分的变量替换
**** 什么是reparameterization trick?
- 如果 $z\sim p_z, z = g(\varepsilon), \varepsilon \sim p_\varepsilon$
- 那么 $\mathrm{E}_{p_{z}}f(z)=\mathrm{E}_{p_\varepsilon}f(g(\varepsilon))$
**** 解读
- 期望依赖的随机变量换了，pdf也换了
- 旧的pdf中可能不好处理，但是新的pdf比较容易处理
**** 应用
如果期望依赖的pdf中有参数，而我们需要针对这个期望对参数求导
$\begin{aligned}
\nabla_\theta \mathrm{E}_{p_\theta(z)}[f(z)]
&= \nabla_\theta \mathrm{E}_{p(\varepsilon)}[f(g_\theta(\varepsilon}))] \\
&= \mathrm{E}_{p(\varepsilon)}[\nabla_\theta f(g_\theta(\varepsilon}))] \\
&\approx \frac{1}{L} \sum_{l=1}^L \nabla_\theta f(g_\theta(\varepsilon^{(l)}))
\end{aligned}$
**** proof:
- 首先，两个pdf之间满足 $p_\varepsilon=p_z(g(\varepsilon))g'(\varepsilon)$
     $\begin{aligned}P(\varepsilon < y)
     &= P(g^{-1}(z)<y) \\
     &= P(z < g(y)) \\
     & = \int_{-\infty}^{g(y)} p_z(s) ds \\
     & \overset{s=g(\varepsilon)}{=} \int_{-\infty}^{y} p_z(g(\varepsilon))g'(\varepsilon) d\varepsilon
     \end{aligned}$
- 其次
    $\begin{aligned}
    & \quad\mathrm{E}_{p_{z}}f(z)\\
    =&\int f(s)p_z(s) ds \\
    =&\int f(g(\varepsilon))p_z(g(\varepsilon))g'(\varepsilon) d\varepsilon \\
    =&\int f(g(\varepsilon))p_{\varepsilon}(\varepsilon)d\varepsilon \\
    =& \mathrm{E}_{p_\varepsilon}f(g(\varepsilon)) \\
    \end{aligned}$
*** 类比
可以类比于强化学习中的 policy gradient 求导
   $J(\theta)= E_{\tau\sim \pi_{\theta}(\tau)} r(\tau)$
   
   $\begin{aligned}\nabla_{\theta}J(\theta) = & \int \nabla_{\theta}\pi_{\theta}(\tau)r(\tau)d\tau \\
     = & \int \pi_{\theta}(\tau) \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau)d\tau \\
     = & E_{\tau \sim \pi_{\theta}(\tau)}\left[ \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau) \right]
     \end{aligned}$
** 变分
*** 泛函 fuctional
- 输入是一个函数，输出一个值
- 例如熵的定义：$H(p)=\int p(x) \log p(x)dx$
- 变分：是在一个函数空间中针对一个泛函来寻求极值。
*** DNN 求解是在做泛函极小化的事情
- 给定数据集合 $D={(x_i,y_i)|i=1,\ldots, N}
- loss
   
   $J(f) = \sum\limits_{(x_i,y_i)\in D} L(f(x_i), y_i)$
- 在函数空间$F$ 中寻找极小化
   
   $\min\limits_{f\in F} J(f)$
- DNN 参数化
   
   $\min\limits_{\theta} J(\theta)$


* VAE
** VAE 的思路
#+DOWNLOADED: screenshot @ 2024-03-28 23:14:24
#+ATTR_HTML: :width 600px :align middle
[[file:images/2024-03-28_23-14-24_screenshot.png]]

- 不从$P(Z)$ 中sample $Z$
- 而从$Q_{\phi}(Z|X)$ 中sample $Z$，提升$P(X|Z)$ 的概率
- 然后基于$Z$ 重建 $\widehat{X}=f_{\theta}(Z)$
- 计算误差 $\widehat{X}$ 和 $X$ 的误差，再反向传播回去
** variational inference
- $\log P(X) = \mathcal{L} (Q) + \mathcal{D}(Q\|P)$
- $\mathcal{L}(Q) = \int Q(Z) \log \dfrac{P(X,Z)}{Q(Z)}dZ$
- $\mathcal{D}(Q\|P) = \int Q(Z) \log\dfrac{Q(Z)}{P(Z|X)} dZ$
- 寻找$Q(Z)$ 中最大化 $\mathcal{L}(Q)$
- 参数化 $Q_{\phi}(Z)$ 最大化 $\mathcal{L}(Q)$
   $\max\limits_{\phi}\mathcal{L}(Q)$
*** 证明
$\begin{aligned}
\log P(X) &= E_{Q(Z)} \log P(X) \\
&=E_{Q(Z)} \log \dfrac{P(X,Z)}{P(Z|X)} \\
&=E_{Q(Z)} \log \dfrac{P(X,Z)}{P(Z|X)} \dfrac{Q(Z)}{Q(Z)} \\
& = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)} + E_{Q(Z)} \log \dfrac{Q(Z)}{P(Z|X)}\\
& = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)} + \mathcal{D}[Q(Z)||P(Z|X)]
\end{aligned}$

** core of VAE
对于任意的$Q(Z)$， 有
$\log P(X) - \mathcal{D}[Q(Z) \| P(Z|X)] = \mathrm{E}_{Z \sim Q}[\log P(X|Z)] - \mathcal{D}[Q(Z) \| P(Z)]$
*** 其中涉及到的几个分布
- $P(X)$ 似然性
- $P(X|Z)$
- $P(Z)$ 先验分布
- $Q(Z)$ 
*** proof:
*** ELBO
由于 KL Divergence 非负，所以
$\begin{aligned}
\log P(X) & \ge  E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}
\end{aligned}$
** 理解ELBO
*** 理解ELBO
- $\log P_{\theta}(X) = \mathcal{L}(Q, \theta) + \mathcal{D}[Q(Z)\|P(Z|X)}]$
#+DOWNLOADED: screenshot @ 2024-03-29 11:29:54
#+ATTR_HTML: :width 400px :align middle
[[file:images/2024-03-29_11-29-54_screenshot.png]]
- 对于任意的$Q(Z)$，$\log P_\theta(X)\ge \mathcal{L}(Q,\theta)$
- 给定一个$\theta$, $\mathcal{L}(Q(Z), \theta)$ 是一个泛函
  + 这也是变分的意义所在，在各种函数中寻找一个最好的。
- 给定一个$Q(Z)$, $\mathcal{L}(Q(Z), \theta)$ 提供了一个$\theta$ 的函数曲线
  + 不断地优化和提升下界 $Q(Z)$，下界成为一个代理的优化目标
  + 通过不多优化下界来更新$\theta$
#+DOWNLOADED: screenshot @ 2024-03-29 11:50:58
[[file:images/2024-03-29_11-50-58_screenshot.png]]
      
*** 带上参数来理解
- 变分：$Q_{\phi}(Z|X)$ 参数化
- 左边是优化的目标似然性
- 右边第一项是ELBO
- 右边第一项是KL divergence
- 我们希望不断去优化$Q_{\phi}(Z|X)$ 提升ELBO，
- 当$Q_{\phi}(Z|X)= P(Z|X)$ 的时候，结束。
*** 继续拆分ELBO
$\begin{aligned}
E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)} &= E_{Q(Z)}\log \dfrac{P(X|Z)P(Z)}{Q(Z)} \\
&= E_{Q(Z)}\log P(X|Z) + E_{Q(Z)}\log \dfrac{P(Z)}{Q(Z)} \\
& = E_{Q(Z)}\log P(X|Z) - D_{KL}[Q(Z) \| P(Z)]
\end{aligned}$
- 第一项:
  + 有了encoding,decoding的意思
  + 可以使用MC的方法来优化
- 第二项:
   + 正则的一个效果
*** 最后
$\log P(X) - \mathcal{D}[Q(Z) \| P(Z|X)] = \mathrm{E}_{Q(Z))}[\log P(X|Z)] - \mathcal{D}[Q(Z) \|P(Z)]$
ELBO
$\log P(X) \ge \mathrm{E}_{Q(Z))}[\log P(X|Z)] - \mathcal{D}[Q(Z) \|P(Z)]$
** 参数化ELBO
$\log P(X) \ge \mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]$
*** 将Q,P 做参数化
- $Q_{\phi}(Z|X)=\mathcal{N}(Z|\mu_{\phi}(X), \Sigma_{\phi}(X))$
  + 每个样本对应一个独立的正态分布
  + $\mu_{\phi}(X)$ DNN
  + $\Sigma_{\phi}(X)$ DNN
- $P_{\theta}(X|Z)=\mathcal{N}(X|f_{\theta}(Z), 1)$
   + $f_{\theta}(Z)$ DNN
*** 其他
- $P(Z)=\mathcal{N}(0,1)$
- $\log P(X|Z)\sim \|X-f_{\theta}(Z)\|^{2}$
*** 两个gauss分布之间的KL散度可以显式计算
$\begin{aligned}
&\mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
=& D\left[ \mathcal{N}(\mu_{\phi}(X), \Sigma_{\theta}(X)) \parallel \mathcal{N}(0, I) \right] \\
=& \frac{1}{2} \left( \text{tr}(\Sigma_{\phi}(X)) + (\mu_{\phi}(X))^T (\mu(X)) - k - \log \det (\Sigma_{\phi}(X)) \right)
\end{aligned}$
*** 前向的步骤
#+DOWNLOADED: screenshot @ 2024-04-01 16:39:15
#+ATTR_HTML: :width 600px :align middle
[[file:images/2024-04-01_16-39-15_screenshot.png]]

*** 整体求导处理的思路：
我们有两个参数$\phi, \theta$, $\phi$ 出现在变分的候选函数里面，$\theta$出现在decoder里面。
- 第一项利用MC 近似期望，
- 第二项基于显式的计算来求导
*** 下一步：求导
** 重参数化
*** why
- $E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)$ 的梯度计算问题
  + $\nabla_{\phi} E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)$
  + 期望依赖的分布依赖于参数$\phi$
*** how
- 重参数化
  + $Z\sim Q_{\phi}(Z|X)=\mathcal{N}(Z|\mu(X;\phi), \Sigma(X;\phi))$ 
  + $Z=\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon, \quad \varepsilon \sim N(0,1)$
  + $E_{Q_{\phi}(Z|X)}\log P(X|Z)=E_{\varepsilon}\log P(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon))$
- 最后期望的分布不再依赖于参数
- 求导此时可以穿过期望
   
   $\begin{aligned}
   & \nabla_{\phi} E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)\\
   = &\nabla_{\phi} E_{\varepsilon}\log P_{\theta}(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon) \\
   = &E_{\varepsilon} \nabla_{\phi} \log P_{\theta}(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon)
   \end{aligned}$
   
** 基于MC的优化算法
ELBO，我们有两个参数$\phi, \theta$

$\begin{aligned}
\mathcal{L} &=
\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
&=\mathrm{E}_{\varepsilon}[\log P_{\theta}(X|Z(\phi,\varepsilon))] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
\end{aligned}$

应用MC方法
1. sample $\varepsilon_l \sim N(0,1), Z_l=\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon_{l}$
2. 计算ELBO
   $\mathcal{L}(\theta, \phi, X)=\dfrac{1}{L}\sum\limits_{l=1}^{L} \log P_{\theta}(X| Z_{l})}$
3. 对$\phi, \theta$ 求导
** 理解encoder和decoder
*** encoder $Q_{\psi}(Z|X)$
训练好之后，
*** decoder $P_{\theta}(X|Z)$
** VAE训练好后怎么用？
*** 直接生成
这个时候可以抛弃encoder了 $Q(Z|X)$
- sample $Z$ from $P(Z)$
- 确定性函数做一个映射 $f(Z)$
  + 虽然 $f(Z)$ 只是gauss的 $\mu$ 参数，无需再次sample
*** 重构原来的图像
encoder 和decoder 都需要使用
- 基于Q(Z
** open problems
*** 如何来理解整个训练过程中的噪音？
基于sample得到了$Z$，基于加入噪音后的sample来还原图像，这是一种提升模型鲁棒性的方法。
*** 如何来理解训练完了以后，基于$P(Z)$ 来sample？
- 正常使用生成工具的时候，原图是没有的
   + 你需要基于给与的条件或者是无中生有。
** VAE 图示
* VAE代码实现
** ELBO backbone
*** 背景
$P(x)=\int P(x|z) p(z)$
直接做主要的困难来源于MC在高维空间中的sample的效率问题
*** 引入Q分布
1. 聚焦有意义的 z 值
2. Q(z∣X)：为此，我们引入了一个新的函数 
Q(z∣X)，这是一个编码器网络，它可以基于观察到的数据 
X 提供一个关于 z 值的分布。这个分布专门针对那些可能产生 
X 的 z 值。变分方法使得这个分布可以通过学习数据来逼近真实的后验分布 
P(z∣X)。
*** formula
$\log P(X) - D_{KL}[Q(z|X) \| P(z|X)] = \mathbb{E}_{z \sim Q}[\log P(X|z)] - D_{KL}[Q(z|X) \| P(z)]$

* others
** 建模
*** 建模
- latent variable space $\mathcal{Z}$, pdf $p(z)$
- $z\in\mathcal{Z}$ 是一个随机变量
- deterministic function: $f:\mathcal{Z} \times {\Theta} \rightarrow \mathcal{X}$
- $f(z,\theta)$ 是一个随机变量
  + 希望$f$ 描述了这个数据的分布
- 优化的难点，在于随机变量的一个引入。
*** 建模两步
- sample z from $P(z)$
- sample x from $P(z|x)$
** 图像生成的两种思路
*** 自回归的方式
- 像素级别的自回归
- PixelRNN/PixelCNN
*** patch级别的自回归
*** 基于MC的优化算法
$\begin{aligned}
\mathcal{L} &=
\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
&=\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathrm{E}_{Q_{\phi}(Z|X)} \dfrac{Q_{\phi}(Z|X)}{P(Z)}\\
&=\mathrm{E}_{\varepsilon}[\log P_{\theta}(X|Z(\phi,\varepsilon))] - \mathrm{E}_{\varepsilon} \dfrac{Q_{\phi}(Z|X)}{P(Z)}\\
\end{aligned}$
简化一下
$\begin{aligned}
\mathcal{L} &=
\mathrm{E}_{\varepsilon}[\log P_{\theta}(X|Z(X, \phi,\varepsilon)) - Q_{\phi}(Z(X, \phi, \varepsilon)|X)]\\
\end{aligned}$

$\log P(X) \ge \mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]$

我们来做一个简化：
$\mathcal{L}(\theta, \phi, X)=
\dfrac{1}{L}\sum\limits_{l=1}^{L} \log P_{\theta}(X, Z^{l})}$

* todos
** 完成理论的讲解
** 脉络的梳理
*** TODO latent variable
*** TODO 积分存在，优化困难
*** TODO vae 出现
*** TODO ELBO 来优化，为什么基于lower bound可以优化
*** DONE reparametrization trick
*** TODO vae 在训练和预测的区别
*** TODO vae为什么丢弃掉 encoder, 直接基于一个全新的prior来sample P(Z)
*** TODO vae的训练脉络
**** 直接sample p(z) 效率很低，没法做
**** 最好从P(Z|X)中sample，不能
**** 从Q(Z|X) 从sample，Q(Z|X)会 converge 到P(Z|X)
**** Q(Z|X) 做了编码，然后 P(X|Z) 来解码
*** TODO loss的构建：
**** z生成 x，比较一下x的似然性，l2 loss, 或者说
直接来比较 f(Z) 和 x的距离
*** TODO 我们的问题的优化难点在哪里？
**** 我们要把不确定的东西引入到问题的优化过程中来。
**** TODO variational inference
