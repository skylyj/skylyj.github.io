<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>揭秘VAE背后的数学原理 - 连义江的博客</title><meta name=Description content="揭秘VAE背后的数学原理"><meta property="og:url" content="https://skylyj.github.io/vae/">
<meta property="og:site_name" content="连义江的博客"><meta property="og:title" content="揭秘VAE背后的数学原理"><meta property="og:description" content="揭秘VAE背后的数学原理"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-21T16:39:06+08:00"><meta property="og:image" content="https://skylyj.github.io/favicon-16x16.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://skylyj.github.io/favicon-16x16.png"><meta name=twitter:title content="揭秘VAE背后的数学原理"><meta name=twitter:description content="揭秘VAE背后的数学原理"><meta name=application-name content="AI"><meta name=apple-mobile-web-app-title content="AI"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://skylyj.github.io/vae/><link rel=next href=https://skylyj.github.io/configure_emacs/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"揭秘VAE背后的数学原理","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/skylyj.github.io\/vae\/"},"image":["https:\/\/skylyj.github.io\/favicon-32x32.png"],"genre":"posts","wordcount":8339,"url":"https:\/\/skylyj.github.io\/vae\/","datePublished":"2024-03-15T00:00:00+00:00","dateModified":"2024-04-21T16:39:06+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"连义江","logo":"https:\/\/skylyj.github.io\/favicon-32x32.png"},"author":{"@type":"Person","name":"连义江"},"description":"揭秘VAE背后的数学原理"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=连义江的博客>连义江的博客</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title=选择语言><i class="fa fa-globe" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/vae/ selected>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=连义江的博客>连义江的博客</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>所有文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/about/ title>关于</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/vae/ selected>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">揭秘VAE背后的数学原理</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>连义江</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-03-15>2024-03-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 8339 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 17 分钟&nbsp;<span id=/vae/ class=leancloud_visitors data-flag-title=揭秘VAE背后的数学原理>
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
</span>&nbsp;
<span id=busuanzi_container_value_page_pv><i class="far fa-eye fa-fw"></i>
<span id=busuanzi_value_page_pv></span>&nbsp;次阅读</span></div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#背景>背景</a><ul><li><a href=#主要内容>主要内容</a></li><li><a href=#生成模型>生成模型</a><ul><li><a href=#机器学习的几个topic>机器学习的几个topic</a></li><li><a href=#监督学习中的概率建模>监督学习中的概率建模</a></li><li><a href=#生成模型在做什么>生成模型在做什么？</a></li><li><a href=#用概率的语言描述这个世界>用概率的语言描述这个世界</a></li><li><a href=#生成模型的目的>生成模型的目的</a></li></ul></li><li><a href=#概率密度建模>概率密度建模</a><ul><li><a href=#概率密度建模的难点在哪里>概率密度建模的难点在哪里？</a></li><li><a href=#限定概率分布的形状-对pdf参数做函数化>限定概率分布的形状，对pdf参数做函数化</a></li><li><a href=#限定序列结构做条件概率展开>限定序列结构做条件概率展开</a></li><li><a href=#其他方法>其他方法：</a></li></ul></li></ul></li><li><a href=#数学基础回顾>数学基础回顾</a><ul><li><a href=#关于最大似然估计>关于最大似然估计</a><ul><li><a href=#什么是最大似然>什么是最大似然</a></li><li><a href=#理解>理解</a></li><li><a href=#最大似然会有什么问题>最大似然会有什么问题？</a></li></ul></li><li><a href=#蒙特卡洛方法和维度灾难>蒙特卡洛方法和维度灾难</a><ul><li><a href=#什么是mc>什么是MC</a></li><li><a href=#计算期望>计算期望</a></li><li><a href=#背后的依据-大数定律>背后的依据：大数定律</a></li><li><a href=#缺点-在高维空间中效率非常的低>缺点：在高维空间中效率非常的低</a></li><li><a href=#注>注：</a></li></ul></li><li><a href=#概率论基础的公式>概率论基础的公式</a><ul><li><a href=#链式法则>链式法则</a></li><li><a href=#贝叶斯公式>贝叶斯公式</a></li></ul></li><li><a href=#高斯分布>高斯分布</a><ul><li><a href=#定义>定义</a></li><li><a href=#封闭性>封闭性</a></li><li><a href=#kl-divergence>KL divergence</a></li></ul></li><li><a href=#重参数化>重参数化</a><ul><li><a href=#问题>问题：</a></li><li><a href=#pdf不含有参数>pdf不含有参数</a></li><li><a href=#pdf中含有参数>pdf中含有参数</a></li><li><a href=#重参数化-积分的变量替换>重参数化=积分的变量替换</a></li><li><a href=#类比log-trick>类比log trick</a></li><li><a href=#启发>启发：</a></li></ul></li><li><a href=#jensen-s-inequality>Jensen&rsquo;s inequality</a><ul><li><a href=#statement>statement：</a></li><li><a href=#概率中的表述>概率中的表述：</a></li><li><a href=#注>注</a></li></ul></li><li><a href=#变分>变分</a><ul><li><a href=#泛函-fuctional>泛函 fuctional</a></li><li><a href=#dnn-求解是在做泛函极小化的事情>DNN 求解是在做泛函极小化的事情</a></li><li><a href=#变分的感觉>变分的感觉</a></li></ul></li></ul></li><li><a href=#基于隐变量的生成模型>基于隐变量的生成模型</a><ul><li><a href=#latent-variables>latent variables</a><ul><li><a href=#什么是latent-variables>什么是latent variables</a></li><li><a href=#图片的latent-variable>图片的latent variable</a></li><li><a href=#对的latent-variables的一些假设>对的latent variables的一些假设</a></li><li><a href=#基于隐变量的生成模型>基于隐变量的生成模型</a></li><li><a href=#一个例子-高斯混合模型>一个例子：高斯混合模型</a></li></ul></li><li><a href=#基于隐变量的图像生成模型>基于隐变量的图像生成模型</a><ul><li><a href=#不是把所有的图像放在一起做一个分布-p--x>不是把所有的图像放在一起做一个分布 \(P(X)\)</a></li><li><a href=#每个图像可以看做对应一个分布>每个图像可以看做对应一个分布</a></li><li><a href=#从一图一个分布到一个编码一个分布>从一图一个分布到一个编码一个分布</a></li></ul></li><li><a href=#隐变量生成模型的概率建模>隐变量生成模型的概率建模</a><ul><li><a href=#一般的模型的概率建模方式>一般的模型的概率建模方式</a></li><li><a href=#生成式模型的概率建模方式>生成式模型的概率建模方式</a></li><li><a href=#生成式模型的概率建模本质>生成式模型的概率建模本质</a></li></ul></li><li><a href=#隐变量生成模型的优化困难>隐变量生成模型的优化困难</a><ul><li><a href=#优化的目标>优化的目标：</a></li><li><a href=#使用蒙特卡洛方法>使用蒙特卡洛方法？</a></li><li><a href=#困难>困难</a></li></ul></li><li><a href=#隐变量生成模型的另一个挑战>隐变量生成模型的另一个挑战</a><ul><li><a href=#关于完备性>关于完备性：</a></li></ul></li></ul></li><li><a href=#vae>VAE</a><ul><li><a href=#vae-的思路>VAE 的思路</a><ul><li><a href=#思考点一-log穿过积分>思考点一：log穿过积分</a></li><li><a href=#思考点二-提升采样有效性>思考点二：提升采样有效性</a></li></ul></li><li><a href=#variational-inference>variational inference</a><ul><li><a href=#证明>证明</a></li></ul></li><li><a href=#elbo-重建误差-plus-正则项>ELBO=重建误差+正则项</a><ul><li><a href=#理解>理解</a></li><li><a href=#proof>proof</a></li></ul></li><li><a href=#参数化elbo>参数化ELBO</a><ul><li><a href=#参数化>参数化</a></li><li><a href=#结果>结果</a></li></ul></li><li><a href=#理解elbo>理解ELBO</a></li><li><a href=#vae求解>VAE求解</a><ul><li><a href=#对-phi-求导思路>对\(\phi\) 求导思路</a></li><li><a href=#重参数化>重参数化</a></li><li><a href=#最终的求解算法>最终的求解算法</a></li></ul></li><li><a href=#前向的步骤>前向的步骤</a></li><li><a href=#vae训练好后怎么用>VAE训练好后怎么用？</a><ul><li><a href=#直接生成>直接生成</a></li><li><a href=#重构原来的图像>重构原来的图像</a></li></ul></li><li><a href=#理论的依据>理论的依据</a><ul><li><a href=#高斯分布-plus-cdf逆变换可以拟合任意的分布>高斯分布+CDF逆变换可以拟合任意的分布</a></li><li><a href=#在生成式模型中运用>在生成式模型中运用：</a></li><li><a href=#为什么不用均匀分布做先验-而使用高斯>为什么不用均匀分布做先验？而使用高斯？</a></li></ul></li><li><a href=#一些思考>一些思考</a><ul><li><a href=#我们的假设>我们的假设：</a></li><li><a href=#本质的建模-无穷个高斯模型做加权混合>本质的建模：无穷个高斯模型做加权混合</a></li><li><a href=#vae-的生成的图像模糊>VAE 的生成的图像模糊</a></li></ul></li><li><a href=#关于噪音>关于噪音</a><ul><li><a href=#open-problems>open problems</a></li></ul></li></ul></li><li><a href=#vae代码实现>VAE代码实现</a><ul><li><a href=#代码实现>代码实现</a><ul><li><a href=#model>model</a></li><li><a href=#loss>loss</a></li><li><a href=#train>train</a></li></ul></li><li><a href=#总结>总结</a><ul><li><a href=#生成模型>生成模型</a></li><li><a href=#求解>求解：</a></li><li><a href=#使用>使用：</a></li><li><a href=#优化中的独特点>优化中的独特点：</a></li><li><a href=#建模的本质-无穷个高斯和一个高斯>建模的本质：无穷个高斯和一个高斯</a></li></ul></li></ul></li><li><a href=#参考论文>参考论文</a></li></ul></nav></div></div><div class=content id=content><h2 id=背景>背景</h2><h3 id=主要内容>主要内容</h3><ul><li>背景<ul><li>VAE在图像生成模型扮演了非常重要的角色</li><li>ELBO应用: VQVAE, Diffusion Model, Dalle</li><li>数学门槛较高: 概率/泛函/微积分</li></ul></li><li>生成模型之概率建模<ul><li>生成模型在做什么/如何对概率密度建模/挑战</li></ul></li><li>VAE建模之数学基础<ul><li>最大似然/蒙特卡洛/大数定律/维数灾难/积分变量替换/变分</li></ul></li><li>VAE<ul><li>基于隐变量的生成模型/优化的困难</li><li>变分思想/下界的导出/如何求解</li></ul></li><li>VAE的代码实现</li><li>参考：<ul><li>Tutorial on Variational Autoencoders (NO_ITEM_DATA:doerschTutorialVariationalAutoencoders2021)</li><li>Auto-Encoding Variational Bayes (NO_ITEM_DATA:kingmaAutoEncodingVariationalBayes2022a)</li></ul></li></ul><h3 id=生成模型>生成模型</h3><h4 id=机器学习的几个topic>机器学习的几个topic</h4><ul><li>监督学习：分类/回归</li><li>无监督学习: 概率密度估计</li></ul><h4 id=监督学习中的概率建模>监督学习中的概率建模</h4><ul><li>基于特征来推断目标事件发生的概率
\(P(Y=1|X)=\sigma(f_{\theta}(X))\)</li><li>ctr 里面：
基于用户和item的特征信息来推测用户点击item的概率</li><li>NLP 中的相关性模型：
基于用户输入的query和doc特征来推测两者相关的概率</li><li>我们很少需要对样本特征的分布\(P(X)\) 直接建模</li></ul><h4 id=生成模型在做什么>生成模型在做什么？</h4><ul><li>对样本\(X\) 的概率分布\(P(X)\) 做估计<ul><li>知道什么样的样本出现的概率高，什么样的样本出现的概率低</li></ul></li><li>\(X\) 是什么？原始的特征输入。<ul><li>文本：一句话<ul><li>✅ X=I have a lovely dog.</li><li>❌ X=Lovely Dog have a i.</li></ul></li><li>图像: 灰度的图的话，一个矩阵
\(X=\begin{pmatrix}
1 & 2 & 3 \\
5 & 7 & 9 \\
200 & 222 & 255 \\
\end{pmatrix}\)</li></ul></li></ul><ul><li><p>图像的例子</p><div align=middle><img src=images/2024-04-18_23-10-59_screenshot.png alt=2024-04-18_23-10-59_screenshot.png width=300px align=middle>
✅</div><div align=middle><img src=images/2024-04-18_23-13-34_screenshot.png alt=2024-04-18_23-13-34_screenshot.png width=300px align=middle>
❌</div></li></ul><h4 id=用概率的语言描述这个世界>用概率的语言描述这个世界</h4><ul><li>我看到的都是一些实实在在的东西，一个图像怎么用概率来描述？</li><li>可以认为<ul><li>这个世界上的一切都是不确定的</li><li>发生的事实背后都有一个对应的概率分布</li><li>发生的事实其实都是上帝在基于这个分布采样的结果</li></ul></li><li>比如：<ul><li>P(用户点击广告|用户, 广告)<ul><li>基于用户和广告的特征，经过一个DNN输出了一个概率值P</li><li>然后基于二项分布采样得到了最后的事实</li></ul></li></ul></li><li>一张图像，我们可以认为是采样的结果<ul><li>一张灰度图像可以看做是一个n*n维度的向量</li><li>整体是一个gauss分布</li><li>简单的可以认为是各向同性的，像素之间相互独立。</li></ul></li></ul><h4 id=生成模型的目的>生成模型的目的</h4><ul><li>对图像的likelihood给出一个客观的判断</li><li>基于概率采样生成<ul><li>我们可以生成更多的类似的，不在样本集合中的 X</li></ul></li><li>例子：<ul><li>给一些树木的样例图片，可以生成一片森林出来。</li><li>conditional 的时候会产生更大的价值<ul><li>控制的图像生成，P(x|c), 文本到图像的生成</li></ul></li></ul></li></ul><ul><li><p>例子1</p><figure><img src=/vae/images/2024-04-19_23-52-53_screenshot.png width=600px></figure></li></ul><ul><li><p>例子2</p><figure><img src=/vae/images/2024-04-19_23-56-46_screenshot.png width=600px></figure></li></ul><h3 id=概率密度建模>概率密度建模</h3><h4 id=概率密度建模的难点在哪里>概率密度建模的难点在哪里？</h4><p>建模密度函数\(P_{\theta}(x)\)</p><ul><li><p>约束</p><ul><li>\(P_{\theta}(x)\ge 0, \forall x \in \mathcal{X}\)</li><li>\(\int P_{\theta}(x) dx=1, \forall \theta \in \Theta\) ✅ 真正的难点</li></ul></li><li><p>\(\nabla_{\theta} log P_{\theta}(x)\) 是否容易计算</p><ul><li>\(P(x)=\int P(x,z) dz\) ❌</li></ul></li><li><p>能够容易sample 采样生成</p><ul><li>autogressive model 应用在图像像素级别 ❌</li></ul></li></ul><h4 id=限定概率分布的形状-对pdf参数做函数化>限定概率分布的形状，对pdf参数做函数化</h4><ul><li>假设这个分布是 \(\mathcal{N}(\mu_{\theta}(x),\Sigma_{\theta}(x))\)</li><li>极大似然估计来推断 \(\mu_{\theta}, \Sigma_{\theta}\)</li><li>这个方法在VAE中大量的出现，比如对\(Q_{\phi}(Z|X), P_{\theta}(Z|X)\)</li></ul><h4 id=限定序列结构做条件概率展开>限定序列结构做条件概率展开</h4><ul><li>依赖：<ul><li>\(X\) 是可以序列化的 \(X=(x_{1}, x_{2},\ldots, x_{n})\)</li><li>\(P(x_{1}, x_{2},\ldots, x_{n})= P(x_{1})P(x_{2}|x_{1})\ldotsP(x_{n}|x_{1,\ldots,n-1})\)</li></ul></li><li>对中间的每个条件概率项做好归一化即可<ul><li><p>\(\int P(x_i|x_{1,\ldots,i-1}) dx_i=1\)</p></li><li><p>\(\int P(x_{1}, x_{2},\ldots, x_{n}) dx_1dx_2\cdots dx_n=1\)</p></li><li><p>在DNN中，表现为最后一层做softmax</p><p>比如dnn的language model 在训练的时候，为了输出token \(x_{i}\) 的概率，需要对所有的token做一个打分, 然后softmax</p></li></ul></li><li>autoregressive models</li><li>例子：<ul><li>language model</li><li>image model: pixelRNN<ul><li>效率的问题: 1920*1080=207,3600</li><li>如何序列化？</li></ul></li></ul></li></ul><h4 id=其他方法>其他方法：</h4><ul><li>基于隐变量的生成模型<ul><li>VAE</li><li>diffusion model</li></ul></li><li>GAN：其实绕开了对于概率密度的数学建模</li><li>flow based</li></ul><h2 id=数学基础回顾>数学基础回顾</h2><h3 id=关于最大似然估计>关于最大似然估计</h3><h4 id=什么是最大似然>什么是最大似然</h4><ul><li>数据集合 \(\mathcal{X}=\{X_1, X_2, \cdots, X_N}\}\)</li><li>建模 \(P_{\theta}(\mathcal{X})=\prod_{i}P(X_{i})\)</li><li>寻找参数\(\theta\)，使得\(\mathcal{X}\) 发生的概率 \(P_{\theta}(\mathcal{X})\) 最大化</li></ul><h4 id=理解>理解</h4><ul><li>事实为依据，存在即合理</li><li>用模型来解释已经发生的事实 \(P(\mathcal{X}|\theta)\)</li><li>哪个模型输出的概率高，就用哪个<ul><li>model1 预测 \(X\) 概率0.9</li><li>model2 预测 \(X\) 概率0.2</li><li>而\(X\) 已经发生，选择和事实最接近的。</li></ul></li></ul><h4 id=最大似然会有什么问题>最大似然会有什么问题？</h4><ul><li>当数据量少的时候，会发生过拟合。<ul><li><p>发生的事实可能有噪音, 你的结论可能是拟合了噪音。</p></li><li><p>类比地域歧视：</p><blockquote><p>你雇过两个阿姨，打扫卫生都不干净，你发现他们都来自于A 省份，然后你得出一个结论：A省份的阿姨打扫卫生都很差，以后坚决不找A省份的阿姨。</p></blockquote></li><li><p>不要因为一两次次失败就否定自己, 你的否定很可能是过拟合了。 :)</p></li></ul></li><li>怎么办？<ul><li>读万卷书，先验的知识来纠偏, 贝叶斯的方法
\(P(\theta|\mathcal{X})=\dfrac{P(\mathcal{X}|\theta)P(\theta)}{P(\mathcal{X})}\)</li><li>行万里路，看更多的数据，调整你的\(\theta\)<ul><li>但是人生短暂，实践的代价可能会很大 :)</li></ul></li></ul></li></ul><h3 id=蒙特卡洛方法和维度灾难>蒙特卡洛方法和维度灾难</h3><h4 id=什么是mc>什么是MC</h4><blockquote><p>The underlying concept is to use randomness to solve problems that might be deterministic in principle.</p></blockquote><h4 id=计算期望>计算期望</h4><p>\(E_{X\sim p(x)} f(X)\)</p><ul><li>i.i.d sample \(\{X_{i}\}_{i=1}^{n} \sim p(x)\)</li><li>\(\dfrac{1}{n}\sum_{i=1}^{n}f(X_{i})\rightarrow \mathrm{E}(f(X))\)</li></ul><h4 id=背后的依据-大数定律>背后的依据：大数定律</h4><ul><li><p>如果\(\{X_i\}_{i=1}^{n}\) 独立同分布，那么 \(\dfrac{1}{n}\sum_{i=1}^{n}X_{i}\rightarrow \mathrm{E}(X)\)</p></li><li><p>应用到上面：</p><p>如果\(\{X_i\}_{i=1}^{n}\) 独立同部分, 那么 \(\{f(X_i)\}_{i=1}^{n}\) 也是独立同分布的，
且\(\dfrac{1}{n}\sum_{i=1}^{n}f(X_{i})\rightarrow \mathrm{E}(f(X))\)</p></li></ul><h4 id=缺点-在高维空间中效率非常的低>缺点：在高维空间中效率非常的低</h4><ul><li>在高维空间里面，你的采样到的大部分的点都不是你想要的</li></ul><ul><li><p>看一个面试题目：计算\(\pi\)</p><ul><li>在二维空间的解法<ul><li>sample n个[-1,1] 之间的均匀随机变量</li><li>计算落入半径为1的圆形中间的比例</li><li>\(\pi r^2/4r^{2}=\pi/4\)</li></ul></li><li>推广到n维空间中<ul><li>\(n\) 维的球体的体积, 半径为\(R\)</li><li>\(V_n=\dfrac{\pi^{\frac{n}{2}}R^{n}}{\Gamma(\frac{n}{2}+1)},~\Gamma(n+1)=n!\)</li><li>基于单位球体的体积公式反向推导
\(\begin{aligned}\pi
&=(\dfrac{V_n\Gamma(\frac{n}{2}+1)}{R^n})^{\frac{2}{n}}\\
&=(\dfrac{V_n}{(2R)^{n}} \times 2^{n}\times \Gamma(\frac{n}{2}+1))^{\frac{2}{n}}\\
&=(\dfrac{V_{n}}{V_{cube}} \times 2^{n}\times \Gamma(\frac{n}{2}+1))^{\frac{2}{n}}
\end{aligned}\)</li><li>方法：<ul><li>sample n个[-1,1] 之间的均匀随机变量</li><li>计算单位球中的个数比例得到 \(V_{n}/V_{cube}\)</li></ul></li></ul></li><li>维度灾难<ul><li>\(V_{n}\rightarrow 0, n\rightarrow \infty\)</li><li>n维单位球体的体积趋向于0</li><li>sample失效</li></ul></li></ul></li></ul><h4 id=注>注：</h4><ul><li>MC 偏好期望形式的优化目标 \(E_{P(X)} f(X)\)</li><li>例子：<ul><li>\(E_{\tau\sim p(\tau)}r(\tau)\)</li><li>\(\int_{p(z|x)}p(x|z)dz\)</li></ul></li></ul><h3 id=概率论基础的公式>概率论基础的公式</h3><h4 id=链式法则>链式法则</h4><p>\(P(A_1 \cap A_2 \cap \ldots \cap A_n) = P(A_1) \times P(A_2 \mid A_1) \times \ldots P(A_n \mid A_1 \cap \ldots \cap A_{n-1})\)</p><ul><li>\(P(X_{1}X_{2}\ldots X_n) = P(X_1)P(X_2|X_1)\ldots P(X_{n}|X_{&lt;n}})\)</li></ul><h4 id=贝叶斯公式>贝叶斯公式</h4><p>\(P(Z|X)=\dfrac{P(X|Z)P(Z)}{P(X)}\)</p><ul><li>\(P(Z)\) 先验</li><li>\(P(X|Z)\) 似然性</li><li>\(P(Z|X)\) 后验</li><li>在VAE中，\(Z\) 是隐变量，\(X\) 是图像</li><li>贝叶斯估计MAP，\(P(\theta|\mathcal{X}) = \dfrac{P(\mathcal{X}|\theta)P(\theta)}{P(\mathcal{X})}\)</li></ul><h3 id=高斯分布>高斯分布</h3><h4 id=定义>定义</h4><ul><li>\(X\sim \mathcal{N}(\mu,\Sigma)\)<ul><li>一维：\(X\sim \mathcal{N}(\mu, \sigma^{2})\) , \(p(x)=\dfrac{1}{\sqrt{2\pi\sigma^{2}}}\mathrm{exp}({-\dfrac{1}{2}(\dfrac{x-\mu}{\sigma})^{2}})\)</li><li>k维: \(p(x)=\dfrac{1}{\sqrt{(2\pi)^{k}|\Sigma|}}\mathrm{exp}(-\dfrac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\)</li></ul></li><li>mean vector: \(\mu=\mathrm{E}[X]\)</li><li>covariance matrix:<ul><li>\(\Sigma_{i,j} = \mathrm{E}[(X_i-\mu_i)(X_{j}-\mu_j)}]\)</li><li>在VAE/diffusion model中，出现的都是对角矩阵 \(\Sigma=\sigma^{2}I\)</li></ul></li></ul><h4 id=封闭性>封闭性</h4><ul><li>Affine transformation: if p(x) is Gaussian, then p(Ax + b) is a Gaussian.</li><li>高斯随机变量的线性组合还是高斯分布<ul><li>VDM: \(z_{t}=\sqrt{1-\beta_t}{z_{t-1}}+\beta_{t}\varepsilon\)</li></ul></li><li>Product: if p(x) and p(z) are Gaussian, then p(x)p(z) is proportional to a Gaussian<ul><li>先验高斯，似然高斯，后验依然是高斯</li><li>\(P(X|Z)=\dfrac{P(Z|X)P(X)}{P(Z)}\)</li><li>VDM：\(P(z_{t-1})\) 高斯，\(P(z_t|z_{t-1})\) 高斯，那么\(P(z_{t-1}|z_{t})\) 也是高斯。</li></ul></li><li>Marginalization: if p(x, z) is Gaussian, then p(x) is Gaussian.</li><li>Conditioning: if p(x, z) is Gaussian, then p(x | z) is Gaussian.</li></ul><h4 id=kl-divergence>KL divergence</h4><ul><li>两个高斯分布之间的KL divergence
\(\begin{aligned}
&\mathcal{D}\left[ \mathcal{N}(\mu_0, \Sigma_0) \parallel \mathcal{N}(\mu_1, \Sigma_1) \right] \\
=& \frac{1}{2} \left( \text{tr} \left( \Sigma_1^{-1}\Sigma_0 \right) + (\mu_1 - \mu_0)^T \Sigma_1^{-1} (\mu_1 - \mu_0) - k + \log \left( \dfrac{\det\Sigma_1}{\det\Sigma_0} \right) \right)
\\
\end{aligned}\)</li><li>VAE中的涉及：后验和先验之间的距离
\(\begin{aligned}
& \mathcal{D}\left[P(Z|X) \parallel P(Z)\right] \\
=&\mathcal{D}\left[ \mathcal{N}(\mu(X), \Sigma(X)) \parallel \mathcal{N}(0, I) \right] \\
=& \frac{1}{2} \left( \text{tr}(\Sigma(X)) + (\mu(X))^T (\mu(X)) - k - \log \det (\Sigma(X)) \right)
\end{aligned}\)</li></ul><h3 id=重参数化>重参数化</h3><h4 id=问题>问题：</h4><ul><li>两种情形下求梯度<ul><li>\(\nabla_{\theta}\mathrm{E}_{p(z)} \left[ f_\theta(z) \right]\)，pdf没有参数</li><li>\(\nabla_{\theta}\mathrm{E}_{p_{\theta}(z)} \left[ f_\theta(z) \right]\)，pdf有参数</li></ul></li><li>VAE: reconstruction error 的梯度计算<ul><li>\(\nabla_{\phi}E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)\)</li></ul></li></ul><h4 id=pdf不含有参数>pdf不含有参数</h4><p>\[
\nabla_\theta \mathrm{E}_{p(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p(z)f_\theta(z)dz
\]
\[
= \int p(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \mathrm{E}_{p(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]
求导穿过了期望，这个好处在于可以对最后这个式子做MC</p><h4 id=pdf中含有参数>pdf中含有参数</h4><p>\[
\nabla_\theta \mathrm{E}_{p_\theta(z)} \left[ f_\theta(z) \right] = \nabla_\theta \int p_\theta(z)f_\theta(z)dz
\]
\[
= \int \nabla_\theta \left[ p_\theta(z)f_\theta(z) \right] dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \int p_\theta(z) \nabla_\theta f_\theta(z) dz
\]
\[
= \int f_\theta(z) \nabla_\theta p_\theta(z) dz + \mathrm{E}_{p_\theta(z)} \left[ \nabla_\theta f_\theta(z) \right]
\]</p><ul><li>多出来一个左端项，不好处理</li><li>进一步，如果我们基于MC来表达期望的话</li></ul><h4 id=重参数化-积分的变量替换>重参数化=积分的变量替换</h4><ul><li><p>什么是reparameterization trick?</p><ul><li>如果 \(z\sim p_z, z = g(\varepsilon), \varepsilon \sim p_\varepsilon\)</li><li>那么 \(\mathrm{E}_{p_{z}}f(z)=\mathrm{E}_{p_\varepsilon}f(g(\varepsilon))\)</li></ul></li></ul><ul><li><p>应用</p><p>如果期望依赖的pdf中有参数，而我们需要针对这个期望对参数求导
\(\begin{aligned}
\nabla_\theta \mathrm{E}_{p_\theta(z)}[f(z)]
&= \nabla_\theta \mathrm{E}_{p(\varepsilon)}[f(g_\theta(\varepsilon}))] \\
&= \mathrm{E}_{p(\varepsilon)}[\nabla_\theta f(g_\theta(\varepsilon}))] \\
&\approx \frac{1}{L} \sum_{l=1}^L \nabla_\theta f(g_\theta(\varepsilon^{(l)}))
\end{aligned}\)</p></li></ul><ul><li><p>proof:</p><ul><li>首先，两个pdf之间满足 \(p_\varepsilon=p_z(g(\varepsilon))g&rsquo;(\varepsilon)\)
\(\begin{aligned}P(\varepsilon &lt; y)
&= P(g^{-1}(z)&lt;y) \\
&= P(z &lt; g(y)) \\
& = \int_{-\infty}^{g(y)} p_z(s) ds \\
& \overset{s=g(\varepsilon)}{=} \int_{-\infty}^{y} p_z(g(\varepsilon))g&rsquo;(\varepsilon) d\varepsilon
\end{aligned}\)</li><li>其次
\(\begin{aligned}
& \quad\mathrm{E}_{p_{z}}f(z)\\
=&\int f(s)p_z(s) ds \\
\overset{s=g(\varepsilon)}{=}&\int f(g(\varepsilon))p_z(g(\varepsilon))g&rsquo;(\varepsilon) d\varepsilon \\
=&\int f(g(\varepsilon))p_{\varepsilon}(\varepsilon)d\varepsilon \\
=& \mathrm{E}_{p_\varepsilon}f(g(\varepsilon)) \\
\end{aligned}\)</li></ul></li></ul><h4 id=类比log-trick>类比log trick</h4><p>可以类比于强化学习中的 policy gradient 求导
\(J(\theta)= E_{\tau\sim \pi_{\theta}(\tau)} r(\tau)\)</p><p>\(\begin{aligned}
\nabla_{\theta}J(\theta)
= & \nabla_{\theta} \int\pi_{\theta}(\tau)r(\tau)d\tau \\
= & \int \nabla_{\theta}\pi_{\theta}(\tau)r(\tau)d\tau \\
= & \int \pi_{\theta}(\tau) \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau)d\tau \\
= & E_{\tau \sim \pi_{\theta}(\tau)}\left[ \nabla_{\theta}\log \pi_{\theta}(\tau)r(\tau) \right]
\end{aligned}\)</p><h4 id=启发>启发：</h4><ol><li>目标函数是一个期望的形式, 依赖的pdf中含有参数</li><li>两种处理的手段<ul><li>重参数化</li><li>log trick</li></ul></li></ol><h3 id=jensen-s-inequality>Jensen&rsquo;s inequality</h3><h4 id=statement>statement：</h4><p>the secant line of a convex function lies above the graph of the function.
\(f(tx_1+(1-t)x_{2}) \le tf(x_1)+(1-t)f(x_2), \forall t \in [0,1]\)</p><figure><img src=/vae/images/2024-04-13_16-46-32_screenshot.png width=500px></figure><h4 id=概率中的表述>概率中的表述：</h4><p>如果 \(X\) 是随机变量，\(f\) 是一个凸函数的话，\(f (E(X)) \le E(f(X))\)</p><h4 id=注>注</h4><ul><li>log函数是一个凹函数, \(\log (E(X)) \ge E(\log(X))\)</li><li>如果遇到 log和期望的时候，我们可以让log穿过期望符号，得到一个好的下界<ul><li>VAE: \(\log E_{Q(Z)} \dfrac{P(X,Z)}{Q(Z)} dZ \ge E_{Q(Z)}\log\dfrac{P(X,Z)}{Q(Z)}\)</li></ul></li></ul><h3 id=变分>变分</h3><h4 id=泛函-fuctional>泛函 fuctional</h4><ul><li>泛函是一个函数：<ul><li>输入是一个函数</li><li>输出一个值</li></ul></li><li>例如熵的定义：\(H(p)=\int p(x) \log p(x)dx\)</li><li>变分：是在一个函数空间中针对一个泛函来寻求极值。</li></ul><h4 id=dnn-求解是在做泛函极小化的事情>DNN 求解是在做泛函极小化的事情</h4><ul><li><p>给定数据集合 \(D=\{(x_i,y_i)|i=1,\ldots, N\}\)</p></li><li><p>loss：就是一个泛函</p><p>\(J(f) = \sum\limits_{(x_i,y_i)\in D} L(f(x_i), y_i)\)</p></li><li><p>在函数空间\(F\) 中做泛函的极小, F是连续函数的空间，无穷维。
\(\min\limits_{f\in F} J(f)\)</p></li><li><p>参数化: 泛函极小化到参数极小化</p><ul><li>选定参数空间是DNN形式的函数</li><li>\(f(x)=f_{n}\circ \ldots f_1(x)}\), \(f_{i}\) 是一层DNN变换，\(\theta\) 是全部的DNN参数</li><li>\(f(x)=f_{\theta}(x)\)</li><li>\(\min\limits_{\theta} J(\theta)\) 有限维空间中求解</li></ul></li></ul><h4 id=变分的感觉>变分的感觉</h4><ul><li>有一个泛函的存在，\(J(f)\)</li><li>\(f\) 在一个函数空间中变化 \(\mathcal{F}\)</li><li>对 \(J\) 求极值</li></ul><ul><li><p>VAE中的变分</p><p>\(\begin{aligned}
\log P(X)
& = \log \int P(X,Z)dZ \\
& = \log \int \dfrac{P(X,Z)}{Q(Z)} Q(Z) dZ \\
&= \log E_{Q(Z)} \dfrac{P(X,Z)}{Q(Z)} \\
&\ge E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}
\end{aligned}\)</p><ul><li>上面的式子对任意的概率分布\(Q(Z)\) 都成立</li><li>泛函 \(\mathcal{L}(Q) = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}\)</li><li>函数的空间是所有的PDF \(Q(Z)\)</li><li>下界极大化</li></ul></li></ul><ul><li><p>这个不是在做变分</p><p>\(\begin{aligned}
\log P(X)
& = \log \int P(X,Z)dZ \\
& = log \int P(X|Z) P(Z) dZ \\
&= log E_{P(Z)} P(X|Z) \\
& \ge E_{P(Z)} \log P(X|Z)
\end{aligned}\)</p></li></ul><h2 id=基于隐变量的生成模型>基于隐变量的生成模型</h2><h3 id=latent-variables>latent variables</h3><h4 id=什么是latent-variables>什么是latent variables</h4><ul><li>我们看到的世界可能是高维空间到低维子空间的一个投影</li><li>我们观察获取到的信息\(X\) 本身是不完整的<ul><li>或者说，我们无法观测到完整的信息，盲人摸象</li><li>例子：ctr预估，天气，人的心情，或者平台获取不到信息等</li></ul></li><li>我们可以把观测之外的这些特征可以记作 latent variables</li></ul><ul><li><p>Plato&rsquo;s Cave</p><blockquote><p>In Plato’s allegory, a group of prisoners face the wall as a punishment and there are some physical objects behind them which the prisoners cannot see – the prisoners can only see the shadows of these objects on the wall. The shadows are otherwise the “observations” which the prisoners make – the observed variables. The physical objects are the “latent variables” the underlying variables governing the actual behaviour which we cannot directly see in Plato’s cave example.</p></blockquote><figure><img src=/vae/images/2024-04-14_17-22-01_screenshot.png width=600px></figure></li></ul><h4 id=图片的latent-variable>图片的latent variable</h4><ul><li>尽管说，一张图片包含了所有的信息，但是很多的信息是你的人脑反馈分析出来的。<ul><li>比如这个图里面，你会反应出来它有簇，群的概念，这些就是\(Z\)</li></ul></li></ul><figure><img src=/vae/images/2024-03-29_09-31-36_screenshot.png width=400px></figure><ul><li>在人脸的图片中，\(Z\) 可能是肤色，脸型，发型，眼睛，鼻子的形状等</li></ul><h4 id=对的latent-variables的一些假设>对的latent variables的一些假设</h4><ul><li>完整的样本是\((X, Z)\)<ul><li>每个样本\(X\) 对应一个latent variable \(Z\)</li></ul></li><li>但是我们只能观测到\(X\), \(Z\) 是观测不到的, 且一般没有一个明确的定义<ul><li>VAE中的\(Z\) 我们也不知道是feature，只是一种感觉</li></ul></li><li>直接去优化 \(P_{\theta}(X)\) 是困难的, 有积分的存在<ul><li>\(P(X)=\int_Z P_{\theta}(X,Z)dZ=\int_{Z}P_{\theta}(X|Z)P_{\theta}(Z)dZ\)</li></ul></li><li>但是知道了\(Z\) 后，\(P(X,Z)\) 或者 \(P(X|Z)\) 是容易优化的</li></ul><h4 id=基于隐变量的生成模型>基于隐变量的生成模型</h4><ul><li>latent variable 变的至关重要<ul><li>\(Z\rightarrow X\)</li><li>\(Z\) 表达了图像中至关重要的特征</li><li>知道了\(Z\), 整个图像就可以基于\(P(X|Z)\) decode 构建出来。</li><li>我们会把 \(Z\) 当做是 $X$对一个的encoding 向量。</li></ul></li><li>生成：观测值是基于隐变量的值来生成的。<ul><li>先sample \(Z\)</li><li>再基于 \(P(X|Z)\) sample得到 \(X\)</li></ul></li><li>\(P(X,Z)=P(Z)P(X|Z)\)</li></ul><h4 id=一个例子-高斯混合模型>一个例子：高斯混合模型</h4><p>\(P(X)=\sum_{Z}P(Z)P(X|Z)=\sum\limits_{k=1}^{K} \pi_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})\)</p><ul><li>sample过程<ul><li>先根据先验\(P(Z)\) 决定在哪个群落点，</li><li>再根据局部的似然 \(P(X|Z)\) 采样，(根据这个群的均值，方差采样)</li></ul></li><li>直接优化 \(\log P(X)\) 非常困难</li><li>但是 \(P(X,Z)=\prod\limits_{k=1}^K \pi_k^{Z_k}\mathcal{N}(X|\mu_{k}, \Sigma_{k})^{Z_k}\),<ul><li>其中 \(Z=(Z_1, Z_2, \ldots, Z_{k})\) one-hot 形式</li><li>\(\log P(X,Z)=\sum\limits_{k=1}^{K}Z_k[log\pi_k+\log\mathcal{N}(X|\mu_{k}, \Sigma_{k})]\) 容易优化</li></ul></li></ul><figure><img src=/vae/images/2024-03-29_09-31-36_screenshot.png width=400px></figure><ul><li><p>VAE 就是一个高斯混合模型</p><ul><li>假设 \(Z \sim \mathcal{N}(0, I)\)</li><li>假设 \(P(X|Z) \sim \mathcal{N}(\mu(X), \Sigma(X))\)</li><li>\(P(X)=\int P(X|Z)P(Z) dX\)<ul><li>无穷个高斯的混合</li></ul></li></ul></li></ul><h3 id=基于隐变量的图像生成模型>基于隐变量的图像生成模型</h3><h4 id=不是把所有的图像放在一起做一个分布-p--x>不是把所有的图像放在一起做一个分布 \(P(X)\)</h4><ul><li>这样的分布有了可能用处也不大，你在高维sample的时候困难极其的大</li><li>另外你对sample做不了太多的控制</li></ul><h4 id=每个图像可以看做对应一个分布>每个图像可以看做对应一个分布</h4><ul><li>每个图像都可以看做是一个高斯分布下的采样</li><li>按照像素点的当前值，是一个高斯分布，有均值，方差</li><li>如果我们把方差relax以下，我们也可以基于每个图像得到一些新的图像</li></ul><h4 id=从一图一个分布到一个编码一个分布>从一图一个分布到一个编码一个分布</h4><ul><li>一个图像一个分布有点太细了，没法泛化</li><li>对图像\(X\) 来做低维编码\(Z\) ，如果编码相同，那么对应的分布是\(P(X|Z)\) 是相同的。</li><li>这套编码至关重要，有了它，图像的基本特征就决定了，<ul><li>人脸中，它可能包括了性别，肤色，发型，脸型等等</li></ul></li><li>把这个编码空间学好了，你可以对生成的东西做更好的操控</li><li>如果把 \(P(X|Z)\) 学好了<ul><li>那么我直接采样 \(Z\) 就好了，</li><li>这个分布需要很容易sample，首选的还是多维的gaussian 分布</li></ul></li></ul><h3 id=隐变量生成模型的概率建模>隐变量生成模型的概率建模</h3><h4 id=一般的模型的概率建模方式>一般的模型的概率建模方式</h4><ul><li>参数化单个样本\(X\) 的概率<ul><li>\(P_{\theta}(X)\)</li><li>\(\log P_{\theta}(X)\) is easy</li></ul></li><li>最大似然估计 \(\sum_{i} \log P_{\theta}(X_i)\)</li></ul><h4 id=生成式模型的概率建模方式>生成式模型的概率建模方式</h4><ul><li>参数化单个样本<ul><li>\(P_{\theta}(X)=\int P_{\theta}(X|Z)P_{\theta}(Z)dZ\)</li></ul></li><li>VAE:<ul><li>\(P_{\theta}(X|Z)=\mathcal{N}(\mu(Z;\theta),\Sigma(Z;\theta))\)</li><li>\(P_{\theta}(Z)=\mathcal{N}(0,I)\)</li></ul></li><li>\(\log P_{\theta}(X)=\log \int P_{\theta}(X|Z)P_{\theta}(Z)dZ\) ???</li></ul><h4 id=生成式模型的概率建模本质>生成式模型的概率建模本质</h4><ul><li>无穷个高斯模型的混合</li><li>每个图像\(X\)，都有一个对应\(Z\) 编码, 再对应一个该图像的分布</li><li>sample:<ol><li>ancestral sample：\(Z\rightarrow X\)</li><li>VDM \(Z_n\rightarrow Z_{n-1} \rightarrow Z_{n-2} \ldots \rightarrow X\)</li></ol></li></ul><h3 id=隐变量生成模型的优化困难>隐变量生成模型的优化困难</h3><h4 id=优化的目标>优化的目标：</h4><ul><li>\(P(X)=\int_Z P_{\theta}(X|Z)P(Z) dZ\)</li><li>积分的存在，导致 \(\log P(X)\) 无法直接优化</li></ul><h4 id=使用蒙特卡洛方法>使用蒙特卡洛方法？</h4><p>对于一个给定的样本 \(X\)</p><ul><li>写成期望的形式
\(P(X)= E\limits_{Z\sim P(Z)} P_{\theta}(X|Z)\)</li><li>MC<ul><li>sample \(Z_1, Z_2, \ldots, Z_n\) from \(P(Z)\)</li><li>\(P(X) \approx \dfrac{1}{n} \sum_{i} P_{\theta}(X|Z_i)\)</li><li>再针对\(\theta\) 做梯度下降</li></ul></li></ul><h4 id=困难>困难</h4><ul><li>维度灾难的问题：高维空间中的sample 效率很低。</li><li>\(Z_{i}\) 的有效性<ul><li>直接来sample \(Z_{i} \sim P(Z)\)，\(P(X|Z_i)\) 的概率大多为0，可能导致模型一直
error很大，很难拟合样本。</li><li>所以我们要更加有效的 Z, 最好使用 \(P(Z|X)\) 来sample \(Z\)</li><li>但是 \(P(Z|X)\) 是未知的，找 \(Q(Z|X)\) 来近似 $P(Z|X)</li><li>计算 \(E_{Z\sim Q} P(X|Z)\)</li></ul></li></ul><figure><img src=/vae/images/2024-03-29_09-31-36_screenshot.png width=400px></figure><ul><li>\(\log P(X)\) 的问题</li></ul><h3 id=隐变量生成模型的另一个挑战>隐变量生成模型的另一个挑战</h3><ul><li>我们的初衷：\(Z\rightarrow X\)<ul><li>先sample \(Z\sim P(Z)\)</li><li>再sample \(X\sim P(X|Z)\)</li></ul></li><li>\(Z\) 所在的 latent space 应该满足<ul><li>连续性：\(Z\) 连续变化的时候，生成的图像也是在连续变化的。</li><li>完备性: 任意的一个sample \(Z\), 都可以被解码生成一个有意义的图像。</li></ul></li></ul><h4 id=关于完备性>关于完备性：</h4><p>本质是encoding部分\(Q(Z|X)\) 需要和你将来要sample的 \(P(Z)\) 之间是契合兼容的。</p><figure><img src=/vae/images/2024-04-20_10-49-15_screenshot.png width=600px></figure><h2 id=vae>VAE</h2><h3 id=vae-的思路>VAE 的思路</h3><p>\(\log P(X)= \log \int P(X,Z) dZ\)</p><h4 id=思考点一-log穿过积分>思考点一：log穿过积分</h4><ul><li>基于Jensen 不等式，\(\log\) 可以穿过去积分, 得到一个下界<ul><li>积分项中加入分布Q(Z)</li><li>积分变成了期望，log穿过期望</li></ul></li><li>而这个下界是容易优化的， 同时可以对下界优化不断提升</li></ul><p>\(\begin{aligned}
\log P(X)
&= \log \int P(X, Z) dZ\\
&= \log \int P(X, Z) \dfrac{Q(Z)}{Q(Z)}dZ\\
&= \log E_{Q(Z)} \dfrac{P(X,Z)}{Q(Z)}\\
&\ge E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}\\
\end{aligned}\)</p><h4 id=思考点二-提升采样有效性>思考点二：提升采样有效性</h4><ul><li>不从\(P(Z)\) 中sample \(Z\)</li><li>而从\(Q(Z)\) 中sample \(Z\)，\(Q(Z)\rightarrow P(Z|X)\)</li><li>然后基于\(Z\) 重建 \(\widehat{X}=f_{\theta}(Z)\)</li></ul><figure><img src=/vae/images/2024-04-21_11-13-56_screenshot.png width=600px></figure><h3 id=variational-inference>variational inference</h3><p>对于任意的\(Q(Z)\)， 有</p><ul><li>\(\log P(X) = \mathcal{L} (Q) + \mathcal{D}(Q(Z)\|P(Z|X))\)<ul><li>\(\mathcal{L}(Q) = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}\)</li><li>\(\mathcal{D}(Q(Z)\|P(Z|X)) = E_{Q(Z)} \log\dfrac{Q(Z)}{P(Z|X)}\)</li></ul></li><li>\(\log P(X) \ge \mathcal{L} (Q)\) ，由于 KL 非负<ul><li>\(\mathcal{L}(Q)\) 是关于 \(Q(Z)\) 的一个泛函</li><li>参数化: \(Q_{\phi}(Z)\)</li><li>\(\max\limits_{\phi}\mathcal{L}(Q)\) (变分的思想)</li><li>\(\mathcal{L} (Q)\) 就是 ELBO</li></ul></li><li>当\(Q(Z)=P(Z|X)\) 时，\(\log P(X) = \mathcal{L}(Q)\)</li></ul><h4 id=证明>证明</h4><p>\(\begin{aligned}
\log P(X) &= E_{Q(Z)} \log P(X) \\
&=E_{Q(Z)} \log \dfrac{P(X,Z)}{P(Z|X)} \\
&=E_{Q(Z)} \log \dfrac{P(X,Z)}{P(Z|X)} \dfrac{Q(Z)}{Q(Z)} \\
& = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)} + E_{Q(Z)} \log \dfrac{Q(Z)}{P(Z|X)}\\
& = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)} + \mathcal{D}[Q(Z)||P(Z|X)]
\end{aligned}\)</p><h3 id=elbo-重建误差-plus-正则项>ELBO=重建误差+正则项</h3><ul><li>\(\mathcal{L}(Q) = \mathrm{E}_{Q(Z)}[\log P(X|Z)] - \mathcal{D}[Q(Z) \| P(Z)]\)</li><li>一般的，我们会让 \(Q(Z)\) 直接依赖于\(X\), 变成\(Q(Z|X)\)</li><li>\(\mathcal{L}(Q) = \mathrm{E}_{Q(Z|X)}[\log P(X|Z)] - \mathcal{D}[Q(Z|X) \| P(Z)]\)</li></ul><h4 id=理解>理解</h4><ul><li>第一项: reconstruction error<ul><li>对所有可能生成\(X\) 的\(Z\), 把似然性加权平均</li><li>有了encoding,decoding的意思<ul><li>Q(Z|X) 是对 X的一个编码</li><li>P(X|Z) 是对 Z的一个解码</li></ul></li><li>可以使用MC的方法来优化</li></ul></li><li>第二项: 正则项<ul><li>需要同时最小化 Q(Z|X) 和 P(Z) 之间的距离</li><li>一方面：对Q(Z|X) 做一个正则，防止 \(Q(Z|X)\) 变得过于尖锐，得到一个Dirac 分布</li><li>另一方面：我们需要学习到latent space和我们要采样的空间是契合的。</li></ul></li><li>这两项之间需要有一个balance<ul><li><p>这里我们并不希望第二项成为0，否则采样效率又降低回到从前（KL散度消失）</p><p>如果正则项成为0，这将意味着 Q(Z|X) 不含有关于X的任何信息，隐变量失去了
他的数据表征的能力。</p></li></ul></li></ul><h4 id=proof>proof</h4><p>\(\begin{aligned}
\mathcal{L}(Q)
& = E_{Q(Z)}\log\dfrac{P(X,Z)}{Q(Z)}\\
& = E_{Q(Z)}\log\dfrac{P(X|Z)P(Z)}{Q(Z)}\\
& = E_{Q(Z)}\log P(X|Z) + E_{Q(Z)}\log\dfrac{P(Z)}{Q(Z)}\\
& = E_{Q(Z)}\log P(X|Z) - E_{Q(Z)}\log\dfrac{Q(Z)}{P(Z)}\\
\end{aligned}\)</p><h3 id=参数化elbo>参数化ELBO</h3><p>给定一个\(X\), \(\mathcal{L} = \mathrm{E}_{Q(Z)}[\log P(X|Z)] - \mathcal{D}[Q(Z) \| P(Z)]\)</p><h4 id=参数化>参数化</h4><ul><li>\(Q(Z)\) 参数化为 \(Q_{\phi}(Z|X)=\mathcal{N}(Z|\mu_{\phi}(X), \Sigma_{\phi}(X))\)<ul><li>每个样本对应一个z的独立的正态分布</li><li>\(\mu_{\phi}(X)\)，\(\Sigma_{\phi}(X)\) DNN</li></ul></li><li>\(P(X|Z)\) 参数化为 \(P_{\theta}(X|Z)=\mathcal{N}(X|f_{\theta}(Z), I)\)<ul><li>\(f_{\theta}(Z)\) DNN</li></ul></li><li>\(P(Z)=\mathcal{N}(0,1)\)</li></ul><h4 id=结果>结果</h4><p>\(\mathcal{L}(\phi,\theta)=\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\)
我们有两个参数\(\phi, \theta\), \(\phi\) 出现在变分的候选函数里面，\(\theta\) 出现在decoder里面。</p><h3 id=理解elbo>理解ELBO</h3><ul><li>\(\log P_{\theta}(X) = \mathcal{L}(Q, \theta) + \mathcal{D}[Q(Z)\|P(Z|X)}]\)</li></ul><figure><img src=/vae/images/2024-03-29_11-29-54_screenshot.png width=400px></figure><ul><li>对于任意的\(Q(Z)\)，\(\log P_\theta(X)\ge \mathcal{L}(Q,\theta)\)</li></ul><ul><li>给定一个\(\theta\), \(\mathcal{L}(Q(Z), \theta)\) 是一个泛函<ul><li>这也是变分的意义所在，在各种函数中寻找一个最好的。</li></ul></li><li>给定一个\(Q(Z)\), \(\mathcal{L}(Q(Z), \theta)\) 提供了一个\(\theta\) 的函数曲线<ul><li>不断地优化和提升下界 \(Q(Z)\)，下界成为一个代理的优化目标</li><li>通过不多优化下界来更新\(\theta\)</li></ul></li></ul><figure><img src=/vae/images/2024-03-29_11-50-58_screenshot.png></figure><h3 id=vae求解>VAE求解</h3><p>\(\mathcal{L}(\phi,\theta)=\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\)
如何对\(\phi,\theta\) 求导？</p><ul><li>对\(\theta\) 求导，easy</li><li>对\(\phi\) 求导？</li></ul><h4 id=对-phi-求导思路>对\(\phi\) 求导思路</h4><ul><li>第一项期望依赖的分布中有求导对应的参数, 需要重参数化</li><li>第二项可以显式计算，只和\(\phi\) 相关，容易计算梯度
\(\begin{aligned}
&\mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
=& D\left[ \mathcal{N}(\mu_{\phi}(X), \Sigma_{\phi}(X)) \parallel \mathcal{N}(0, I) \right] \\
=& \frac{1}{2} \left( \text{tr}(\Sigma_{\phi}(X)) + (\mu_{\phi}(X))^T (\mu_{\phi}(X)) - k - \log \det (\Sigma_{\phi}(X)) \right)
\end{aligned}\)</li></ul><h4 id=重参数化>重参数化</h4><p>\(\nabla_{\phi} E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)\) ，期望依赖的分布依赖于参数\(\phi\)</p><ul><li><p>重参数化：对随机变量做变量替换</p><ul><li>before： \(Z\sim Q_{\phi}(Z|X)=\mathcal{N}(Z|\mu(X;\phi), \Sigma(X;\phi))\)</li><li>after： \(Z=\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon, \quad \varepsilon \sim N(0,1)\)</li><li>\(E_{Q_{\phi}(Z|X)}\log P(X|Z)=E_{\varepsilon}\log P(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon))\)</li></ul></li><li><p>最后期望的分布不再依赖于参数</p></li><li><p>求导此时可以穿过期望</p><p>\(\begin{aligned}
& \nabla_{\phi} E_{Q_{\phi}(Z|X)}\log P_{\theta}(X|Z)\\
= &\nabla_{\phi} E_{\varepsilon}\log P_{\theta}(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon) \\
= &amp;E_{\varepsilon} \nabla_{\phi} \log P_{\theta}(X|\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon)
\end{aligned}\)</p></li></ul><h4 id=最终的求解算法>最终的求解算法</h4><ul><li><p>最后的优化目标</p><p>\(\begin{aligned}
\mathcal{L} &=
\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
&=\mathrm{E}_{\varepsilon}[\log P_{\theta}(X|Z(\phi,\varepsilon))] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\\
\end{aligned}\)</p></li><li><p>应用MC方法</p><ul><li>sample \(\varepsilon_l \sim N(0,1), Z_l=\mu(X,\phi)+\Sigma^{1/2}(X,\phi)*\varepsilon_{l}\)</li><li>计算ELBO
\(\mathcal{L}(\theta, \phi, X)=\dfrac{1}{L}\sum\limits_{l=1}^{L} \log P_{\theta}(X| Z_{l})} -\mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\)</li><li>对\(\phi, \theta\) 求导</li></ul></li></ul><h3 id=前向的步骤>前向的步骤</h3><figure><img src=/vae/images/2024-04-20_00-13-24_screenshot.png width=600px></figure><h3 id=vae训练好后怎么用>VAE训练好后怎么用？</h3><h4 id=直接生成>直接生成</h4><p>这个时候可以抛弃encoder \(Q(Z|X)\) 了</p><ul><li>sample \(Z\) from \(P(Z)\)</li><li>确定性函数做一个映射 \(f(Z)\)</li></ul><ul><li><p>why？</p><p>因为在优化的过程中 \(Q(Z|X)\) 和 \(P(Z)\) 已经比较靠近，作为优化的第二项</p></li></ul><h4 id=重构原来的图像>重构原来的图像</h4><p>encoder 和decoder 都需要使用</p><ul><li>基于\(Q(Z|X)\) 得到encoding \(Z\)</li><li>基于\(P(X|Z)\) 生成出来 \(\hat{X}\)</li></ul><h3 id=理论的依据>理论的依据</h3><h4 id=高斯分布-plus-cdf逆变换可以拟合任意的分布>高斯分布+CDF逆变换可以拟合任意的分布</h4><ul><li>假设<ul><li>随机变量 \(N\sim \mathcal{N}[0,1]\), 对应的CDF 是\(\Psi\)</li><li>那么\(Y=\Psi(N)\sim \text{Uniform}[0,1]\)</li><li>目标随机变量\(X\) 对应的分布的CDF是 \(F\)</li></ul></li><li>那么随机变量 \(X=F^{-1}(Y)\) 分布满足\(F\)</li></ul><ul><li><p>均匀分布+CDF逆变换可以拟合任意的分布</p><ul><li><p>假设</p><ul><li>随机变量 \(U\sim \text{Uniform}[0,1]\)</li><li>目标随机变量对应的CDF是 \(F(x)=P(X\le x)\)</li></ul></li><li><p>结论：随机变量 \(X=F^{-1}(U)\) 分布满足\(F\)</p></li><li><p>证明:</p><p>\(P(X\le x)=P(F^{-1}(U)\le x)=P(U\le F(x))=F(x)\)</p></li></ul></li></ul><ul><li><p>高斯分布到均匀分布</p><ul><li>假设<ul><li>随机变量 \(N\sim \mathcal{N}[0,1]\), 对应的CDF 是\(\Psi\)</li><li>那么\(Y=\Psi(N)\sim \text{Uniform}[0,1]\)</li></ul></li><li>证明:
\(P(Y\le y)=P(\Psi(N)\le y )=P(N\le \Psi^{-1}(y))=\Psi(\Psi^{-1}(y))=y\)</li></ul></li></ul><ul><li><p>本质：</p><p>如果能在source, target随机变量之间建立一个单调的一一映射的关系，就可以得到target 随机变量的模拟。</p></li></ul><h4 id=在生成式模型中运用>在生成式模型中运用：</h4><p>sample \(X\) 可以分两步走</p><ul><li>先sample \(Z\sim \mathcal{N}(0,1)\)</li><li>然后再基于一个复杂的确定函数变换（交给DNN学习）得到 \(f(Z)\) 变换得到\(X\)</li><li>随机变量 \(X=f(Z)\) 就是对整体的sample建模</li></ul><h4 id=为什么不用均匀分布做先验-而使用高斯>为什么不用均匀分布做先验？而使用高斯？</h4><ul><li>高斯分布在整个空间上有定义，计算KL 不会有除以0的问题发生</li><li>高斯分布有很多很好的性质可以使用</li></ul><h3 id=一些思考>一些思考</h3><h4 id=我们的假设>我们的假设：</h4><ul><li>可以基于隐变量来做sample<ul><li>先sample \(Z\), 然后基于 \(P(X|Z)\) 再sample 出来 \(X\)</li></ul></li><li>各种高斯：<ul><li>\(P(Z)\) gaussian，\(P(Z|X)\) ，\(P(X|Z)\) 都是gaussian， \(P(X,Z)\) 是gaussian</li></ul></li></ul><h4 id=本质的建模-无穷个高斯模型做加权混合>本质的建模：无穷个高斯模型做加权混合</h4><ul><li>\(P(X) = \int P(X|Z)P(Z) dZ \approx \sum_{i} P(X|Z_i) P(Z_{i}) \delta Z\)</li><li>整体看：图像的概率分布是无穷多个高斯分布来做加权的组合<ul><li>且权重的分布是符合高斯分布</li></ul></li><li>微观的看：一个图像只对应于一个高斯分布 \(\mathcal{N}(\mu(Z), \Sigma(Z))\)<ul><li>从生成的角度看，\(Z\) 决定和生成了\(X\)</li></ul></li></ul><h4 id=vae-的生成的图像模糊>VAE 的生成的图像模糊</h4><ul><li>最后的重建误差是所有像素点上的各个误差平均的结果</li><li>重建误差和正则项之间的balance<ul><li>重建误差控制了图像的质量</li><li>正则项让我们向latent space的完备性上倾斜</li></ul></li></ul><h3 id=关于噪音>关于噪音</h3><ul><li>训练的过程中出现了一次\(Z\) 的sample</li><li>\(X\) 是基于一个不确定的，有噪音的东西生成的。</li><li>监督学习中也会有加入噪音的方式来训练模型，提升模型的鲁棒性，抗过拟合。</li></ul><figure><img src=/vae/images/2024-04-20_00-13-24_screenshot.png width=600px></figure><h4 id=open-problems>open problems</h4><ul><li><p>为什么不考虑协方差？如何建模像素之间的依赖呢？</p><ul><li>图像中最重要的一个特征是相互之间的像素的依赖性</li><li>而我们的给出的假设中，\(P(Z), P(X|Z), P(Z|X)\) 统统都去掉了协方差。</li></ul></li></ul><ul><li><p>\(P(Z)\) 可以sample空间这么大，如何保证 \(P(X|Z)\) 生成出来的也是个正常的人脸？</p><ul><li>我们能把latent space 充分的学习到吗？保证它的完备性？</li></ul></li></ul><h2 id=vae代码实现>VAE代码实现</h2><h3 id=代码实现>代码实现</h3><h4 id=model>model</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>VAE_Model</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>VAE</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># 网络的维度是 raw_dim -&gt; hidden_dim -&gt; latent_dim -&gt; hidden_dim -&gt; raw_dim</span>
</span></span><span class=line><span class=cl>        <span class=c1># encoder raw_dim -&gt; hidden_dim -&gt; latent_dim</span>
</span></span><span class=line><span class=cl>        <span class=c1># decoder latent_dim -&gt; hidden_dim -&gt; raw_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_l1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>raw_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_mu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>  <span class=c1># Mean</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_logvar</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>)</span>  <span class=c1># Log variance</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_l1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_l2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>raw_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder_l1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_mu</span><span class=p>(</span><span class=n>h</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_logvar</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>sample_z</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>std_var</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>logvar</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>eps</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>std_var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>mu</span> <span class=o>+</span> <span class=n>eps</span> <span class=o>*</span> <span class=n>std_var</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder_l1</span><span class=p>(</span><span class=n>z</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder_l2</span><span class=p>(</span><span class=n>h</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>raw_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sample_z</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>z</span><span class=p>),</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=loss>loss</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Model, Optimizer, and Loss Function</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>loss_function</span><span class=p>(</span><span class=n>recon_x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算重建的误差，只和recon_x, x 有关</span>
</span></span><span class=line><span class=cl>    <span class=n>RECON</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>recon_x</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>raw_dim</span><span class=p>),</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;sum&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># 计算KL散度 D(P(Z|X)||Q(Z)) 只和 mu, logvar 有关</span>
</span></span><span class=line><span class=cl>    <span class=n>KLD</span> <span class=o>=</span> <span class=o>-</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>logvar</span> <span class=o>-</span> <span class=n>mu</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>-</span> <span class=n>logvar</span><span class=o>.</span><span class=n>exp</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>RECON</span> <span class=o>+</span> <span class=n>KLD</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=train>train</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>VAE_Model</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>train_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>recon_x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_function</span><span class=p>(</span><span class=n>recon_x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=n>logvar</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>train_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=总结>总结</h3><h4 id=生成模型>生成模型</h4><ul><li>概率建模 \(P(X)\)<ul><li>目的是为了采样，生成样本以外的x</li><li>计算似然性</li></ul></li><li>如果直接基于autoregressive model，decode代价太大</li><li>基于隐变量的方式: \(Z\rightarrow X\)</li></ul><h4 id=求解>求解：</h4><ul><li>每个X的背后都有一个Z，但是Z未知，所以需要遍历Z
\(P(X)=\int P(Z)P(X|Z) dZ\)<ul><li>log 很难处理</li><li>很难采样到对X有实质贡献的Z</li></ul></li><li>Jensen 不等式得到了似然函数的变分下界<ul><li>最大似然转化为了对下界的优化上</li><li>\(\log P(X) \ge \mathcal{L} (Q)\)</li><li>\(\mathcal{L}(Q) = E_{Q(Z)} \log \dfrac{P(X,Z)}{Q(Z)}\)</li></ul></li><li>优化下界： \(\mathcal{L}(\phi,\theta)=\mathrm{E}_{Q_{\phi}(Z|X))}[\log P_{\theta}(X|Z)] - \mathcal{D}[Q_{\phi}(Z|X) \|P(Z)]\)<ul><li>下界中有两套参数 \(\phi, \theta\)</li><li>变分相关的参数 \(\phi\), encoder, 不断地提升下界</li><li>似然函数中的\(\theta\), decoder</li></ul></li></ul><h4 id=使用>使用：</h4><ul><li>使用的时候我们会丢掉 Encoder, 使用Decoder</li></ul><h4 id=优化中的独特点>优化中的独特点：</h4><ul><li>对下界来优化</li><li>有两套参数：一个是优化提升下界的，一个是正常的参数</li></ul><h4 id=建模的本质-无穷个高斯和一个高斯>建模的本质：无穷个高斯和一个高斯</h4><p>对于一个图像\(X\)</p><ul><li>如果Z不知道的情况下，X是无穷个高斯分布的组合
\(P(X)=\int P(Z)P(X|Z) dZ\)</li><li>当Z知道的情况下，X只对应唯一一个高斯分布\(\mathcal{N}(\mu(Z), \Sigma(Z))\)</li></ul><h2 id=参考论文>参考论文</h2><style>.csl-entry{text-indent:-1.5em;margin-left:1.5em}</style><div class=csl-bib-body><div class=csl-entry>NO_ITEM_DATA:doerschTutorialVariationalAutoencoders2021</div><div class=csl-entry>NO_ITEM_DATA:kingmaAutoEncodingVariationalBayes2022a</div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2024-04-21</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/vae/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://skylyj.github.io/vae/ data-title=揭秘VAE背后的数学原理><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://skylyj.github.io/vae/><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Hacker News" data-sharer=hackernews data-url=https://skylyj.github.io/vae/ data-title=揭秘VAE背后的数学原理><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://skylyj.github.io/vae/ data-title=揭秘VAE背后的数学原理><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://skylyj.github.io/vae/ data-title=揭秘VAE背后的数学原理 data-ralateuid=xxxx><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/configure_emacs/ class=next rel=next title=Emacs配置文件>Emacs配置文件<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript><div id=giscus class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app>Giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.125.2">Hugo</a> 强力驱动</div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><section><span id=busuanzi_container_value_site_pv><i class="far fa-eye fa-fw"></i>
<span id=busuanzi_value_site_pv></span>
</span>&nbsp;|&nbsp;
<span id=busuanzi_container_value_site_uv><i class="fa fa-user"></i>
<span id=busuanzi_value_site_uv></span></span></section></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:50},comment:{giscus:{category:"General",categoryId:"DIC_kwDOLM9Aus4Cc5HY",darkTheme:"dark_dimmed",emitMetadata:"0",inputPosition:"bottom",lang:"zh-CN",lazyLoading:!1,lightTheme:"light",mapping:"pathname",reactionsEnabled:"1",repo:"skylyj/Giscus",repoId:"R_kgDOLM9Aug"},valine:{appId:"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI",appKey:"WBmoGyJtbqUswvfLh6L8iEBr",avatar:"mp",el:"#valine",emojiCDN:"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/",emojiMaps:{100:"1f4af.png",alien:"1f47d.png",anger:"1f4a2.png",angry:"1f620.png",anguished:"1f627.png",astonished:"1f632.png",black_heart:"1f5a4.png",blue_heart:"1f499.png",blush:"1f60a.png",bomb:"1f4a3.png",boom:"1f4a5.png",broken_heart:"1f494.png",brown_heart:"1f90e.png",clown_face:"1f921.png",cold_face:"1f976.png",cold_sweat:"1f630.png",confounded:"1f616.png",confused:"1f615.png",cry:"1f622.png",crying_cat_face:"1f63f.png",cupid:"1f498.png",dash:"1f4a8.png",disappointed:"1f61e.png",disappointed_relieved:"1f625.png",dizzy:"1f4ab.png",dizzy_face:"1f635.png",drooling_face:"1f924.png",exploding_head:"1f92f.png",expressionless:"1f611.png",face_vomiting:"1f92e.png",face_with_cowboy_hat:"1f920.png",face_with_hand_over_mouth:"1f92d.png",face_with_head_bandage:"1f915.png",face_with_monocle:"1f9d0.png",face_with_raised_eyebrow:"1f928.png",face_with_rolling_eyes:"1f644.png",face_with_symbols_on_mouth:"1f92c.png",face_with_thermometer:"1f912.png",fearful:"1f628.png",flushed:"1f633.png",frowning:"1f626.png",ghost:"1f47b.png",gift_heart:"1f49d.png",green_heart:"1f49a.png",grimacing:"1f62c.png",grin:"1f601.png",grinning:"1f600.png",hankey:"1f4a9.png",hear_no_evil:"1f649.png",heart:"2764-fe0f.png",heart_decoration:"1f49f.png",heart_eyes:"1f60d.png",heart_eyes_cat:"1f63b.png",heartbeat:"1f493.png",heartpulse:"1f497.png",heavy_heart_exclamation_mark_ornament:"2763-fe0f.png",hole:"1f573-fe0f.png",hot_face:"1f975.png",hugging_face:"1f917.png",hushed:"1f62f.png",imp:"1f47f.png",innocent:"1f607.png",japanese_goblin:"1f47a.png",japanese_ogre:"1f479.png",joy:"1f602.png",joy_cat:"1f639.png",kiss:"1f48b.png",kissing:"1f617.png",kissing_cat:"1f63d.png",kissing_closed_eyes:"1f61a.png",kissing_heart:"1f618.png",kissing_smiling_eyes:"1f619.png",laughing:"1f606.png",left_speech_bubble:"1f5e8-fe0f.png",love_letter:"1f48c.png",lying_face:"1f925.png",mask:"1f637.png",money_mouth_face:"1f911.png",nauseated_face:"1f922.png",nerd_face:"1f913.png",neutral_face:"1f610.png",no_mouth:"1f636.png",open_mouth:"1f62e.png",orange_heart:"1f9e1.png",partying_face:"1f973.png",pensive:"1f614.png",persevere:"1f623.png",pleading_face:"1f97a.png",pouting_cat:"1f63e.png",purple_heart:"1f49c.png",rage:"1f621.png",relaxed:"263a-fe0f.png",relieved:"1f60c.png",revolving_hearts:"1f49e.png",right_anger_bubble:"1f5ef-fe0f.png",robot_face:"1f916.png",rolling_on_the_floor_laughing:"1f923.png",scream:"1f631.png",scream_cat:"1f640.png",see_no_evil:"1f648.png",shushing_face:"1f92b.png",skull:"1f480.png",skull_and_crossbones:"2620-fe0f.png",sleeping:"1f634.png",sleepy:"1f62a.png",slightly_frowning_face:"1f641.png",slightly_smiling_face:"1f642.png",smile:"1f604.png",smile_cat:"1f638.png",smiley:"1f603.png",smiley_cat:"1f63a.png",smiling_face_with_3_hearts:"1f970.png",smiling_imp:"1f608.png",smirk:"1f60f.png",smirk_cat:"1f63c.png",sneezing_face:"1f927.png",sob:"1f62d.png",space_invader:"1f47e.png",sparkling_heart:"1f496.png",speak_no_evil:"1f64a.png",speech_balloon:"1f4ac.png","star-struck":"1f929.png",stuck_out_tongue:"1f61b.png",stuck_out_tongue_closed_eyes:"1f61d.png",stuck_out_tongue_winking_eye:"1f61c.png",sunglasses:"1f60e.png",sweat:"1f613.png",sweat_drops:"1f4a6.png",sweat_smile:"1f605.png",thinking_face:"1f914.png",thought_balloon:"1f4ad.png",tired_face:"1f62b.png",triumph:"1f624.png",two_hearts:"1f495.png",unamused:"1f612.png",upside_down_face:"1f643.png",weary:"1f629.png",white_frowning_face:"2639-fe0f.png",white_heart:"1f90d.png",wink:"1f609.png",woozy_face:"1f974.png",worried:"1f61f.png",yawning_face:"1f971.png",yellow_heart:"1f49b.png",yum:"1f60b.png",zany_face:"1f92a.png",zipper_mouth_face:"1f910.png",zzz:"1f4a4.png"},enableQQ:!1,highlight:!0,lang:"zh-CN",pageSize:10,placeholder:"你的评论 ...",recordIP:!0,serverURLs:"https://leancloud.hugoloveit.com",visitor:!0}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"PASDMWALPK",algoliaIndex:"index.zh-cn",algoliaSearchKey:"b42948e51daaa93df92381c8e2ac0f93",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>