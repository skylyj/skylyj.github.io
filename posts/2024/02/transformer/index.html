<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>深入理解transformer | 连博讲AI</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta name=generator content="Hugo 0.122.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=https://skylyj.github.io/posts/2024/02/transformer/><meta property="og:title" content="深入理解transformer"><meta property="og:description" content="transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta property="og:type" content="article"><meta property="og:url" content="https://skylyj.github.io/posts/2024/02/transformer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-01T20:06:00+08:00"><meta property="article:modified_time" content="2024-02-01T20:16:46+08:00"><meta itemprop=name content="深入理解transformer"><meta itemprop=description content="transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta itemprop=datePublished content="2024-02-01T20:06:00+08:00"><meta itemprop=dateModified content="2024-02-01T20:16:46+08:00"><meta itemprop=wordCount content="1245"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="深入理解transformer"><meta name=twitter:description content="transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">连博讲AI</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">深入理解transformer</h1><time class="f6 mv4 dib tracked" datetime=2024-02-01T20:06:00+08:00>February 1, 2024</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id=transformer背景>transformer背景</h2><h3 id=主要内容>主要内容</h3><ul><li>参考<ul><li><ol start=2017><li>(引用 106576)
Attention Is All You Need</li></ol></li><li><ol start=2020><li>(引用 975)
Fast Autoregressive Transformers with Linear Attention</li></ol></li><li><a href=https://github.com/karpathy/minGPT/tree/master/mingpt>mingpt by karpathy</a></li></ul></li><li>主要内容<ul><li>transformer 的设计推演</li><li>transformer 的代码讲解</li><li>transformer的参数和运算量</li><li>linear attention</li></ul></li></ul><h3 id=矩阵知识>矩阵知识</h3><h4 id=why>why</h4><ul><li>原文直接从整个矩阵作用出发
\[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\
\mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i)
\end{aligned}\]</li><li>从行向量的角度更容易理解</li></ul><h4 id=矩阵和行向量>矩阵和行向量</h4><ul><li>矩阵
\(X\in R^{N\times F}\)
\(X=\begin{pmatrix}
X_{11}, X_{12},\ldots, X_{1F} \\
X_{21}, X_{22},\ldots, X_{2F} \\
\vdots\\
X_{N1}, X_{N2},\ldots, X_{NF}
\end{pmatrix}\)</li><li>行向量
\(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\)</li><li>分块矩阵
\(X=\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}\)</li></ul><ul><li><p>例子</p><ul><li>\(N\) 个token，\(F\) 是embedding的维度</li><li>每行对应于一个token的embedding 行向量</li></ul><p>\(tokens=\begin{pmatrix}
hello \\
world \\
\\
\\
\end{pmatrix}
\rightarrow X=\begin{pmatrix}
[0.59, 0.20, 0.04, 0.96] \\
[0.96, 0.30, 0.16, 0.63] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25]
\end{pmatrix}\)</p></li></ul><h4 id=矩阵相乘和算子作用>矩阵相乘和算子作用</h4><ul><li>定义线性算子 \(\mathcal{A}\)<ul><li>可以作用到行向量 \(\mathcal{A}(X_i) = X_{i} A\)</li><li>也可以作用到矩阵 \(\mathcal{A}(X) = XA\)</li></ul></li><li>右乘矩阵等于对每个行向量逐个施加行变换
\(XA=\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}A=
\begin{pmatrix}
X_1 A\\
X_2 A\\
\vdots\\
X_N A
\end{pmatrix}=
\begin{pmatrix}
\mathcal{A}(X_1) \\
\mathcal{A}(X_2) \\
\vdots\\
\mathcal{A}(X_N)
\end{pmatrix}=\mathcal{A}(X)\)</li><li>代码对应于 nn.Linear</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(in_features<span style=color:#f92672>=</span>F, out_features<span style=color:#f92672>=</span>F)
</span></span><span style=display:flex><span>X_i <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>print(linear(X_i)<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(linear(X)<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><ul><li>算子是对矩阵乘法的一种物理理解<ul><li>旋转矩阵
\(R(\theta)=\begin{pmatrix}
cos\theta& sin\theta\\
-sin\theta& cos\theta\\end{pmatrix}\)</li><li>缩放变换
\(R(\lambda_1,\lambda_2)=\begin{pmatrix} \lambda_1 & \\
& \lambda_2 \\ \end{pmatrix}\)</li></ul></li></ul><h4 id=transformer中的-qk-t-v>transformer中的\(QK^{T}V\)</h4><ul><li>\(S=QK^T\) = 行向量两两计算点积相似性</li></ul><p>$\begin{pmatrix}
Q1
Q2
\vdots
Q_N
\end{pmatrix}</p><p>\begin{pmatrix}
K_{1}^T, K_2^T,\ldots,K_N^T\\
\end{pmatrix}=(Q_{i}K_j^T)_{ij}=S$</p><ul><li>$SV$ = 对行向量做加权求和
$\begin{pmatrix}
S_{11},S_{12},\ldots, S_{1N}\\
S_{21},S_{22},\ldots, S_{2N}\\
\vdots\\
S_{N1},S_{N2},\ldots, S_{NN}\\
\end{pmatrix}</li></ul><p>\begin{pmatrix}
Q_{1}\\
Q_{2}\\
\vdots\\
Q_N
\end{pmatrix}</p><p>=(QiK_j^T)ij=S$</p><h4 id=代码>代码</h4><ul><li>pytorch/tensorflow中的代码都是按照作用于行向量来组织的</li><li>nn.Linear 作用于行向量</li><li>nn.Embedding 按照行向量来组织数据</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>N <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>embed <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>30</span>, F)
</span></span><span style=display:flex><span>idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> embed(idx)
</span></span><span style=display:flex><span>print(X<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><h4 id=注>注</h4><ul><li>左乘以一个矩阵相当于对每个列向量来施加变化</li><li>论文：一般会有行/列向量两种表示方式</li><li>代码：基本都是行向量来作为数据组织的标准</li><li>本文:<ul><li>向量都按照行向量的形式来组织</li><li>按照作用于单个行向量的方式来讲解transformer</li></ul></li></ul><h3 id=encoder-decoder>encoder-decoder</h3><ul><li>大部分的s2s 的任务建模为 encoder-decoder的结构<ul><li>机器翻译，语音识别，文本摘要，问答系统等</li></ul></li><li>encoder<ul><li>把token序列\((x_{1}, x_2,\ldots, x_N)\) 转化为语义向量序列 \((Y_{1}, Y_2, \ldots, Y_N)\)</li><li>一般组织为多层的网络的形式<ul><li>第一层：基础语义向量序列
\((x_{1}, x_2,\ldots, x_N)\rightarrow (X_{1}, X_2,\ldots, X_N)\)</li><li>其它层：高阶语义向量序列
\((X_{1}, X_2,\ldots, X_N)\rightarrow (Y_{1}, Y_2,\ldots, Y_N)\)</li></ul></li></ul></li><li>decoder
基于\((Y_{1}, Y_2, \ldots, Y_N)\) 自回归式的逐个token解码</li></ul><p>focus到 encoder部分来理解transformer</p><h3 id=低阶到高阶语义向量的转换>低阶到高阶语义向量的转换</h3><p>寻找算子 \(\mathcal{T}\) 将低阶的语义向量序列变换为高阶的语义向量序列
\(\mathcal{T}\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}
\rightarrow\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}\)</p><ul><li>输入: \(X\) 低阶语义向量序列，输出: \(Y\) 高阶语义向量序列</li><li>意义<ul><li>\(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)</li><li>对低阶语义向量做加工组合处理和抽象，变换为一个高阶的语义向量序列</li><li>高阶语义向量考虑了 <em>上下文</em> 的语义向量表达</li></ul></li><li>motivation<ul><li><ol start=1957><li>Firth</li></ol><blockquote><p>a word is characterized by the company it keeps.</p></blockquote><p>例子：</p><blockquote><p>The <strong>enigmatic</strong> smile on Mona Lisa&rsquo;s face has intrigued art enthusiasts for centuries, leaving them to speculate about its true meaning.</p></blockquote></li></ul></li><li>用矩阵变换表达 \(Y=\mathcal{T}(X)\)<ul><li>\(X \in R^{N\times F}\), \(Y=\mathcal{T}(X): \quad R^{N\times F}\rightarrow R^{N\times F}\)</li><li>这个算子天然可以复合嵌套，形成多层的网络结构
\(Y=\mathcal{T}_{L}\circ \mathcal{T}_{L-1}\circ \ldots \circ \mathcal{T}_{1}(X)\)</li></ul></li></ul><h3 id=核心的问题>核心的问题</h3><h4 id=问题>问题</h4><p>如何设计 \(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)</p><ul><li>\(Y_{1}, \ldots, Y_N\) 能否并行得到</li><li>\(Y_{i}\) 能否高效的建立起远程的依赖</li></ul><h4 id=rnn>RNN</h4><figure><img src=images/2024-01-18_14-03-26_screenshot.png width=600px></figure><ul><li>递归语义序列 \(Y_{0}\rightarrow Y_1 \rightarrow \ldots \rightarrow Y_{N}\)</li><li>\(Y_{i}=tanh(X_{i}W + Y_{i-1}U)\)</li><li>串行</li><li>单方向的依赖关系，间接</li></ul><h4 id=cnn>CNN</h4><figure><img src=images/2024-01-18_14-04-23_screenshot.png width=600px></figure><ul><li>\(Y_{i}=(X_{i-1},X_i, X_{i+1}) W\) 假设窗口宽度是3</li><li>并行</li><li>长距离依赖？<ul><li>一层卷积只能依赖于当前窗口内，不能对窗口外的形成依赖。</li></ul></li></ul><h4 id=transformer思路>transformer思路</h4><p>设计\(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)，使得</p><ul><li>使得 \(Y_{1},\ldots, Y_N\) 可以做并行计算</li><li>同时解决长距离依赖的问题</li></ul><figure><img src=images/2024-01-18_14-13-40_screenshot.png width=400px></figure><p>\(Y=\mathcal{F}\circ \mathcal{A}(X)\) 做两次矩阵的变换</p><ul><li><p>\(Y=\mathcal{A}(X)\) MultiHead Attention</p><ul><li>高阶的语义等于对 <em>全部</em> 的低阶语义向量基于 <em>相似性(Attention)</em> 做 <em>加权平均</em></li><li>\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\]</li><li>attention = 相似性</li></ul></li><li><p>\(Y&rsquo;=\mathcal{F}(Y)\) Position-wise Feedforward</p><ul><li>再施加若干非线性变换</li></ul></li></ul><h2 id=tranformer网络结构>tranformer网络结构</h2><h3 id=基于kv查询的相似性计算>基于KV查询的相似性计算</h3><p>\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\]</p><h4 id=如何来定义相似性>如何来定义相似性</h4><ul><li>\(sim(X_{i}, X_j)= \mathrm{exp}(\dfrac{X_i X_{j}^T}{\sqrt{D}})\)</li><li>所有的正的kernel函数都可以</li></ul><h4 id=直接计算相似性>直接计算相似性？</h4><ul><li>参数太少</li><li>投影到别的空间来计算相似度 \(X_{i}\rightarrow X_iW\)
\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(X_iW_1,X_jW_{2}) X_jW_3}{\sum_{j=1}^N sim(X_iW_1,X_jW_2)} \\end{aligned}\]</li></ul><h4 id=基于kv查询理解>基于KV查询理解</h4><ul><li>把\(X_i\) 投影出三个向量 \(Q_i,K_i,V_i\)</li><li>QKV<ul><li>KV 是大家熟悉的key-value存储 \(K_{j}\rightarrow V_{j}\)</li><li>Q 是查询使用的query向量 \(Q_{i}\)</li></ul></li><li>QKV的查询方法<ol><li><p>query查询多个key，获取多个value</p></li><li><p>最后把这些value加权平均</p><p>\(Q_i\Rightarrow \begin{pmatrix}
K_{1}\rightarrow V_{1}\\
K_2\rightarrow V_2\\
\vdots\\
K_N\rightarrow V_N
\end{pmatrix}
\Rightarrow \begin{pmatrix}
sim(Q_i,K_1)V_{1} \\
sim(Q_i,K_2)V_{2} \\
\vdots\\
sim(Q_i,K_N)V_N
\end{pmatrix}\Rightarrow\sum_{j=1}^N sim(Q_i,K_j)V_j\)</p></li></ol></li><li>\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\]</li><li>参数： 对应于\(Q,K,V\) 产生了三个投影矩阵矩阵 \(W_{Q}, W_K,W_V\)</li></ul><h3 id=在一个低维空间做attention>在一个低维空间做attention</h3><h4 id=单个头的attention>单个头的attention</h4><ul><li><p>把\(X_{i}\) 从\(F\) 维空间投影到\(D\) 维空间
\(W_{Q}\in R^{F\times D}, W_K\in R^{F\times D}, W_{V} \in R^{F\times M}\)
\(Q_i &= X_iW_{Q}, \quad K_i &= X_iW_{K}, \quad V_i &= X_iW_{V}\)</p></li><li><p>\(Q_i\) 和所有的\(K_j\) 做基于点积的相似度计算，
这里简单起见，我们省略掉了scaling \(\frac{1}{\sqrt{D}}\)</p><p>\(Q_iK^{T}=Q_i(K^T_1, \ldots, K^T_N)=(Q_iK^T_1, \ldots, Q_iK^T_N)\)</p></li><li><p>对相似度的分布做softmax</p><p>\(S=\mathrm{soft}(Q_iK^T_1, \ldots, Q_iK^T_N)=(s_{i1},\ldots, s_{iN})\)</p><p>\(s_{i,j}= \dfrac{exp(Q_iK_j^T)}{\sum_{j=1}^N exp(Q_iK_j^T)}\)</p></li><li><p>加权平均
\(\mathcal{A}(X_i)=\sum_{j=1}^Ns_jV_j=(s_{i1},\ldots, s_{iN})\begin{pmatrix}V_1\\V_2\\vdots\\V_N\end{pmatrix}\)
\(\mathcal{A}(X_i) &= \mathrm{soft}(Q_iK^{T})V = \mathrm{soft}(X_iW_QW_K^TX^T)XW_V\)</p></li></ul><h4 id=矩阵表达>矩阵表达</h4><p>\[\begin{aligned}Y&=\mathcal{A}(X)=\begin{pmatrix}
\mathcal{A}(X_1)\\
\mathcal{A}(X_2)\\
\vdots\\
\mathcal{A}(X_N)
\end{pmatrix}=\begin{pmatrix}
\mathrm{soft}(Q_1K^T)V\\
\mathrm{soft}(Q_2K^T)V\\
\vdots \\
\mathrm{soft}(Q_NK^T)V\end{pmatrix}\\
&=\mathrm{soft}(QK^T)V=\mathrm{soft}(XW_QW_K^TX^T)XW_V\end{aligned}\]
简化符号 \(sim(Q,K)V\)</p><h4 id=代码实现>代码实现</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.nn <span style=color:#f92672>import</span> functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SingleHeadAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>      super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>F <span style=color:#f92672>=</span> config[<span style=color:#e6db74>&#34;fea_size&#34;</span>] <span style=color:#75715e>#F</span>
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>D <span style=color:#f92672>=</span> config[<span style=color:#e6db74>&#34;subspace_dim&#34;</span>] <span style=color:#75715e>#D</span>
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>q_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>F, self<span style=color:#f92672>.</span>D)
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>k_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>F, self<span style=color:#f92672>.</span>D)
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>v_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>F, self<span style=color:#f92672>.</span>D)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>      B, N, F <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size()
</span></span><span style=display:flex><span>      q <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>q_proj(x)
</span></span><span style=display:flex><span>      k <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>k_proj(x)
</span></span><span style=display:flex><span>      v <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>v_proj(x)
</span></span><span style=display:flex><span>      att <span style=color:#f92672>=</span> (q <span style=color:#f92672>@</span> k<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(k<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>      att <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(att, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>      y <span style=color:#f92672>=</span> att <span style=color:#f92672>@</span> v
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>return</span> y
</span></span></code></pre></div><h4 id=注>注:</h4><ol><li>\(D\neq F\) 时，\(\mathcal{A}(X)\) 还不可用</li></ol><h3 id=在多个低维空间做attention>在多个低维空间做attention</h3><h4 id=why>why</h4><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p></blockquote><ul><li>一词多义</li><li>把\(F\) 维的语义向量投影到 \(H\) 个不同的子空间中去计算相似加权组合</li></ul><h4 id=做法>做法</h4><ul><li>每个头投做独立的Attention变换 \(\mathcal{A}^{h}(X)\)<ul><li>假设有\(H\) 个头，每个头作用的低维空间维度是\(D\)</li><li>\(D\times H = F\)</li></ul></li><li>对\(H\) 个 \(D\) 行向量拼接
\(W_O\in R^{F\times F}\)
\(\mathcal{A}(X) = \mathrm{concat}(\mathcal{A}^1(X), \mathcal{A}^2(X), \ldots, \mathcal{A}^{H}(X) W_O\)</li><li>或者对前面的符号简化<ul><li>在第\(j\) 个子空间做单头注意力 \(Y^{j}=sim(Q^{j}, K^{j})V^{j}\)</li><li>合并 \(Y=(Y^{1},\ldots, Y^H)\)</li></ul></li></ul><h4 id=代码实现>代码实现</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 参考 https://github.com/karpathy/minGPT/tree/master/mingpt</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.nn <span style=color:#f92672>import</span> functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SelfAttention</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>      super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>H <span style=color:#f92672>=</span> config[<span style=color:#e6db74>&#34;n_head&#34;</span>]
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>F <span style=color:#f92672>=</span> config[<span style=color:#e6db74>&#34;fea_size&#34;</span>] <span style=color:#75715e>#F</span>
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>D <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fea_size <span style=color:#f92672>//</span> self<span style=color:#f92672>.</span>n_head <span style=color:#75715e>#D</span>
</span></span><span style=display:flex><span>      <span style=color:#75715e># 一次把qkv 全部映射完成，对应W_Q, W_K, W_V</span>
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>qkv_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>fea_size, <span style=color:#ae81ff>3</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>      <span style=color:#75715e># 最后的投影，对应于 $W_O$</span>
</span></span><span style=display:flex><span>      self<span style=color:#f92672>.</span>out_proj <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>fea_size, self<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>      B, N, fea_size <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size()
</span></span><span style=display:flex><span>      q, k, v <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>qkv_proj(x)<span style=color:#f92672>.</span>split(<span style=color:#ae81ff>3</span>, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>      <span style=color:#75715e># matmul 只能在最后两个维度相乘，需要对NxD的矩阵相乘，做1,2维度的交换</span>
</span></span><span style=display:flex><span>      k <span style=color:#f92672>=</span> k<span style=color:#f92672>.</span>view(B, N, self<span style=color:#f92672>.</span>H, self<span style=color:#f92672>.</span>D)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>      q <span style=color:#f92672>=</span> q<span style=color:#f92672>.</span>view(B, N, self<span style=color:#f92672>.</span>H, self<span style=color:#f92672>.</span>D)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>      v <span style=color:#f92672>=</span> v<span style=color:#f92672>.</span>view(B, N, self<span style=color:#f92672>.</span>H, self<span style=color:#f92672>.</span>D)<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>      <span style=color:#75715e># 一次把多个头的映射全部完成</span>
</span></span><span style=display:flex><span>      att <span style=color:#f92672>=</span> (q <span style=color:#f92672>@</span> k<span style=color:#f92672>.</span>transpose(<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>sqrt(k<span style=color:#f92672>.</span>size(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>      att <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(att, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>      y <span style=color:#f92672>=</span> att <span style=color:#f92672>@</span> v
</span></span><span style=display:flex><span>      <span style=color:#75715e># 多头拼接</span>
</span></span><span style=display:flex><span>      y <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>transpose(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(B, N, F)
</span></span><span style=display:flex><span>      y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>out_proj(y)
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>return</span> y
</span></span></code></pre></div><h4 id=代码示意>代码示意</h4><figure><img src=images/2024-01-31_11-11-07_screenshot.png width=600px></figure><h3 id=位置无关的全连接>位置无关的全连接</h3><ul><li>两层的全连接
\(\mathcal{F}(X_i)=(g(X_iW_1)+b_1)W_2+b_2)\)</li></ul><h4 id=代码>代码</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PWiseFeedForward</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fea_size <span style=color:#f92672>=</span> config[<span style=color:#e6db74>&#34;fea_size&#34;</span>]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>proj_wide <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>fea_size, <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>proj_narrow <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>fea_size, self<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>act <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>proj_narrow(self<span style=color:#f92672>.</span>act(self<span style=color:#f92672>.</span>proj_wide(x)))
</span></span></code></pre></div><h3 id=归一化-plus-残差网络>归一化 + 残差网络</h3><p>\(\mathcal{T}(X)=\mathcal{F}\circ\mathcal{A}(X)\)</p><h4 id=layer-normalization>Layer Normalization</h4><p>\(\mathcal{A}&rsquo;(X)=\mathcal{N}\circ\mathcal{A}(X)\)
\(\dfrac{x-\mu}{\sqrt{\sigma}}\gamma + \beta,\mu=\dfrac{1}{d}\sum\limits_{i=1}^{d}x_{i}, \sigma=\sqrt{\dfrac{1}{d}\sum\limits_{i=1}^{d}(x_{i}-\mu)^{2}}\)
可以看成是作用在行向量上的算子</p><h4 id=输入矩阵例子>输入矩阵例子</h4><p>\(\begin{pmatrix}
hello \\
world \\
\\
\\
\end{pmatrix}
\rightarrow X= \begin{pmatrix}
[0.59, 0.20, 0.04, 0.96] \\
[0.96, 0.30, 0.16, 0.63] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25]
\end{pmatrix}\)</p><h4 id=行归一化-or-列归一化>行归一化 or 列归一化</h4><ul><li>在NLP的序列建模里面，Layer Normalization</li><li>在CV/CTR预估里面, Batch Normalization</li></ul><h4 id=why>Why</h4><ul><li>padding的影响
不同batch中&lt;pad>个数不同，沿着token方向做归一化没有意义</li><li>每个位置做独立的归一化更有意义</li></ul><h4 id=其他的可能选择>其他的可能选择</h4><ul><li>RMSNorm
\(\dfrac{x}{RMS(x)}, RMS(x)=\sqrt{\dfrac{1}{d}\sum\limits_{i=1}^{d}x_i^2\)</li></ul><h3 id=整体的变换>整体的变换</h3><p>\(Y=\mathcal{T}(X)\)</p><ol><li>Attention \(Z=\mathcal{N}\circ(X+\mathcal{A}(X))\)</li><li>位置无关的全连接 \(Y=\mathcal{N}\circ(X+\mathcal{F}(Z))\)</li></ol><h4 id=residual-network>residual network</h4><p>\(\mathcal{A}&rsquo;(X)=\mathcal{N}\circ(X+\mathcal{A}(X))\)
\(\mathcal{F}&rsquo;(X)=\mathcal{N}\circ(X+\mathcal{F}(X))\)</p><h4 id=多层>多层</h4><p>一个 \(L\) 层的transformer 模型</p><p>\begin{equation*}
\begin{split}
\mathcal{T}(X) & = \mathcal{T}_L \circ \ldots \mathcal{T}_{2}\circ \mathcal{T}_{1}(X)
\end{split}
\end{equation*}</p><h4 id=代码>代码</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Block</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, config):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>attn <span style=color:#f92672>=</span> SelfAttention(config)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layer_norm_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LayerNorm(config<span style=color:#f92672>.</span>fea_size)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>mlp <span style=color:#f92672>=</span> PWiseFeedForward(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm_1(x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>attn(x))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_norm_2(x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>mlp(x))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><h2 id=transformer参数和计算量>transformer参数和计算量</h2><h3 id=关于参数量>关于参数量</h3><ul><li>我们需要一种模型能够方便的去增加模型的复杂度<ul><li>比如增加深度，增加宽度</li><li>增加token的embedding size</li><li>增加词典的大小</li></ul></li><li>transformer模型可以在此之外非常有效的提升模型的参数量</li><li>而且在参数量提升之后效果也有了巨大的提升</li></ul><h3 id=参数的分布>参数的分布</h3><h4 id=多头注意力-4f-2>多头注意力 \(4F^2\)</h4><ul><li>每个头有<ul><li>3个投影矩阵 \(W_Q, W_K, W_V\)</li><li>1个投影concat结果的矩阵 \(W_O\)</li></ul></li><li>参数量: 假设投射到的子空间维度是\(D\), $H$个子空间，\(D\times H = F\)<ul><li>\(F\times D \times 3 \times H = 3F^{2}\)</li><li>\(F^{2}\)</li></ul></li></ul><h4 id=ffw-8f-2>FFW \(8F^2\)</h4><ul><li>两个矩阵，先从\(F\) 变宽到\(4F\)，再收窄回来到\(F\)</li><li>参数量\(F\times4F + 4F\times F= 8F^{2}\)</li></ul><h4 id=word-embedding>word embedding</h4><p>\(E\) 是token字典的大小</p><ul><li>\(E\times F\)</li></ul><h4 id=total>total</h4><p>\(L(12F^{2})+EF\)</p><table><thead><tr><th>model</th><th>维度</th><th>层数</th><th>头数</th><th>字典大小</th><th>参数量</th></tr></thead><tbody><tr><td>bertBase</td><td>768</td><td>12</td><td>12</td><td>30000</td><td>110M</td></tr><tr><td>bertLarge</td><td>1024</td><td>24</td><td>12</td><td>30000</td><td>340M</td></tr></tbody></table><h3 id=linear-transformer>linear transformer</h3><h4 id=两个算子的计算量>两个算子的计算量</h4><ul><li>\(\mathcal{A}(X)\) 计算量 \(O(N^2)\)</li><li>\(\mathcal{F}(X)\) 计算量 \(O(N)\)</li></ul><h4 id=softmax-导致了-o--n-2>softmax 导致了\(O(N^2)\)</h4><p>核心的计算量在这三个矩阵的相乘上，\(QK^{T}V\)</p><ul><li><p>有softmax的存在的话
只能先计算\(H=QK^{T}\), 对\(H\) 做softmax 变换后，再计算\(HV\)
乘法的计算量是 \(N^2D+N^2M\), 整体的复杂度是\(O(N^{2})\)
\(QK^TV=(QK^T)V=\begin{pmatrix}
H_{11},H_{12},\ldots,H_{1N} \\
\vdots\\
H_{N1},H_{N2},\ldots,H_{NN} \\
\end{pmatrix}V\)</p></li><li><p>如果没有softmax的话
可以先计算后两个矩阵相乘\(H=K^TV\), 再计算\(QH\)
计算量可以是\(O(N)\), 因为\(K^TV\) 可以提前算出来缓存，大致如下面这个表达所示
\(Q(K^TV)=\begin{pmatrix}
Q_1 \\
Q_2 \\
\vdots\\
Q_{N}
\end{pmatrix}(K^TV)\)</p></li></ul><h4 id=kernel>kernel</h4><p>\(\mathcal{A}(X_i)=\dfrac{\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\sum_{j=1}^N sim(Q_i,K_j)}\)</p><ul><li>kernel: \(k(x,y)=&lt;\phi(x),\phi(y)>\)
\(k(x,y)=(x\cdot z)^2, \phi(x)=(x_{1}^{2},x_{2}^2,\sqrt{2}x_1x_{2})\)<ul><li>kernel 对应一个feature map</li><li>可以用非负的kernel来替换掉</li><li>当前的sim函数 \(sim(x,y)=\mathrm{exp}(xy^{T}/\sqrt{D})\)</li></ul></li></ul><h4 id=linear-transformer-o--n>linear transformer \(O(N)\)</h4><ul><li>用kernel来替换掉sim
\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\sum_{j=1}^N sim(Q_i,K_j)} \\
&=\frac{\sum_{j=1}^{N} \phi(Q_i)\phi(K_j)^T V_j}{\sum_{j=1}^N \phi(Q_i)\phi(K_j)^T} \\
&=\frac{ \phi(Q_i) \sum_{j=1}^{N}\phi(K_j)^T V_j}{\phi(Q_i)\sum_{j=1}^N \phi(K_j)^T}
\end{aligned}
\]<ul><li><p>\(\sum_{j=1}^{N}\phi(K_j)^T V, \sum_{j=1}^N \phi(K_j)^T\) 可以提前算好</p></li><li><p>去掉归一化来看 \[(\phi(Q)\phi(K)^{T})V=\phi(Q)(\phi(K)^{T}V)\]</p><p>\[\begin{aligned} \begin{pmatrix}
\phi(Q_1)\sum_{j=1}^{N} \phi(K_j)^{T} V_j \\
\vdots \\
\phi(Q_N)\sum_{j=1}^{N} \phi(K_j)^T V_j \\
\end{pmatrix}& =\begin{pmatrix}
\phi(Q_1)\phi(K)^{T}V\\
\vdots \\
\phi(Q_N)\phi(K)^{T}V \\
\end{pmatrix} \\
&=
\begin{pmatrix}
\phi(Q_{1})\\
\vdots\\
\phi(Q_N)
\end{pmatrix}\phi(K)^TV \\
&=\phi(Q)\phi(K)^TV
\end{aligned}\]</p></li><li><p>\(O(N)\) 复杂度，Linear Transformer</p></li><li><p>\(\phi(x)=\mathrm{elu}(x)+1\)</p></li></ul></li></ul><h3 id=优缺点>优缺点</h3><h4 id=优点>优点</h4><ul><li>并行</li><li>长距离依赖</li><li>可解释性</li></ul><h4 id=缺点>缺点</h4><ul><li>本身对顺序无感，操作是在集合层次上的，需要额外加入位置编码
下面的cls token得到的语义向量是完全一样的。<ul><li>&lt;cls> 从 北京 到 上海 的 火车票</li><li>&lt;cls> 从 上海 到 北京 的 火车票</li></ul></li><li>计算的复杂度是序列长度平方</li></ul><h3 id=下期内容预告>下期内容预告</h3><ul><li>positional embedding</li></ul><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">What's in this posts</p><nav id=TableOfContents><ul><li><a href=#transformer背景>transformer背景</a><ul><li><a href=#主要内容>主要内容</a></li><li><a href=#矩阵知识>矩阵知识</a></li><li><a href=#encoder-decoder>encoder-decoder</a></li><li><a href=#低阶到高阶语义向量的转换>低阶到高阶语义向量的转换</a></li><li><a href=#核心的问题>核心的问题</a></li></ul></li><li><a href=#tranformer网络结构>tranformer网络结构</a><ul><li><a href=#基于kv查询的相似性计算>基于KV查询的相似性计算</a></li><li><a href=#在一个低维空间做attention>在一个低维空间做attention</a></li><li><a href=#在多个低维空间做attention>在多个低维空间做attention</a></li><li><a href=#位置无关的全连接>位置无关的全连接</a></li><li><a href=#归一化-plus-残差网络>归一化 + 残差网络</a></li><li><a href=#整体的变换>整体的变换</a></li></ul></li><li><a href=#transformer参数和计算量>transformer参数和计算量</a><ul><li><a href=#关于参数量>关于参数量</a></li><li><a href=#参数的分布>参数的分布</a></li><li><a href=#linear-transformer>linear transformer</a></li><li><a href=#优缺点>优缺点</a></li><li><a href=#下期内容预告>下期内容预告</a></li></ul></li></ul></nav></div></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://skylyj.github.io/>&copy; 连博讲AI 2024</a><div><div class=ananke-socials></div></div></div></footer></body></html>