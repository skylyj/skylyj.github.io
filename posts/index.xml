<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on 连博讲AI</title><link>https://skylyj.github.io/posts/</link><description>Recent content in Posts on 连博讲AI</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 01 Feb 2024 20:16:46 +0800</lastBuildDate><atom:link href="https://skylyj.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>深入理解transformer</title><link>https://skylyj.github.io/posts/2024/02/transformer/</link><pubDate>Thu, 01 Feb 2024 20:06:00 +0800</pubDate><guid>https://skylyj.github.io/posts/2024/02/transformer/</guid><description>transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子</description></item><item><title>My First Post</title><link>https://skylyj.github.io/posts/my-first-post/</link><pubDate>Thu, 01 Feb 2024 17:24:30 +0800</pubDate><guid>https://skylyj.github.io/posts/my-first-post/</guid><description>大家好吗, xxxx, not good</description></item></channel></rss>