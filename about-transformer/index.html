<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>transformer入门 | 连博讲AI</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="transformer背景 主要内容 矩阵知识 why 矩阵和行向量 矩阵相乘和算子作用 transformer中的\(QK^{T}V\) 代码 注 encoder-decoder 低阶到高阶语义向量的转换 核心的问题 transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta name=generator content="Hugo 0.122.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=canonical href=https://skylyj.github.io/about-transformer/><meta property="og:title" content="transformer入门"><meta property="og:description" content="transformer背景 主要内容 矩阵知识 why 矩阵和行向量 矩阵相乘和算子作用 transformer中的\(QK^{T}V\) 代码 注 encoder-decoder 低阶到高阶语义向量的转换 核心的问题 transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta property="og:type" content="article"><meta property="og:url" content="https://skylyj.github.io/about-transformer/"><meta property="article:section" content><meta property="article:published_time" content="2024-02-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-01T23:52:04+08:00"><meta itemprop=name content="transformer入门"><meta itemprop=description content="transformer背景 主要内容 矩阵知识 why 矩阵和行向量 矩阵相乘和算子作用 transformer中的\(QK^{T}V\) 代码 注 encoder-decoder 低阶到高阶语义向量的转换 核心的问题 transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"><meta itemprop=datePublished content="2024-02-01T00:00:00+00:00"><meta itemprop=dateModified content="2024-02-01T23:52:04+08:00"><meta itemprop=wordCount content="478"><meta itemprop=keywords content="transformer,org,"><meta name=twitter:card content="summary"><meta name=twitter:title content="transformer入门"><meta name=twitter:description content="transformer背景 主要内容 矩阵知识 why 矩阵和行向量 矩阵相乘和算子作用 transformer中的\(QK^{T}V\) 代码 注 encoder-decoder 低阶到高阶语义向量的转换 核心的问题 transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子"></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">连博讲AI</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about-transformer/ title="transformer入门 page">transformer入门</a></li></ul><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns mw7"><header><h1 class=f1>transformer入门</h1></header><div class="nested-copy-line-height lh-copy f4 nested-links mid-gray"><ul><li><a href=#transformer%E8%83%8C%E6%99%AF>transformer背景</a><ul><li><a href=#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9>主要内容</a></li><li><a href=#%E7%9F%A9%E9%98%B5%E7%9F%A5%E8%AF%86>矩阵知识</a><ul><li><a href=#why>why</a></li><li><a href=#%E7%9F%A9%E9%98%B5%E5%92%8C%E8%A1%8C%E5%90%91%E9%87%8F>矩阵和行向量</a></li><li><a href=#%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E5%92%8C%E7%AE%97%E5%AD%90%E4%BD%9C%E7%94%A8>矩阵相乘和算子作用</a></li><li><a href=#transformer%E4%B8%AD%E7%9A%84-qk-t-v>transformer中的\(QK^{T}V\)</a></li><li><a href=#%E4%BB%A3%E7%A0%81>代码</a></li><li><a href=#%E6%B3%A8>注</a></li></ul></li><li><a href=#encoder-decoder>encoder-decoder</a></li><li><a href=#%E4%BD%8E%E9%98%B6%E5%88%B0%E9%AB%98%E9%98%B6%E8%AF%AD%E4%B9%89%E5%90%91%E9%87%8F%E7%9A%84%E8%BD%AC%E6%8D%A2>低阶到高阶语义向量的转换</a></li><li><a href=#%E6%A0%B8%E5%BF%83%E7%9A%84%E9%97%AE%E9%A2%98>核心的问题</a></li></ul></li></ul><h2 id=transformer背景>transformer背景</h2><h3 id=主要内容>主要内容</h3><ul><li>参考<ul><li><ol start=2017><li>(引用 106576)
Attention Is All You Need</li></ol></li><li><ol start=2020><li>(引用 975)
Fast Autoregressive Transformers with Linear Attention</li></ol></li><li><a href=https://github.com/karpathy/minGPT/tree/master/mingpt>mingpt by karpathy</a></li></ul></li><li>主要内容<ul><li>transformer 的设计推演</li><li>transformer 的代码讲解</li><li>transformer的参数和运算量</li><li>linear attention</li></ul></li></ul><h3 id=矩阵知识>矩阵知识</h3><h4 id=why>why</h4><ul><li>原文直接从整个矩阵作用出发
\[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\
\mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i)
\end{aligned}\]</li><li>从行向量的角度更容易理解</li></ul><h4 id=矩阵和行向量>矩阵和行向量</h4><ul><li>矩阵
\(X\in R^{N\times F}\)
\(X=\begin{pmatrix}
X_{11}, X_{12},\ldots, X_{1F} \\
X_{21}, X_{22},\ldots, X_{2F} \\
\vdots\\
X_{N1}, X_{N2},\ldots, X_{NF}
\end{pmatrix}\)</li><li>行向量
\(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\)</li><li>分块矩阵
\(X=\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}\)</li></ul><ul><li><p>例子</p><ul><li>\(N\) 个token，\(F\) 是embedding的维度</li><li>每行对应于一个token的embedding 行向量</li></ul><p>\(tokens=\begin{pmatrix}
hello \\
world \\
pad \\
pad \\
pad
\end{pmatrix}
\rightarrow X=\begin{pmatrix}
[0.59, 0.20, 0.04, 0.96] \\
[0.96, 0.30, 0.16, 0.63] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25] \\
[0.02, 0.19, 0.34, 0.25]
\end{pmatrix}\)</p></li></ul><h4 id=矩阵相乘和算子作用>矩阵相乘和算子作用</h4><ul><li>定义线性算子 \(\mathcal{A}\)<ul><li>可以作用到行向量 \(\mathcal{A}(X_i) = X_{i} A\)</li><li>也可以作用到矩阵 \(\mathcal{A}(X) = XA\)</li></ul></li><li>右乘矩阵等于对每个行向量逐个施加行变换
\(XA=\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}A=
\begin{pmatrix}
X_1 A\\
X_2 A\\
\vdots\\
X_N A
\end{pmatrix}=
\begin{pmatrix}
\mathcal{A}(X_1) \\
\mathcal{A}(X_2) \\
\vdots\\
\mathcal{A}(X_N)
\end{pmatrix}=\mathcal{A}(X)\)</li><li>代码对应于 nn.Linear</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(in_features<span style=color:#f92672>=</span>F, out_features<span style=color:#f92672>=</span>F)
</span></span><span style=display:flex><span>X_i <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>print(linear(X_i)<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(linear(X)<span style=color:#f92672>.</span>shape)
</span></span></code></pre></td></tr></table></div></div><ul><li>算子是对矩阵乘法的一种物理理解<ul><li><p>旋转矩阵</p><p>\(R(\theta)=\begin{pmatrix}
cos\theta& sin\theta\\
-sin\theta& cos\theta
\end{pmatrix}\)</p></li><li><p>缩放变换</p><p>\(R(\lambda_1,\lambda_2)=\begin{pmatrix} \lambda_1 & \\
& \lambda_2 \end{pmatrix}\)</p></li></ul></li></ul><h4 id=transformer中的-qk-t-v>transformer中的\(QK^{T}V\)</h4><ul><li><p>\(S=QK^T\) = 行向量两两计算点积相似性
\(\begin{pmatrix}
Q_{1}\\
Q_{2}\\
\vdots\\
Q_N
\end{pmatrix}
\begin{pmatrix}
K_{1}^T, K_2^T,\ldots,K_N^T
\end{pmatrix}=(Q_{i}K_j^T)_{ij}=S\)</p></li><li><p>\(SV\) = 对行向量做加权求和
$\begin{pmatrix}
S11,S12,\ldots, S1N
S21,S22,\ldots, S2N
\vdots
SN1,SN2,\ldots, SNN
\end{pmatrix}</p><p>\begin{pmatrix}
Q_{1}\\
Q_{2}\\
\vdots\\
Q_N
\end{pmatrix}</p><p>=(QiK_j^T)ij=S$</p></li></ul><h4 id=代码>代码</h4><ul><li>pytorch/tensorflow中的代码都是按照作用于行向量来组织的</li><li>nn.Linear 作用于行向量</li><li>nn.Embedding 按照行向量来组织数据</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>N <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>F <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>embed <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>30</span>, F)
</span></span><span style=display:flex><span>idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> embed(idx)
</span></span><span style=display:flex><span>print(X<span style=color:#f92672>.</span>shape)
</span></span></code></pre></td></tr></table></div></div><h4 id=注>注</h4><ul><li>左乘以一个矩阵相当于对每个列向量来施加变化</li><li>论文：一般会有行/列向量两种表示方式</li><li>代码：基本都是行向量来作为数据组织的标准</li><li>本文:<ul><li>向量都按照行向量的形式来组织</li><li>按照作用于单个行向量的方式来讲解transformer</li></ul></li></ul><h3 id=encoder-decoder>encoder-decoder</h3><ul><li>大部分的s2s 的任务建模为 encoder-decoder的结构<ul><li>机器翻译，语音识别，文本摘要，问答系统等</li></ul></li><li>encoder<ul><li>把token序列\((x_{1}, x_2,\ldots, x_N)\) 转化为语义向量序列 \((Y_{1}, Y_2, \ldots, Y_N)\)</li><li>一般组织为多层的网络的形式<ul><li>第一层：基础语义向量序列
\((x_{1}, x_2,\ldots, x_N)\rightarrow (X_{1}, X_2,\ldots, X_N)\)</li><li>其它层：高阶语义向量序列
\((X_{1}, X_2,\ldots, X_N)\rightarrow (Y_{1}, Y_2,\ldots, Y_N)\)</li></ul></li></ul></li><li>decoder
基于\((Y_{1}, Y_2, \ldots, Y_N)\) 自回归式的逐个token解码</li></ul><p>focus到 encoder部分来理解transformer</p><h3 id=低阶到高阶语义向量的转换>低阶到高阶语义向量的转换</h3><p>寻找算子 \(\mathcal{T}\) 将低阶的语义向量序列变换为高阶的语义向量序列
\(\mathcal{T}\begin{pmatrix}
X_1\\
X_2\\
\vdots\\
X_N
\end{pmatrix}
\rightarrow\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}\)</p><ul><li>输入: \(X\) 低阶语义向量序列，输出: \(Y\) 高阶语义向量序列</li><li>意义<ul><li>\(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)</li><li>对低阶语义向量做加工组合处理和抽象，变换为一个高阶的语义向量序列</li><li>高阶语义向量考虑了 <em>上下文</em> 的语义向量表达</li></ul></li><li>motivation<ul><li><ol start=1957><li>Firth</li></ol><blockquote><p>a word is characterized by the company it keeps.</p></blockquote><p>例子：</p><blockquote><p>The <strong>enigmatic</strong> smile on Mona Lisa&rsquo;s face has intrigued art enthusiasts for centuries, leaving them to speculate about its true meaning.</p></blockquote></li></ul></li><li>用矩阵变换表达 \(Y=\mathcal{T}(X)\)<ul><li>\(X \in R^{N\times F}\), \(Y=\mathcal{T}(X): \quad R^{N\times F}\rightarrow R^{N\times F}\)</li><li>这个算子天然可以复合嵌套，形成多层的网络结构
\(Y=\mathcal{T}_{L}\circ \mathcal{T}_{L-1}\circ \ldots \circ \mathcal{T}_{1}(X)\)</li></ul></li></ul><h3 id=核心的问题>核心的问题</h3><ul><li><p>问题</p><p>如何设计 \(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)</p><ul><li>\(Y_{1}, \ldots, Y_N\) 能否并行得到</li><li>\(Y_{i}\) 能否高效的建立起远程的依赖</li></ul></li></ul><ul><li><p>RNN</p><figure><img src=/images/2024-01-18_14-03-26_screenshot.png width=600px></figure><ul><li>递归语义序列 \(Y_{0}\rightarrow Y_1 \rightarrow \ldots \rightarrow Y_{N}\)</li><li>\(Y_{i}=tanh(X_{i}W + Y_{i-1}U)\)</li><li>串行</li><li>单方向的依赖关系，间接</li></ul></li></ul><ul><li><p>CNN</p><figure><img src=/images/2024-01-18_14-04-23_screenshot.png width=600px></figure><ul><li>\(Y_{i}=(X_{i-1},X_i, X_{i+1}) W\) 假设窗口宽度是3</li><li>并行</li><li>长距离依赖？<ul><li>一层卷积只能依赖于当前窗口内，不能对窗口外的形成依赖。</li></ul></li></ul></li></ul><ul><li><p>transformer思路</p><p>设计\(Y_{i}=f(X_{1}, X_2, \ldots, X_{N})\)，使得</p><ul><li>使得 \(Y_{1},\ldots, Y_N\) 可以做并行计算</li><li>同时解决长距离依赖的问题</li></ul><figure><img src=images/2024-01-18_14-13-40_screenshot.png width=400px></figure><p>\(Y=\mathcal{F}\circ \mathcal{A}(X)\) 做两次矩阵的变换</p><ul><li><p>\(Y=\mathcal{A}(X)\) MultiHead Attention</p><ul><li>高阶的语义等于对 <em>全部</em> 的低阶语义向量基于 <em>相似性(Attention)</em> 做 <em>加权平均</em></li><li>\[\begin{aligned}\mathcal{A}(X_i) &= \frac{\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\sum_{j=1}^N sim(X_i,X_j)} \end{aligned}\]</li><li>attention = 相似性</li></ul></li><li><p>$Y&rsquo;=\mathcal{</p></li></ul></li></ul></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://skylyj.github.io/>&copy; 连博讲AI 2024</a><div><div class=ananke-socials></div></div></div></footer></body></html>