[{"categories":null,"content":"Whole hugo blog in plain text!","date":"2024-02-01","objectID":"/transformer/","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"transformer背景 ","date":"2024-02-01","objectID":"/transformer/:1:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"主要内容 主要内容 从高阶语义向量的组合表达来看attention \\(\\mathcal{A}(X_i) = \\dfrac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\) 提供transformer 的代码讲解 从kernel的角度来看attention 计算量\\(O(N^2)\\) 源于softmax的存在 linear attention 参考 Attention Is All You Need (Vaswani et al. 2023) Fast Autoregressive Transformers with Linear Attention (Katharopoulos et al. 2020) mingpt by karpathy ","date":"2024-02-01","objectID":"/transformer/:1:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"矩阵知识 why 原文直接用矩阵来表达 \\[\\begin{aligned}\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\dfrac{QK^T}{\\sqrt{d_k}})V \\\\ \\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_1,\\ldots,\\mathrm{head}_h)W^{O} \\\\ \\mathrm{head}_i=\\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \\end{aligned}\\] 把矩阵剖解成从行向量来看更容易理解 矩阵和行向量 矩阵 \\(X\\in R^{N\\times F}\\) \\(X=\\begin{pmatrix} X_{11}, X_{12},\\ldots, X_{1F} \\\\ X_{21}, X_{22},\\ldots, X_{2F} \\\\ \\vdots\\\\ X_{N1}, X_{N2},\\ldots, X_{NF} \\end{pmatrix}\\) 行向量 \\(X_{i}=\\begin{pmatrix} X_{i1}, X_{i2},\\ldots, X_{iF}\\end{pmatrix}, X_i \\in R^{1\\times F}\\) 分块矩阵 \\(X=\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix}\\) nn.Embedding 按照行向量来组织数据 import torch import torch.nn as nn N = 3 F = 8 embed = nn.Embedding(30, F) idx = torch.tensor([1,2,3]) X = embed(idx) print(X.shape) 例子 \\(N\\) 个token，\\(F\\) 是embedding的维度 每行对应于一个token的embedding 行向量 \\(tokens=\\begin{pmatrix} hello \\\\ world \\\\ \\\\ \\\\ \\\\ \\end{pmatrix}\\) \\(X=\\begin{pmatrix} [0.59, 0.20, 0.04, 0.96] \\\\ [0.96, 0.30, 0.16, 0.63] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\end{pmatrix}\\) 矩阵相乘和算子作用 定义线性算子 \\(\\mathcal{A}\\) 可以作用到行向量 \\(\\mathcal{A}(X_i) = X_{i} A\\) 也可以作用到矩阵 \\(\\mathcal{A}(X) = XA\\) 右乘矩阵等于对每个行向量逐个施加行变换 \\(XA=\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix}A= \\begin{pmatrix} X_1 A\\\\ X_2 A\\\\ \\vdots\\\\ X_N A \\end{pmatrix}= \\begin{pmatrix} \\mathcal{A}(X_1) \\\\ \\mathcal{A}(X_2) \\\\ \\vdots\\\\ \\mathcal{A}(X_N) \\end{pmatrix}=\\mathcal{A}(X)\\) 代码对应于 nn.Linear import torch import torch.nn as nn F = 6 linear = nn.Linear(in_features=F, out_features=F) X_i = torch.rand(1, 6) X = torch.rand(3, 6) print(linear(X_i).shape) print(linear(X).shape) pytorch/tensorflow中的代码都是按照作用于行向量来组织的 transformer中的\\(QK^{T}V\\) \\(S=QK^T\\) 行向量两两计算点积相似性 \\(\\begin{pmatrix} Q_{1}\\\\ Q_{2}\\\\ \\vdots\\\\ Q_N \\end{pmatrix} \\begin{pmatrix} K_{1}^T, K_2^T,\\ldots,K_N^T\\\\ \\end{pmatrix}=(Q_{i}K_j^T)_{ij}=S\\) \\(SV\\) = 对行向量做加权求和 \\(\\begin{pmatrix} S_{11},S_{12},\\ldots, S_{1N}\\\\ S_{21},S_{22},\\ldots, S_{2N}\\\\ \\vdots\\\\ S_{N1},S_{N2},\\ldots, S_{NN}\\\\ \\end{pmatrix} \\begin{pmatrix} Q_{1}\\\\ Q_{2}\\\\ \\vdots\\\\ Q_N \\end{pmatrix}= \\begin{pmatrix} S_{1j}Q_j\\\\ S_{2j}Q_j\\\\ \\vdots\\\\ S_{Nj}Q_j \\end{pmatrix}\\) 注 左乘以一个矩阵相当于对每个列向量来施加变化 论文：一般会有行/列向量两种表示方式 代码：基本都是行向量来作为数据组织的标准 本文: 向量都按照行向量的形式来组织 按照作用于单个行向量的方式来讲解transformer ","date":"2024-02-01","objectID":"/transformer/:1:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"encoder-decoder 大部分的s2s 的任务建模为 encoder-decoder的结构 机器翻译，语音识别，文本摘要，问答系统等 encoder 把token序列\\((x_{1}, x_2,\\ldots, x_N)\\) 转化为语义向量序列 \\((Y_{1}, Y_2, \\ldots, Y_N)\\) 一般组织为多层的网络的形式 第一层：基础语义向量序列 \\((x_{1}, x_2,\\ldots, x_N)\\rightarrow (X_{1}, X_2,\\ldots, X_N)\\) 其它层：高阶语义向量序列 \\((X_{1}, X_2,\\ldots, X_N)\\rightarrow (Y_{1}, Y_2,\\ldots, Y_N)\\) decoder 基于\\((Y_{1}, Y_2, \\ldots, Y_N)\\) 自回归式的逐个token解码 focus到 encoder部分来理解transformer ","date":"2024-02-01","objectID":"/transformer/:1:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"低阶到高阶语义向量的转换 寻找算子 \\(\\mathcal{T}\\) 将低阶的语义向量序列变换为高阶的语义向量序列 \\(\\mathcal{T}\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix} \\rightarrow\\begin{pmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_N \\end{pmatrix}\\) 输入: \\(X\\) 低阶语义向量序列，输出: \\(Y\\) 高阶语义向量序列 意义 \\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\) 对低阶语义向量做加工组合处理和抽象，变换为一个高阶的语义向量序列 高阶语义向量考虑了 上下文 的语义向量表达 motivation Firth a word is characterized by the company it keeps. 例子： The enigmatic smile on Mona Lisa’s face has intrigued art enthusiasts for centuries, leaving them to speculate about its true meaning. 用算子作用来表达 \\(Y=\\mathcal{T}(X)\\) \\(X \\in R^{N\\times F}\\), \\(Y=\\mathcal{T}(X): \\quad R^{N\\times F}\\rightarrow R^{N\\times F}\\) 这个算子天然可以复合嵌套，形成多层的网络结构 \\(Y=\\mathcal{T}_{L}\\circ \\mathcal{T}_{L-1}\\circ \\ldots \\circ \\mathcal{T}_{1}(X)\\) ","date":"2024-02-01","objectID":"/transformer/:1:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"核心的问题 问题 如何设计 \\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\) \\(Y_{1}, \\ldots, Y_N\\) 能否并行得到 \\(Y_{i}\\) 能否高效的建立起远程的依赖 RNN 递归语义序列 \\(Y_{0}\\rightarrow Y_1 \\rightarrow \\ldots \\rightarrow Y_{N}\\) \\(Y_{i}=tanh(X_{i}W + Y_{i-1}U)\\) 串行 单方向的依赖关系 \\(Y_{3}\\) 直接依赖于\\(Y_{2}, X_{2}\\), 间接依赖于\\(X_1\\) CNN \\(Y_{i}=(X_{i-1},X_i, X_{i+1}) W\\) 假设窗口宽度是3 并行 长距离依赖？ 一层卷积只能依赖于当前窗口内，不能对窗口外的形成依赖。 transformer思路 设计\\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\)，使得 使得 \\(Y_{1},\\ldots, Y_N\\) 可以做并行计算 同时解决长距离依赖的问题 \\(Y=\\mathcal{F}\\circ \\mathcal{A}(X)\\) 做两次矩阵的变换 \\(Y=\\mathcal{A}(X)\\) MultiHead Attention 高阶的语义等于对 全部 的低阶语义向量基于 相似性(Attention) 做 加权平均 \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\\) attention = 相似性 \\(Y’=\\mathcal{F}(Y)\\) Position-wise Feedforward 再施加若干非线性变换 ","date":"2024-02-01","objectID":"/transformer/:1:5","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"tranformer网络结构 ","date":"2024-02-01","objectID":"/transformer/:2:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"基于KV查询的相似性计算 \\[\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\\] 如何来定义相似性 \\(sim(X_{i}, X_j)= \\mathrm{exp}(\\dfrac{X_i X_{j}^T}{\\sqrt{D}})\\) \\(sim(X_{i}, X_j)= X_i X_{j}^T\\) 直接计算相似性？ 参数太少 投影到别的空间来计算相似度 \\(X_{i}\\rightarrow X_iW\\) \\(\\begin{aligned} \\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_iW_1,X_jW_{2}) X_jW_3}{\\sum_{j=1}^N sim(X_iW_1,X_jW_2)} \\end{aligned}\\) 如果我们记 \\(X_{i}W_{1}=Q_i, X_iW_2=K_i, X_iW_3=V_{i}\\)， \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\\) 基于KV查询理解 把\\(X_i\\) 投影出三个向量 \\(Q_i,K_i,V_i\\) QKV KV 是大家熟悉的key-value存储 \\(K_{j}\\rightarrow V_{j}\\) Q 是查询使用的query向量 \\(Q_{i}\\) QKV的查询方法 query查询多个key，获取多个value 最后把这些value加权平均 \\(Q_i\\Rightarrow \\begin{pmatrix} K_{1}\\rightarrow V_{1}\\\\ K_2\\rightarrow V_2\\\\ \\vdots\\\\ K_N\\rightarrow V_N \\end{pmatrix} \\Rightarrow \\begin{pmatrix} sim(Q_i,K_1)V_{1} \\\\ sim(Q_i,K_2)V_{2} \\\\ \\vdots\\\\ sim(Q_i,K_N)V_N \\end{pmatrix}\\Rightarrow\\sum_{j=1}^N sim(Q_i,K_j)V_j\\) \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\\) 参数： 对应于\\(Q,K,V\\) 产生了三个投影矩阵矩阵 \\(W_{Q}, W_K,W_V\\) ","date":"2024-02-01","objectID":"/transformer/:2:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"在一个低维空间做attention 单个头的attention 把\\(X_{i}\\) 从\\(F\\) 维空间投影到\\(D\\) 维空间 \\(W_{Q}\\in R^{F\\times D}, W_K\\in R^{F\\times D}, W_{V} \\in R^{F\\times M}\\) \\(Q_i = X_iW_{Q}, \\quad K_i = X_iW_{K}, \\quad V_i = X_iW_{V}\\) \\(Q_i\\) 和所有的\\(K_j\\) 做基于点积的相似度计算， 这里简单起见，我们省略掉了scaling \\(\\frac{1}{\\sqrt{D}}\\) \\(Q_iK^{T}=Q_i(K^T_1, \\ldots, K^T_N)=(Q_iK^T_1, \\ldots, Q_iK^T_N)\\) 对相似度的分布做softmax \\(S=\\mathrm{soft}(Q_iK^T_1, \\ldots, Q_iK^T_N)=(s_{i1},\\ldots, s_{iN})\\) \\(s_{i,j}= \\dfrac{exp(Q_iK_j^T)}{\\sum_{j=1}^N exp(Q_iK_j^T)}\\) 加权平均 \\(\\mathcal{A}(X_i)=\\sum_{j=1}^Ns_jV_j=(s_{i1},\\ldots, s_{iN}) \\begin{pmatrix} V_1\\\\ V_2\\\\ \\vdots\\\\ V_N\\end{pmatrix}\\) \\(\\mathcal{A}(X_i) = \\mathrm{soft}(Q_iK^{T})V = \\mathrm{soft}(X_iW_QW_K^TX^T)XW_V\\) 矩阵表达 \\(Y=\\mathcal{A}(X) =\\begin{pmatrix} \\mathcal{A}(X_1)\\\\ \\mathcal{A}(X_2)\\\\ \\vdots\\\\ \\mathcal{A}(X_N) \\end{pmatrix} =\\begin{pmatrix} \\mathrm{soft}(Q_1K^T)V\\\\ \\mathrm{soft}(Q_2K^T)V\\\\ \\vdots \\\\ \\mathrm{soft}(Q_NK^T)V \\end{pmatrix}=\\mathrm{soft}(QK^T)V\\) 简化符号 \\(sim(Q,K)V\\) 代码实现 import torch import torch.nn as nn import math from torch.nn import functional as F class SingleHeadAttention(nn.Module): def __init__(self, config): super().__init__() self.F = config[\"fea_size\"] #F self.D = config[\"subspace_dim\"] #D self.q_proj = nn.Linear(self.F, self.D) self.k_proj = nn.Linear(self.F, self.D) self.v_proj = nn.Linear(self.F, self.D) def forward(self, x): B, N, F = x.size() q = self.q_proj(x) k = self.k_proj(x) v = self.v_proj(x) att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v return y 注: \\(D\\neq F\\) 时，\\(\\mathcal{A}(X)\\) 还不可用 ","date":"2024-02-01","objectID":"/transformer/:2:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"在多个低维空间做attention why Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 一词多义 把\\(F\\) 维的语义向量投影到 \\(H\\) 个不同的子空间中去计算相似加权组合 做法 每个头投做独立的Attention变换 \\(\\mathcal{A}^{h}(X)\\) 假设有\\(H\\) 个头，每个头作用的低维空间维度是\\(D\\) \\(D\\times H = F\\) 对\\(H\\) 个 \\(D\\) 行向量拼接 \\(W_O\\in R^{F\\times F}\\) \\(\\mathcal{A}(X) = \\mathrm{concat}(\\mathcal{A}^1(X), \\mathcal{A}^2(X), \\ldots, \\mathcal{A}^{H}(X) W_O\\) 或者对前面的符号简化 在第\\(j\\) 个子空间做单头注意力 \\(Y^{j}=sim(Q^{j}, K^{j})V^{j}\\) 合并 \\(Y=(Y^{1},\\ldots, Y^H)\\) 代码实现 # 参考 https://github.com/karpathy/minGPT/tree/master/mingpt import torch import torch.nn as nn import math from torch.nn import functional as F class SelfAttention(nn.Module): def __init__(self, config): super().__init__() self.H = config[\"n_head\"] self.F = config[\"fea_size\"] #F self.D = self.fea_size // self.n_head #D # 一次把qkv 全部映射完成，对应W_Q, W_K, W_V self.qkv_proj = nn.Linear(self.fea_size, 3 * self.fea_size) # 最后的投影，对应于 $W_O$ self.out_proj = nn.Linear(self.fea_size, self.fea_size) def forward(self, x): B, N, fea_size = x.size() q, k, v = self.qkv_proj(x).split(3, dim=2) # matmul 只能在最后两个维度相乘，需要对NxD的矩阵相乘，做1,2维度的交换 k = k.view(B, N, self.H, self.D).transpose(1, 2) q = q.view(B, N, self.H, self.D).transpose(1, 2) v = v.view(B, N, self.H, self.D).transpose(1, 2) # 一次把多个头的映射全部完成 att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v # 多头拼接 y = y.transpose(1, 2).contiguous().view(B, N, F) y = self.out_proj(y) return y 代码示意 ","date":"2024-02-01","objectID":"/transformer/:2:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"位置无关的全连接 两层的全连接 \\(\\mathcal{F}(X_i)=(g(X_iW_1)+b_1)W_2+b_2)\\) 代码 import torch import torch.nn as nn class PWiseFeedForward(nn.Module): def __init__(self, config): super().__init__() self.fea_size = config[\"fea_size\"] self.proj_wide = nn.Linear(self.fea_size, 4 * self.fea_size) self.proj_narrow = nn.Linear(4 * self.fea_size, self.fea_size) self.act = nn.ReLU() def forward(self, x): return self.proj_narrow(self.act(self.proj_wide(x))) ","date":"2024-02-01","objectID":"/transformer/:2:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"归一化 + 残差网络 \\(\\mathcal{T}(X)=\\mathcal{F}\\circ\\mathcal{A}(X)\\) Layer Normalization \\(\\mathcal{A}’(X)=\\mathcal{N}\\circ\\mathcal{A}(X)\\) \\(\\dfrac{x-\\mu}{\\sqrt{\\sigma}}\\gamma + \\beta,\\mu=\\dfrac{1}{d}\\sum\\limits_{i=1}^{d}x_{i}, \\sigma=\\sqrt{\\dfrac{1}{d}\\sum\\limits_{i=1}^{d}(x_{i}-\\mu)^{2}}\\) 可以看成是作用在行向量上的算子 输入矩阵例子 \\(\\begin{pmatrix} \\text{hello} \\\\ \\text{world} \\\\ \\text{pad} \\\\ \\text{pad} \\\\ \\text{pad} \\end{pmatrix} \\rightarrow X= \\begin{pmatrix} [0.59, 0.20, 0.04, 0.96] \\\\ [0.96, 0.30, 0.16, 0.63] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\end{pmatrix}\\) 行归一化 or 列归一化 在NLP的序列建模里面，Layer Normalization 在CV/CTR预估里面, Batch Normalization Why padding的影响 不同batch中\u003cpad\u003e个数不同，沿着token方向做归一化没有意义 每个位置做独立的归一化更有意义 其他的可能选择 RMSNorm \\(\\dfrac{x}{\\text{RMS}(x)}, \\quad \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2}\\) ","date":"2024-02-01","objectID":"/transformer/:2:5","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"整体的变换 \\(Y=\\mathcal{T}(X)\\) Attention \\(Z=\\mathcal{N}\\circ(X+\\mathcal{A}(X))\\) 位置无关的全连接 \\(Y=\\mathcal{N}\\circ(X+\\mathcal{F}(Z))\\) residual network \\(\\mathcal{A}’(X)=\\mathcal{N}\\circ(X+\\mathcal{A}(X))\\) \\(\\mathcal{F}’(X)=\\mathcal{N}\\circ(X+\\mathcal{F}(X))\\) 多层 一个 \\(L\\) 层的transformer 模型 \\begin{equation*} \\begin{split} \\mathcal{T}(X) \u0026 = \\mathcal{T}_L \\circ \\ldots \\mathcal{T}_{2}\\circ \\mathcal{T}_{1}(X) \\end{split} \\end{equation*} 代码 import torch.nn as nn class Block(nn.Module): def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config.fea_size) self.attn = SelfAttention(config) self.layer_norm_2 = nn.LayerNorm(config.fea_size) self.mlp = PWiseFeedForward(config) def forward(self, x): x = self.layer_norm_1(x + self.attn(x)) x = self.layer_norm_2(x + self.mlp(x)) return x ","date":"2024-02-01","objectID":"/transformer/:2:6","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"transformer参数和计算量 ","date":"2024-02-01","objectID":"/transformer/:3:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"关于参数量 我们需要一种模型能够方便的去增加模型的复杂度 比如增加深度，增加宽度 增加token的embedding size 增加词典的大小 transformer模型可以在此之外非常有效的提升模型的参数量 ","date":"2024-02-01","objectID":"/transformer/:3:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"参数的分布 多头注意力 \\(4F^2\\) 每个头有 3个投影矩阵 \\(W_Q, W_K, W_V\\) 1个投影concat结果的矩阵 \\(W_O\\) 参数量: 假设投射到的子空间维度是\\(D\\), $H$个子空间，\\(D\\times H = F\\) \\(F\\times D \\times 3 \\times H = 3F^{2}\\) \\(F^{2}\\) FFW \\(8F^2\\) 两个矩阵，先从\\(F\\) 变宽到\\(4F\\)，再收窄回来到\\(F\\) 参数量\\(F\\times4F + 4F\\times F= 8F^{2}\\) word embedding \\(E\\) 是token字典的大小 \\(E\\times F\\) total \\(L(12F^{2})+EF\\) model 维度 层数 头数 字典大小 参数量 bertBase 768 12 12 30000 110M bertLarge 1024 24 12 30000 340M ","date":"2024-02-01","objectID":"/transformer/:3:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"linear transformer 两个算子的计算量 \\(\\mathcal{A}(X)\\) 计算量 \\(O(N^2)\\) \\(\\mathcal{F}(X)\\) 计算量 \\(O(N)\\) softmax 导致了\\(O(N^2)\\) 核心的计算量在这三个矩阵的相乘上，\\(QK^{T}V\\) 有softmax的存在的话 只能先计算\\(H=QK^{T}\\), 对\\(H\\) 做softmax 变换后，再计算\\(HV\\) 乘法的计算量是 \\(N^2D+N^2M\\), 整体的复杂度是\\(O(N^{2})\\) \\(QK^TV=(QK^T)V=\\begin{pmatrix} H_{11},H_{12},\\ldots,H_{1N} \\\\ \\vdots\\\\ H_{N1},H_{N2},\\ldots,H_{NN} \\\\ \\end{pmatrix}V\\) 如果没有softmax的话 可以先计算后两个矩阵相乘\\(H=K^TV\\), 再计算\\(QH\\) 计算量可以是\\(O(N)\\), 因为\\(K^TV\\) 可以提前算出来缓存，大致如下面这个表达所示 \\(Q(K^TV)=\\begin{pmatrix} Q_1 \\\\ Q_2 \\\\ \\vdots\\\\ Q_{N} \\end{pmatrix}(K^TV)\\) kernel \\(\\mathcal{A}(X_i)=\\dfrac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\) kernel: \\(k(x,y)=\u003c\\phi(x),\\phi(y)\u003e\\) \\(k(x,y)=(x\\cdot z)^2, \\phi(x)=(x_{1}^{2},x_{2}^2,\\sqrt{2}x_1x_{2})\\) kernel 对应一个feature map 可以用非负的kernel来替换掉 当前的sim函数 \\(sim(x,y)=\\mathrm{exp}(xy^{T}/\\sqrt{D})\\) linear transformer \\(O(N)\\) 用kernel来替换掉sim \\[\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\\\ \u0026=\\frac{\\sum_{j=1}^{N} \\phi(Q_i)\\phi(K_j)^T V_j}{\\sum_{j=1}^N \\phi(Q_i)\\phi(K_j)^T} \\\\ \u0026=\\frac{ \\phi(Q_i) \\sum_{j=1}^{N}\\phi(K_j)^T V_j}{\\phi(Q_i)\\sum_{j=1}^N \\phi(K_j)^T} \\end{aligned} \\] \\(\\sum_{j=1}^{N}\\phi(K_j)^T V, \\sum_{j=1}^N \\phi(K_j)^T\\) 可以提前算好 去掉归一化来看 \\[(\\phi(Q)\\phi(K)^{T})V=\\phi(Q)(\\phi(K)^{T}V)\\] \\[\\begin{aligned} \\begin{pmatrix} \\phi(Q_1)\\sum_{j=1}^{N} \\phi(K_j)^{T} V_j \\\\ \\vdots \\\\ \\phi(Q_N)\\sum_{j=1}^{N} \\phi(K_j)^T V_j \\\\ \\end{pmatrix}\u0026 =\\begin{pmatrix} \\phi(Q_1)\\phi(K)^{T}V\\\\ \\vdots \\\\ \\phi(Q_N)\\phi(K)^{T}V \\\\ \\end{pmatrix} \\\\ \u0026= \\begin{pmatrix} \\phi(Q_{1})\\\\ \\vdots\\\\ \\phi(Q_N) \\end{pmatrix}\\phi(K)^TV \\\\ \u0026=\\phi(Q)\\phi(K)^TV \\end{aligned}\\] \\(O(N)\\) 复杂度，Linear Transformer \\(\\phi(x)=\\mathrm{elu}(x)+1\\) ","date":"2024-02-01","objectID":"/transformer/:3:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"优缺点 优点 并行 长距离依赖 可解释性 缺点 本身对顺序无感，操作是在集合层次上的，需要额外加入位置编码 下面的cls token得到的语义向量是完全一样的。 \u003ccls\u003e 从 北京 到 上海 的 火车票 \u003ccls\u003e 从 上海 到 北京 的 火车票 计算的复杂度是序列长度平方 ","date":"2024-02-01","objectID":"/transformer/:3:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"参考论文 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. “Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention.” Arxiv.Org. https://arxiv.org/abs/2006.16236v3. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762. ","date":"2024-02-01","objectID":"/transformer/:4:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"Whole hugo blog in plain text!","date":"2024-02-01","objectID":"/positional_embedding/","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"背景 ","date":"2024-02-01","objectID":"/positional_embedding/:1:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"主要内容 内容 旋转位置编码的来龙去脉 代码实现 参考： 论文：2021. Enhanced Transformer with Rotary Position Embedding (Su et al. 2022) hugging face llama ","date":"2024-02-01","objectID":"/positional_embedding/:1:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"回顾transformer encoder: 低阶语义向量序列转化为高阶的语义向量序列 \\(\\mathcal{T}\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix} \\rightarrow\\begin{pmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_N \\end{pmatrix}\\) 两个变换组成 \\(Y=\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) Attention \\(\\mathcal{A}\\) Feedforward \\(\\mathcal{F}\\) 详细 \\(\\begin{aligned}Q_{i} \u0026= X_{i} W_{Q} \\\\ K_{i} \u0026= X_{i} W_{K}\\\\ V_{i} \u0026= X_{i} W_{V}\\\\ Y_{i} \u0026= \\sum_{j=1}^{N}sim(Q_i,K_{j}) V_j\\\\ sim(Q_{i},K_j) = \u0026= \\frac{exp(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{d}})} {\\sum_{j=1}^N exp(\\frac{Q_iK_j^{T}}{\\sqrt{d}})}\\\\ \\end{aligned}\\) ","date":"2024-02-01","objectID":"/positional_embedding/:1:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"为什么需要位置编码 因为transformer结构本身是和距离无关的， \\(Y=\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) 高阶语义向量不仅仅是由周围token的语义向量组合表达而成 还需要加上每个token所处的位置 下面的cls token得到的语义向量是完全一样的。 \u003ccls\u003e 从 北京 到 上海 的 火车票 \u003ccls\u003e 从 上海 到 北京 的 火车票 其他的网络结构天然有序列的位置信息 RNN/CNN ","date":"2024-02-01","objectID":"/positional_embedding/:1:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"如何加入位置编码 \\(\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) \\(\\mathcal{F}\\) 是位置无关的 可以修改 \\(X\\) 或者 \\(\\mathcal{A}\\) ","date":"2024-02-01","objectID":"/positional_embedding/:2:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"1. 直接修改输入，加入绝对位置编码 在\\(X_i \\rightarrow Q_i, K_i, V_i\\) 之前，直接加入位置的embedding \\(X_i^{’}=X_i+P_i\\) learned embedding 优点简单，bert/GPT 外推困难，对于超过序列最大长度的位置 自定义绝对位置编码 二维函数 f(position, dimension) 要求 函数随着position,dimension增长应该是有界的 足够的区分度，对position, dimension 例子 \\(\\begin{aligned} P_{i,2t} \u0026= sin(k/10000^{2t/d}) \u0026\u0026\\\\ P_{i,2t+1} \u0026= cos(k/10000^{2t/d})\u0026\u0026\\\\ \\end{aligned}\\) 问题 语义应该是和相对位置有关的，而不是绝对位置 ","date":"2024-02-01","objectID":"/positional_embedding/:2:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"2. 修改Attention，加入相对位置信息 回顾Attention \\(\\begin{aligned}Q_{i} \u0026= X_{i} W_{Q} \\\\ K_{i} \u0026= X_{i} W_{K}\\\\ V_{i} \u0026= X_{i} W_{V}\\\\ Y_{i} \u0026= \\sum_{j=1}^{N}sim(Q_i,K_{j}) V_j\\\\ sim(Q_{i},K_j) \u0026= \\frac{exp(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{d}})} {\\sum_{j=1}^N exp(\\frac{Q_iK_j^{T}}{\\sqrt{d}})}\\\\ \\end{aligned}\\) 可以从相似性入手，让位置的相对关系反应到q，k的相似性中来。 希望 相似性计算只依赖向量还有相对距离, 而不依赖于其绝对的位置。 \\(Q_{i}K_j^T=g(X_{i},X_j,i-j)\\) ","date":"2024-02-01","objectID":"/positional_embedding/:2:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"旋转位置编码 ","date":"2024-02-01","objectID":"/positional_embedding/:3:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"在二维空间中看motivation 假设\\(Q_{i}, K_j\\) 都是二维的向量，\\(i, j\\) 是它们对应的position， 这里\\(\\eta_{i},\\eta_{j}\\) 是$Q_i, K_j$向量的弧度表示对应的角度. 点击只和模长和夹角有关 \\(Q_iK_j^T=\\|Q_i\\|\\|K_j\\| cos(\\eta_{i}-\\eta_j)\\), 如果: 基于位置乘倍数旋转之后做点击 我们把两个向量各自旋转\\(i\\theta,j\\theta\\) 后再来计算点击 其中\\(\\theta\\) 是一个单位角度， 应该就只和\\(Q_i,Q_j,i-j\\) 相关了， 因为: 模长没有变，只是夹角变了，夹角增加了 \\((i-j)\\theta\\). \\(Q_iR(i\\theta)(K_jR(j\\theta))^T=\\|Q_i\\|\\|K_j\\| cos(\\eta_{i}-\\eta_{j}+(i-j)\\theta)\\) ","date":"2024-02-01","objectID":"/positional_embedding/:3:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"回顾矩阵的知识 关于行向量和矩阵 定义线性算子 \\(\\mathcal{A}\\) 可以作用到行向量 \\(\\mathcal{A}(X_i) = X_{i} A\\) 也可以作用到矩阵 \\(\\mathcal{A}(X) = XA\\) 右乘矩阵等于对每个行向量逐个施加行变换 \\(XA=\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix}A= \\begin{pmatrix} X_1 A\\\\ X_2 A\\\\ \\vdots\\\\ X_N A \\end{pmatrix}= \\begin{pmatrix} \\mathcal{A}(X_1) \\\\ \\mathcal{A}(X_2) \\\\ \\vdots\\\\ \\mathcal{A}(X_N) \\end{pmatrix}=\\mathcal{A}(X)\\) 算子是对矩阵乘法的一种物理理解 旋转矩阵 \\(R(\\theta)= \\begin{pmatrix} cos\\theta\u0026 sin\\theta\\\\ -sin\\theta\u0026 cos\\theta \\end{pmatrix}\\) 缩放变换 \\(R(\\lambda_1,\\lambda_2)=\\begin{pmatrix} \\lambda_1 \u0026 \\\\ \u0026 \\lambda_2 \\\\ \\end{pmatrix}\\) 关于旋转矩阵 旋转矩阵 \\(R(\\theta)= \\begin{pmatrix} cos\\theta\u0026 sin\\theta\\\\ -sin\\theta\u0026 cos\\theta \\end{pmatrix}\\) 物理意义 \\(X_iR(i\\theta)\\) 对位置在\\(i\\) 的语义向量\\(X_i\\) 逆时针旋转\\(i\\theta\\) 性质 \\(R(\\theta)^T=R(-\\theta)\\) \\(R(\\theta_1)(\\theta_2)=R(\\theta_1+\\theta_{2})\\) ","date":"2024-02-01","objectID":"/positional_embedding/:3:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"二维空间的一个解 基于旋转矩阵的一个解 \\begin{equation*} \\begin{split} Q_{i}\u0026= X_{i} W_{Q} R(i\\theta) \\\\ K_{j}\u0026= X_j W_{K} R(j\\theta)\\\\ Q_{i}K_j^T \u0026=X_{i}W_QR(i\\theta)R(j\\theta)^{T}W_K^{T}X_{j}^T\\\\ \u0026=X_{i}W_QR(i\\theta)R(j\\theta)^{T}W_K^{T}X_{j}^T\\\\ \u0026=X_{i}W_QR(i\\theta)R(-j\\theta)W_K^{T}X_{j}^T\\\\ \u0026=X_{i}W_QR((i-j)\\theta)W_K^{T}X_{j}^T\\\\ \u0026 =g(X_i,X_j,i-j)\\\\ \\end{split} \\end{equation*} 为什么是在投影之后旋转，不在投影之前转？ \\begin{equation*} \\begin{split} Q_{i}\u0026= f_{Q}(X_{i}, i) = X_{i} R(i\\theta) W_{Q} \\\\ K_{j}\u0026= f_{K}(X_{j}, j) = X_j R(j\\theta) W_{K} \\\\ Q_{i}K_j^T \u0026=X_{i}R(i\\theta)W_QW_KR(j\\theta)^{T}X_{j}^T\\\\ \u0026=?\\\\ \\end{split} \\end{equation*} ","date":"2024-02-01","objectID":"/positional_embedding/:3:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"推广到高纬的空间 整个空间分割成\\(d/2\\) 个子空间，在各个子空间上分别按照一个位置相关的角度旋转 定义 \\(R(i\\Theta)\\) \\(X_{i}R(i\\Theta)\\) 表示对\\(X_{i}\\) 在各个子空间分别做角度为\\(i\\theta_1,i\\theta_2,\\ldots,i\\theta_{d/2}\\) 的旋转. \\(\\Theta=(\\theta_{1},\\theta_2,\\ldots,\\theta_{d/2})\\) \\(R(i \\Theta)=\\begin{pmatrix} cos\\,i\\theta_{1} \u0026 sin\\,i\\theta_1 \u0026 0 \u0026 0 \\\\ -sin\\,i\\theta_{1} \u0026 cos\\,i\\theta_1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 cos\\,i\\theta_{2} \u0026 sin\\,i\\theta_2 \\\\ 0 \u0026 0 \u0026 -sin\\,i\\theta_{2} \u0026 cos\\,i\\theta_2 \\\\ \\end{pmatrix}=\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(i\\theta_2) \\end{pmatrix}\\) 物理意义 依次在独立的二维子空间上做旋转变换 利用分块矩阵的乘法，我们观察一下, 把对应行向量\\(X_i\\) 切分为两部分，用上角标来区分 \\(X_i = (X_{i}^1, X_{i}^2)\\) \\(XR(i\\Theta)=(X^1, X^2)\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(i\\theta_2) \\end{pmatrix}=(X^1R(i\\theta_1), X^2R(i\\theta_2))\\) 可以看出这个矩阵的变化的作用就是在各个独立的二维子空间上分别做独立的旋转变化，最后把变换后的向量拼接即可 性质: \\(R(i\\Theta)=\\widehat{R}(i\\theta_1)\\widehat{R}(i\\theta_2)\\ldots\\widehat{R}(i\\theta_{d/2})\\) 定义\\(\\widehat{R}(i\\theta_1)= \\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 0 \\\\ \\end{pmatrix}\\) \\(R(i\\Theta)=\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(i\\theta_2) \\end{pmatrix}=\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 0 \\\\ \\end{pmatrix}\\begin{pmatrix} 0 \u0026 0 \\\\ 0 \u0026 R(i\\theta_2) \\end{pmatrix}=\\widehat{R}(i\\theta_1)\\widehat{R}(i\\theta_2)\\) 在第一个二维空间按照 \\(\\theta_{1}\\) 来旋转，第二个 \\(\\theta_{2}\\) 来旋转 ROPE在高维空间 \\begin{equation*} \\begin{split} Q_{i}\u0026 = X_{i} W_{Q} R(i\\Theta) \\\\ K_{j}\u0026 = X_j W_{K} R(j\\Theta)\\\\ Q_{i}K_j^T \u0026=X_{i}W_QR(i\\Theta)R(j\\Theta)^{T}W_K^{T}X_{j}\\\\ \u0026=X_{i}W_QR(i\\Theta)R(j\\Theta)^{T}W_K^{T}X_{j}\\\\ \u0026=X_{i}W_QR((i-j)\\Theta)W_K^{T}X_{j}\\\\ \u0026=g(X_i,X_j,i-j)\\\\ \\end{split} \\end{equation*} 其中 \\begin{equation*} \\begin{split} R(i\\Theta)R(j\\Theta)^{T} \u0026= \\widehat{R}(i\\theta_1)\\widehat{R}(i\\theta_2)\\ldots\\widehat{R}(i\\theta_{d/2})\\widehat{R}(j\\theta_{d/2})^{T}\\ldots \\widehat{R}(j\\theta_{2})^{T} \\widehat{R}(j\\theta_{1})^{T} \\\\ \u0026= (\\widehat{R}(i\\theta_1)\\widehat{R}(j\\theta_1)^T)(\\widehat{R}(i\\theta_2)\\widehat{R}(j\\theta_2)^T)\\ldots(\\widehat{R}(i\\theta_{d/2}\\widehat{R}(j\\theta_{d/2})^T)\\\\ \u0026= \\widehat{R}((i-j)\\theta_1)\\widehat{R}((i-j)\\theta_2)\\ldots \\widehat{R}((i-j)\\theta_{d/2})\\\\ \u0026= R((i-j)\\Theta)\\\\ \\end{split} \\end{equation*} 其中\\(\\theta_{k}\\) 是超参数，\\(\\theta_{k}=10000^{-2(k-1)/d}, k\\in[1,2,\\ldots,d/2]\\) ","date":"2024-02-01","objectID":"/positional_embedding/:3:4","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"总结旋转位置编码 总结 旋转位置编码是针对\\(Q,K\\) 的每个行向量做对应的位置旋转变换 \\(Q_{i} = X_{i} W_{Q} R(i\\Theta)\\) \\(K_{j} = X_{j} W_{K} R(j\\Theta)\\) 位置旋转矩阵定义 \\(R(i\\Theta)\\) 其中 \\(\\Theta=(\\theta_{1},\\theta_2,\\ldots,\\theta_{d/2})\\), \\(\\theta_{k}=10000^{-2(k-1)/d}, k\\in[1,2,\\ldots,d/2]\\) 那么 \\(R(i\\theta)= \\begin{pmatrix} cos i\\theta\u0026 sin i\\theta\\\\ -sin i\\theta\u0026 cos i\\theta \\end{pmatrix}\\) \\(R(i \\Theta)=\\begin{pmatrix} cos\\,i\\theta_{1} \u0026 sin\\,i\\theta_1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u00260\\\\ -sin\\,i\\theta_{1} \u0026 cos\\,i\\theta_1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 cos\\,i\\theta_{2} \u0026 sin\\,i\\theta_2 \u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 -sin\\,i\\theta_{2} \u0026 cos\\,i\\theta_2\u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\ldots \u00260 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026\\ldots \u0026 cos\\,i\\theta_{d/2} \u0026 sin\\,i\\theta_{d/2} \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026\\ldots \u0026 -sin\\,i\\theta_{d/2} \u0026 cos\\,i\\theta_{d/2} \\end{pmatrix}\\) \\(R(i\\Theta)=\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \u00260 \u0026 0\\\\ 0 \u0026 R(i\\theta_2) \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026\\ldots \u00260 \\\\ 0 \u0026 0 \u0026 0 \u0026R(i\\theta_{d/2})\\\\ \\end{pmatrix}\\) 再看下绝对位置编码 \\(\\begin{aligned} P_{i,2t} \u0026= sin(i/10000^{2t/d}) \u0026\u0026\\\\ P_{i,2t+1} \u0026= cos(i/10000^{2t/d})\u0026\u0026\\\\ \\end{aligned}\\) 换个表述的形式， \\(P_{i}=\\begin{pmatrix} B_1, B_2, \\ldots, B_{d/2}\\end{pmatrix}\\)， \\(B_{k}=\\begin{pmatrix} sin(i\\theta_k), cos(i\\theta_k) \\end{pmatrix}\\) \\(\\theta_{k}=10000^{-2(k-1)/d}, k\\in[1,2,\\ldots,d/2]\\) ","date":"2024-02-01","objectID":"/positional_embedding/:3:5","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"代码实现 避开旋转矩阵的相乘 我们需要对每个\\(Q_{i}\\) 乘以不同的旋转矩阵，也就是 \\(QR=\\begin{pmatrix} Q_1 R(1\\Theta)\\\\ Q_2 R(2\\Theta)\\\\ \\ldots \\\\ Q_N R(N\\Theta)\\\\ \\end{pmatrix}\\) 假设是二维空间，把\\(Q\\) 拆分成两个列向量\\(U,V\\), 记录 \\(cos=\\begin{pmatrix}cos1\\theta \\\\ cos 2\\theta\\\\ \\ldots,\\\\ cos N\\theta \\end{pmatrix}, sin=\\begin{pmatrix}sin 1\\theta \\\\ sin 2\\theta\\\\ \\ldots,\\\\ sin N\\theta \\end{pmatrix}\\) 那么 \\(\\begin{aligned} QR\u0026=\\begin{pmatrix} u_1 cos 1\\theta-v_1 sin 1\\theta, u_1 sin 1\\theta + v_1 cos 1\\theta\\\\ u_2 cos 2\\theta-v_2 sin 2\\theta, u_2 sin 2\\theta + v_2 cos 2\\theta\\\\ \\ldots\\\\ u_N cos N\\theta-v_N sin N\\theta, u_N sin N \\theta + v_N cos N\\theta\\\\ \\end{pmatrix}\\\\ \u0026=(U * cos - V* sin, U*sin+V*cos) \\\\ \u0026= (U,V)cos +(V, -U) sin \\end{aligned}\\) 同样的，在高维空间，我们可以把\\(Q\\) 拆分成\\(d/2\\) 个列向量\\(U_1,V_1,U_2,V_2,\\ldots,U_{d/2},V_{d/2}\\) tricks trick2：不需要做严格紧密相连的二维子空间序列，将整个空间分成两部分 第一个部分放的是每个子空间的第一维度，第二部分放置的是每个子空间的第二维度 (x1,y1) 是一个子空间，(x2, y2)是一个子空间，(x3, y3)是一个子空间 before： [(x1,y1), (x2,y2), (x3,y3)] after： [(x1,x2,x3), (y1, y2, y3)] code import torch import torch.nn as nn import math from torch.nn import functional as F class Rotator: \"\"\"根据hidden_dim，和position_ids 生成对应的旋转位置编码, 和论文中定义略有不同，一个个二维的子空间被 分割到了前后两部分，分别进行旋转，然后拼接起来 \"\"\" def __init__(self, dim, position_ids): \"\"\" position_ids: [seq_len], dim 和单个头的hidden_dim对应 \"\"\" base = 10000 theta_base = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim)) thetas = position_ids.outer(theta_base) # [seq_len, D/2] full_thetas = torch.cat((thetas, thetas), dim=-1) # [seq_len, D] self.cos = full_thetas.cos() self.sin = full_thetas.sin() def rotate(self, x): \"\"\" x: [bs, num_attention_heads, seq_len, D] q: [bs, num_attention_heads, seq_len, D] cos: [seq_len, D] [x,y] @ [[cos, sin], [-sin, cos]] = [x*cos+y*sin, ycos-x*sin] =[x,y]*cos+[y, -x]*sin \"\"\" return x * self.cos + Rotator.reverse_half(x) * self.sin @staticmethod def reverse_half(q): \"\"\" q: [bs, num_attention_heads, seq_len, D] \"\"\" x = q[..., : q.shape[-1] // 2] y = q[..., q.shape[-1] // 2:] return torch.cat((-y, x), dim=-1) class SelfAttentionWithRoPE(nn.Module): def __init__(self, config): super().__init__() self.H = config[\"n_head\"] self.F = config[\"hidden_dim\"] # F self.D = self.F // self.H # D # 一次把qkv 全部映射完成，对应W_Q, W_K, W_V self.qkv_proj = nn.Linear(self.F, 3 * self.F) # 最后的投影，对应于 $W_O$ self.out_proj = nn.Linear(self.F, self.F) def forward(self, x, position_ids): # position_ids: [seq_len] B, N, _ = x.size() q, k, v = self.qkv_proj(x).split(self.F, dim=-1) # matmul 只能在最后两个维度相乘，需要对NxD的矩阵相乘，做1,2维度的交换 k = k.view(B, N, self.H, self.D).transpose(1, 2) q = q.view(B, N, self.H, self.D).transpose(1, 2) v = v.view(B, N, self.H, self.D).transpose(1, 2) # 旋转位置编码 rotator = Rotator(self.D, position_ids) q = rotator.rotate(q) k = rotator.rotate(k) # 计算相似性 att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v # 多头拼接 y = y.transpose(1, 2).contiguous().view(B, N, self.F) y = self.out_proj(y) return y config = {\"n_head\": 2, \"hidden_dim\": 16, \"batch_size\": 3, \"seq_len\": 5} attn = SelfAttentionWithRoPE(config) x = torch.rand(config[\"batch_size\"], config[\"seq_len\"], config[\"hidden_dim\"]) position_ids = torch.arange(config[\"seq_len\"]) y = attn(x, position_ids) ","date":"2024-02-01","objectID":"/positional_embedding/:3:6","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"参考论文 Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv. https://arxiv.org/abs/2104.09864. ","date":"2024-02-01","objectID":"/positional_embedding/:4:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"关于我 你好，我是连义江，本人博士毕业于北京大学数学科学学院，前百度凤巢主任架构师。 喜欢AI和数学。闲暇喜欢弹弹电吉他。 在这里，你会看到我对技术和生活的一些思考。 感谢你的访问！ ","date":"2024-02-02","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"}]