[{"categories":null,"content":"Whole hugo blog in plain text!","date":"2024-02-01","objectID":"/transformer/","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"transformer背景 ","date":"2024-02-01","objectID":"/transformer/:1:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"主要内容 主要内容 attention设计原理解读 tranformer中的矩阵/行向量乘法 transformer的pytorch代码实现 计算量\\(O(N^2)\\) 源于softmax的存在 从kernel的角度来看attention \\(\\mathcal{A}(X_i) = \\dfrac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\) linear attention 参考 Attention Is All You Need (Vaswani et al. 2023) Fast Autoregressive Transformers with Linear Attention (Katharopoulos et al. 2020) mingpt by karpathy ","date":"2024-02-01","objectID":"/transformer/:1:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"回顾线性代数的知识 why 原文比较晦涩 \\[\\begin{aligned}\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\dfrac{QK^T}{\\sqrt{d_k}})V \\\\ \\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_1,\\ldots,\\mathrm{head}_h)W^{O} \\\\ \\mathrm{head}_i=\\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \\end{aligned}\\] 把矩阵剖解成从行向量来看更容易理解 矩阵和行向量 矩阵 \\(X\\in R^{N\\times F}\\) \\(X=\\begin{pmatrix} X_{11}, X_{12},\\ldots, X_{1F} \\\\ X_{21}, X_{22},\\ldots, X_{2F} \\\\ \\vdots\\\\ X_{N1}, X_{N2},\\ldots, X_{NF} \\end{pmatrix}\\) 行向量 \\(X_{i}=\\begin{pmatrix} X_{i1}, X_{i2},\\ldots, X_{iF}\\end{pmatrix}, X_i \\in R^{1\\times F}\\) 分块矩阵 \\(X=\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix}\\) 比如nn.Embedding 按照行向量来组织数据 import torch import torch.nn as nn N = 3 F = 8 embed = nn.Embedding(N, F) idx = torch.tensor([1,2,3]) X = embed(idx) print(X.shape) 例子 \\(N\\) 个token，\\(F\\) 是embedding的维度 每行对应于一个token的embedding 行向量 \\(tokens=\\begin{pmatrix} \\text{hello} \\\\ \\text{world} \\\\ \\text{pad} \\\\ \\text{pad} \\\\ \\text{pad} \\end{pmatrix}\\) \\(X=\\begin{pmatrix} [0.59, 0.20, 0.04, 0.96] \\\\ [0.96, 0.30, 0.16, 0.63] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\end{pmatrix}\\) 矩阵相乘和算子作用 定义线性算子 \\(\\mathcal{A}\\) 可以作用到行向量 \\(\\mathcal{A}(X_i) = X_{i} A\\) 也可以作用到矩阵 \\(\\mathcal{A}(X) = XA\\) 右乘矩阵等于对每个行向量逐个施加行变换 \\(XA=\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix}A= \\begin{pmatrix} X_1 A\\\\ X_2 A\\\\ \\vdots\\\\ X_N A \\end{pmatrix}= \\begin{pmatrix} \\mathcal{A}(X_1) \\\\ \\mathcal{A}(X_2) \\\\ \\vdots\\\\ \\mathcal{A}(X_N) \\end{pmatrix}=\\mathcal{A}(X)\\) 代码对应于 nn.Linear import torch import torch.nn as nn F = 6 linear = nn.Linear(in_features=F, out_features=F) X_i = torch.rand(1, 6) X = torch.rand(3, 6) print(linear(X_i).shape) print(linear(X).shape) pytorch/tensorflow中的代码都是按照作用于行向量来组织的 从分块矩阵的乘法来看\\(QK^{T}V\\) \\(S=QK^T\\) 行向量两两计算点积相似性 \\(\\begin{pmatrix} Q_{1}\\\\ Q_{2}\\\\ \\vdots\\\\ Q_N \\end{pmatrix} \\begin{pmatrix} K_{1}^T, K_2^T,\\ldots,K_N^T\\\\ \\end{pmatrix}=(Q_{i}K_j^T)_{ij}=S\\) \\(SV\\) = 对行向量做加权求和 \\(\\begin{pmatrix} S_{11},S_{12},\\ldots, S_{1N}\\\\ S_{21},S_{22},\\ldots, S_{2N}\\\\ \\vdots\\\\ S_{N1},S_{N2},\\ldots, S_{NN}\\\\ \\end{pmatrix} \\begin{pmatrix} V_{1}\\\\ V_{2}\\\\ \\vdots\\\\ V_N \\end{pmatrix}= \\begin{pmatrix} \\sum\\limits_{j}S_{1j}V_j\\\\ \\sum\\limits_{j}S_{2j}V_j\\\\ \\vdots\\\\ \\sum\\limits_{j}S_{Nj}V_j \\end{pmatrix}\\) 基于Q,K计算相似性，然后基于V来加权求和 \\(QK^{T}V\\) 的每个行向量都是\\(V\\) 行向量的一个加权求和 注 左乘以一个矩阵相当于对每个列向量来施加变化 论文：一般会有行/列向量两种表示方式 代码：基本都是行向量来作为数据组织的标准 本文: 向量都按照行向量的形式来组织 按照作用于单个行向量的方式来讲解transformer ","date":"2024-02-01","objectID":"/transformer/:1:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"encoder-decoder 大部分的s2s 的任务建模为 encoder-decoder的结构 机器翻译，语音识别，文本摘要，问答系统等 encoder 把token序列\\((x_{1}, x_2,\\ldots, x_N)\\) 转化为语义向量序列 \\((Y_{1}, Y_2, \\ldots, Y_N)\\) 一般组织为多层的网络的形式 第一层：基础语义向量序列 \\((x_{1}, x_2,\\ldots, x_N)\\rightarrow (X_{1}, X_2,\\ldots, X_N)\\) 其它层：从低阶语义向量转化为高阶语义向量序列 \\((X_{1}, X_2,\\ldots, X_N)\\rightarrow (Y_{1}, Y_2,\\ldots, Y_N)\\) decoder 基于\\((Y_{1}, Y_2, \\ldots, Y_N)\\) 自回归式的逐个token解码 focus到 encoder部分来理解transformer ","date":"2024-02-01","objectID":"/transformer/:1:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"低阶到高阶语义向量的转换 encoder的主要工作是寻找算子\\(\\mathcal{T}\\) 将低阶的语义向量序列变换为高阶的语义向量序列 \\(\\mathcal{T}\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix} \\rightarrow\\begin{pmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_N \\end{pmatrix}\\) 输入: \\(X\\) 低阶语义向量序列，输出: \\(Y\\) 高阶语义向量序列 意义 \\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\) 对低阶语义向量做加工组合处理和抽象，变换为一个高阶的语义向量序列 高阶语义向量考虑了 上下文 的语义向量表达 motivation Firth a word is characterized by the company it keeps. 例子： The enigmatic smile on Mona Lisa’s face has intrigued art enthusiasts for centuries, leaving them to speculate about its true meaning. 用算子作用来表达 \\(Y=\\mathcal{T}(X)\\) \\(X \\in R^{N\\times F}\\), \\(Y=\\mathcal{T}(X): \\quad R^{N\\times F}\\rightarrow R^{N\\times F}\\) 这个算子天然可以复合嵌套，形成多层的网络结构 \\(Y=\\mathcal{T}_{L}\\circ \\mathcal{T}_{L-1}\\circ \\ldots \\circ \\mathcal{T}_{1}(X)\\) ","date":"2024-02-01","objectID":"/transformer/:1:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"核心的问题 问题 如何设计 \\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\) \\(Y_{1}, \\ldots, Y_N\\) 能否并行得到 \\(Y_{i}\\) 能否高效的建立起对周围token的远程依赖 RNN 递归语义序列 \\(Y_{0}\\rightarrow Y_1 \\rightarrow \\ldots \\rightarrow Y_{N}\\) \\(Y_{i}=tanh(X_{i}W + Y_{i-1}U)\\) 串行 单方向的依赖关系 \\(Y_{3}\\) 直接依赖于\\(Y_{2}, X_{3}\\), 间接依赖于\\(X_1\\) CNN \\(Y_{i}=(X_{i-1},X_i, X_{i+1}) W\\) 假设窗口宽度是3 并行 长距离依赖？ 一层卷积只能依赖于当前窗口内，不能对窗口外的形成依赖。 transformer思路 设计\\(Y_{i}=f(X_{1}, X_2, \\ldots, X_{N})\\)，使得 使得 \\(Y_{1},\\ldots, Y_N\\) 可以做并行计算 同时解决长距离依赖的问题 \\(Y=\\mathcal{F}\\circ \\mathcal{A}(X)\\) 做两次矩阵的变换 \\(Y=\\mathcal{A}(X)\\) MultiHead Attention 高阶的语义等于对 全部 的低阶语义向量基于 相似性(Attention) 做 加权平均 \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\\) attention = 相似性 \\(Y’=\\mathcal{F}(Y)\\) Position-wise Feedforward 再施加若干非线性变换 ","date":"2024-02-01","objectID":"/transformer/:1:5","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"tranformer网络结构 ","date":"2024-02-01","objectID":"/transformer/:2:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"基于KV查询的相似性计算 \\[\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\\] 直接计算相似性？ 参数太少 投影到别的空间来计算相似度 \\(X_{i}\\rightarrow X_iW\\) \\(\\begin{aligned} \\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_iW_1,X_jW_{2}) X_jW_3}{\\sum_{j=1}^N sim(X_iW_1,X_jW_2)} \\end{aligned}\\) 如果我们记 \\(X_{i}W_{1}=Q_i, X_iW_2=K_i, X_iW_3=V_{i}\\)， \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\\) 基于KV查询理解 把\\(X_i\\) 投影出三个向量 \\(Q_i,K_i,V_i\\) QKV KV 是大家熟悉的key-value存储 \\(K_{j}\\rightarrow V_{j}\\) Q 是查询使用的query向量 \\(Q_{i}\\) QKV的查询方法 query查询多个key，获取多个value 最后把这些value加权平均 \\(Q_i\\Rightarrow \\begin{pmatrix} K_{1}\\rightarrow V_{1}\\\\ K_2\\rightarrow V_2\\\\ \\vdots\\\\ K_N\\rightarrow V_N \\end{pmatrix} \\Rightarrow \\begin{pmatrix} sim(Q_i,K_1)V_{1} \\\\ sim(Q_i,K_2)V_{2} \\\\ \\vdots\\\\ sim(Q_i,K_N)V_N \\end{pmatrix}\\Rightarrow\\sum_{j=1}^N sim(Q_i,K_j)V_j\\) \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\\) 参数： 对应于\\(Q,K,V\\) 产生了三个投影矩阵 \\(W_{Q}, W_K,W_V\\) ","date":"2024-02-01","objectID":"/transformer/:2:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"在一个低维空间做attention 单个头的attention 把\\(X_{i}\\) 从\\(F\\) 维空间投影到\\(D\\) 维空间 \\(W_{Q}\\in R^{F\\times D}, W_K\\in R^{F\\times D}, W_{V} \\in R^{F\\times M}\\) \\(Q_i = X_iW_{Q}, \\quad K_i = X_iW_{K}, \\quad V_i = X_iW_{V}\\) \\(Q_i\\) 和所有的\\(K_j\\) 做基于点积的相似度计算， 这里简单起见，我们省略掉了scaling \\(\\frac{1}{\\sqrt{D}}\\) \\(Q_iK^{T}=Q_i(K^T_1, \\ldots, K^T_N)=(Q_iK^T_1, \\ldots, Q_iK^T_N)\\) 对相似度的分布做softmax \\(S=\\mathrm{soft}(Q_iK^T_1, \\ldots, Q_iK^T_N)=(s_{i1},\\ldots, s_{iN})\\) \\(s_{i,j}= \\dfrac{exp(Q_iK_j^T)}{\\sum_{j=1}^N exp(Q_iK_j^T)}\\) 加权平均 \\(\\mathcal{A}(X_i)=\\sum_{j=1}^Ns_jV_j=(s_{i1},\\ldots, s_{iN}) \\begin{pmatrix} V_1\\\\ V_2\\\\ \\vdots\\\\ V_N\\end{pmatrix}\\) \\(\\mathcal{A}(X_i) = \\mathrm{soft}(Q_iK^{T})V = \\mathrm{soft}(X_iW_QW_K^TX^T)XW_V\\) 矩阵表达 \\(Y=\\mathcal{A}(X) =\\begin{pmatrix} \\mathcal{A}(X_1)\\\\ \\mathcal{A}(X_2)\\\\ \\vdots\\\\ \\mathcal{A}(X_N) \\end{pmatrix} =\\begin{pmatrix} \\mathrm{soft}(Q_1K^T)V\\\\ \\mathrm{soft}(Q_2K^T)V\\\\ \\vdots \\\\ \\mathrm{soft}(Q_NK^T)V \\end{pmatrix}=\\mathrm{soft}(QK^T)V\\) 简化符号 \\(sim(Q,K)V\\) 代码实现 import torch import torch.nn as nn import math from torch.nn import functional as F class SingleHeadAttention(nn.Module): def __init__(self, config): super().__init__() self.F = config[\"hidden_dim\"] #F self.D = config[\"subspace_dim\"] #D self.q_proj = nn.Linear(self.F, self.D) self.k_proj = nn.Linear(self.F, self.D) self.v_proj = nn.Linear(self.F, self.D) def forward(self, x): B, N, F = x.size() q = self.q_proj(x) k = self.k_proj(x) v = self.v_proj(x) att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v return y 注: \\(D\\neq F\\) 时，\\(\\mathcal{A}(X)\\) 还不可用 ","date":"2024-02-01","objectID":"/transformer/:2:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"在多个低维空间做attention why Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 一词多义 把\\(F\\) 维的语义向量投影到 \\(H\\) 个不同的子空间中去计算相似加权组合 做法 每个头投做独立的Attention变换 \\(\\mathcal{A}^{h}(X)\\) 假设有\\(H\\) 个头，每个头作用的低维空间维度是\\(D\\) \\(D\\times H = F\\) 对\\(H\\) 个 \\(D\\) 行向量拼接 \\(W_O\\in R^{F\\times F}\\) \\(\\mathcal{A}(X) = \\mathrm{concat}(\\mathcal{A}^1(X), \\mathcal{A}^2(X), \\ldots, \\mathcal{A}^{H}(X) W_O\\) 或者对前面的符号简化 在第\\(j\\) 个子空间做单头注意力 \\(Y^{j}=sim(Q^{j}, K^{j})V^{j}\\) 合并 \\(Y=(Y^{1},\\ldots, Y^H)\\) 代码实现 # 参考 https://github.com/karpathy/minGPT/tree/master/mingpt import torch import torch.nn as nn import math from torch.nn import functional as F class SelfAttention(nn.Module): def __init__(self, config): super().__init__() self.H = config[\"n_head\"] self.F = config[\"hidden_dim\"] #F self.D = self.F // self.H #D # 一次把qkv 全部映射完成，对应W_Q, W_K, W_V self.qkv_proj = nn.Linear(self.F, 3 * self.F) # 最后的投影，对应于 $W_O$ self.out_proj = nn.Linear(self.F, self.F) def forward(self, x): B, N, F = x.size() q, k, v = self.qkv_proj(x).split(self.F, dim=-1) # matmul 只能在最后两个维度相乘，需要对NxD的矩阵相乘，做1,2维度的交换 k = k.view(B, N, self.H, self.D).transpose(1, 2) q = q.view(B, N, self.H, self.D).transpose(1, 2) v = v.view(B, N, self.H, self.D).transpose(1, 2) # (B,H,N,D) # 一次把多个头的映射全部完成, 对任意的(batch, head) att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v # (B,H,N,D) y = y.transpose(1, 2) # (B,N,H,D) # 最后两个维度合并 y = y.contiguous().view(B, N, F) y = self.out_proj(y) return y 代码示意 ","date":"2024-02-01","objectID":"/transformer/:2:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"位置无关的全连接 两层的全连接 \\(\\mathcal{F}(X_i)=(g(X_iW_1)+b_1)W_2+b_2\\) 代码 import torch import torch.nn as nn class PWiseFeedForward(nn.Module): def __init__(self, config): super().__init__() self.F = config[\"F\"] self.proj_wide = nn.Linear(self.F, 4 * self.F) self.proj_narrow = nn.Linear(4 * self.F, self.F) self.act = nn.ReLU() def forward(self, x): return self.proj_narrow(self.act(self.proj_wide(x))) ","date":"2024-02-01","objectID":"/transformer/:2:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"归一化 + 残差网络 \\(\\mathcal{T}(X)=\\mathcal{F}\\circ\\mathcal{A}(X)\\) Layer Normalization \\(\\mathcal{A}’(X)=\\mathcal{N}\\circ\\mathcal{A}(X)\\) \\(\\dfrac{x-\\mu}{\\sqrt{\\sigma}}\\gamma + \\beta,\\mu=\\dfrac{1}{d}\\sum\\limits_{i=1}^{d}x_{i}, \\sigma=\\sqrt{\\dfrac{1}{d}\\sum\\limits_{i=1}^{d}(x_{i}-\\mu)^{2}}\\) 可以看成是作用在行向量上的算子 行归一化 or 列归一化 在NLP的序列建模里面，Layer Normalization 在CV/CTR预估里面, Batch Normalization Why padding的影响 不同batch中\u003cpad\u003e个数不同，沿着token方向做归一化没有意义 每个位置做独立的归一化更有意义 输入矩阵例子 \\(\\begin{pmatrix} \\text{hello} \\\\ \\text{world} \\\\ \\text{pad} \\\\ \\text{pad} \\\\ \\text{pad} \\end{pmatrix} \\rightarrow X= \\begin{pmatrix} [0.59, 0.20, 0.04, 0.96] \\\\ [0.96, 0.30, 0.16, 0.63] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\\\ [0.02, 0.19, 0.34, 0.25] \\end{pmatrix}\\) 其他的可能选择 RMSNorm \\(\\dfrac{x}{\\text{RMS}(x)}, \\quad \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2}\\) ","date":"2024-02-01","objectID":"/transformer/:2:5","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"整体的变换 \\(Y=\\mathcal{T}(X)\\) Attention \\(Z=\\mathcal{N}\\circ(X+\\mathcal{A}(X))\\) 位置无关的全连接 \\(Y=\\mathcal{N}\\circ(X+\\mathcal{F}(Z))\\) residual network \\(\\mathcal{A}’(X)=\\mathcal{N}\\circ(X+\\mathcal{A}(X))\\) \\(\\mathcal{F}’(X)=\\mathcal{N}\\circ(X+\\mathcal{F}(X))\\) 多层 一个 \\(L\\) 层的transformer 模型 \\begin{equation*} \\begin{split} \\mathcal{T}(X) \u0026 = \\mathcal{T}_L \\circ \\ldots \\mathcal{T}_{2}\\circ \\mathcal{T}_{1}(X) \\end{split} \\end{equation*} 代码 import torch.nn as nn class Block(nn.Module): def __init__(self, config): super().__init__() self.layer_norm_1 = nn.LayerNorm(config[\"hiden_dim\"]) self.attn = SelfAttention(config) self.layer_norm_2 = nn.LayerNorm(config[\"hidden_dim\"]) self.mlp = PWiseFeedForward(config) def forward(self, x): x = self.layer_norm_1(x + self.attn(x)) x = self.layer_norm_2(x + self.mlp(x)) return x ","date":"2024-02-01","objectID":"/transformer/:2:6","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"transformer参数和计算量 ","date":"2024-02-01","objectID":"/transformer/:3:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"关于参数量 一般的模型增加复杂度的方式 增加深度，增加宽度 增加embedding的维度 增加词典的大小 各种dnn主要的参数位置 cnn: \\(Y_{i}=(X_{i-1},X_i, X_{i+1}) W\\) rnn: \\(Y_{i}=tanh(X_{i}W + Y_{i-1}U)\\) ","date":"2024-02-01","objectID":"/transformer/:3:1","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"参数的分布 多头注意力 \\(4F^2\\) 每个头有 3个投影矩阵 \\(W_Q, W_K, W_V\\) 1个投影concat结果的矩阵 \\(W_O\\) 参数量: 假设投射到的子空间维度是\\(D\\), \\(H\\) 个子空间，\\(D\\times H = F\\) \\(F\\times D \\times 3 \\times H = 3F^{2}\\) \\(F^{2}\\) FFW \\(8F^2\\) 两个矩阵，先从\\(F\\) 变宽到\\(4F\\)，再收窄回来到\\(F\\) 参数量\\(F\\times4F + 4F\\times F= 8F^{2}\\) word embedding \\(E\\) 是token字典的大小 \\(E\\times F\\) total \\(L(12F^{2})+EF\\) model 维度 层数 头数 字典大小 参数量 bertBase 768 12 12 30000 110M bertLarge 1024 24 12 30000 340M ","date":"2024-02-01","objectID":"/transformer/:3:2","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"linear transformer 两个算子的计算量 \\(\\mathcal{A}(X)\\) 计算量 \\(O(N^2)\\) \\(\\mathcal{F}(X)\\) 计算量 \\(O(N)\\) softmax 导致了\\(O(N^2)\\) 核心的计算量在这三个矩阵的相乘上，\\(QK^{T}V\\), 乘法的计算量密切依赖于矩阵组合的方式 有softmax的存在的话 只能先计算\\(H=QK^{T}\\), 对\\(H\\) 做softmax 变换后，再计算\\(HV\\) 乘法的计算量是 \\(N^2D+N^2M\\), 整体的复杂度是\\(O(N^{2})\\) \\(QK^TV=(QK^T)V=\\begin{pmatrix} H_{11},H_{12},\\ldots,H_{1N} \\\\ \\vdots\\\\ H_{N1},H_{N2},\\ldots,H_{NN} \\\\ \\end{pmatrix}V\\) 如果没有softmax的话 可以先计算后两个矩阵相乘\\(H=K^TV\\), 再计算\\(QH\\) 乘法的计算量是 \\(NDM+DMN=2NDM\\)，当\\(N\\gg D\\) 的时候, 计算量可以是\\(O(N)\\), \\(K^TV\\) 提前算出来缓存，大致如下面这个表达所示 \\(Q(K^TV)=\\begin{pmatrix} Q_1 \\\\ Q_2 \\\\ \\vdots\\\\ Q_{N} \\end{pmatrix}(K^TV)\\) kernel \\(\\mathcal{A}(X_i)=\\dfrac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)}\\) kernel: \\(k(x,y)=\u003c\\phi(x),\\phi(y)\u003e\\) \\(k(x,y)=(x\\cdot z)^2, \\phi(x)=(x_{1}^{2},x_{2}^2,\\sqrt{2}x_1x_{2})\\) kernel 对应一个feature map 可以用非负的kernel来替换掉 当前的sim函数 \\(sim(x,y)=\\mathrm{exp}(xy^{T}/\\sqrt{D})\\) linear transformer \\(O(N)\\) 用kernel来替换掉sim \\[\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\\\ \u0026=\\frac{\\sum_{j=1}^{N} \\phi(Q_i)\\phi(K_j)^T V_j}{\\sum_{j=1}^N \\phi(Q_i)\\phi(K_j)^T} \\\\ \u0026=\\frac{ \\phi(Q_i) \\sum_{j=1}^{N}\\phi(K_j)^T V_j}{\\phi(Q_i)\\sum_{j=1}^N \\phi(K_j)^T} \\end{aligned} \\] \\(\\sum_{j=1}^{N}\\phi(K_j)^T V, \\sum_{j=1}^N \\phi(K_j)^T\\) 可以提前算好 \\(O(N)\\) 复杂度，Linear Transformer \\(\\phi(x)=\\mathrm{elu}(x)+1\\) ","date":"2024-02-01","objectID":"/transformer/:3:3","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"总结 attention的设计原理解读 从低阶语义向量到高阶语义向量的转化 \\(\\mathcal{T}\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix} \\rightarrow\\begin{pmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_N \\end{pmatrix}\\) \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(X_i,X_j) X_j}{\\sum_{j=1}^N sim(X_i,X_j)} \\end{aligned}\\) \\(\\mathcal{A}(X_i)=\\dfrac{\\sum_{j=1}^{N} sim(X_iW_Q,X_jW_{K}) X_jW_{V}}{\\sum_{j=1}^N sim(X_iW_Q,X_jW_K)}\\) \\(\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\end{aligned}\\) transformer的核心两次变换 \\(Y=\\mathcal{F}\\circ \\mathcal{A}(X)\\) 做两次矩阵的变换 核心的计算量在这三个矩阵的相乘上，\\(QK^{T}V\\) \\((QK^T)V\\) 计算量 \\(O(N^2)\\) \\(Q(K^TV)\\) 计算量 \\(O(N)\\) linear transformer \\[\\begin{aligned}\\mathcal{A}(X_i) \u0026= \\frac{\\sum_{j=1}^{N} sim(Q_i,K_j) V_j}{\\sum_{j=1}^N sim(Q_i,K_j)} \\\\ \u0026=\\frac{\\sum_{j=1}^{N} \\phi(Q_i)\\phi(K_j)^T V_j}{\\sum_{j=1}^N \\phi(Q_i)\\phi(K_j)^T} \\\\ \u0026=\\frac{ \\phi(Q_i) \\sum_{j=1}^{N}\\phi(K_j)^T V_j}{\\phi(Q_i)\\sum_{j=1}^N \\phi(K_j)^T} \\end{aligned} \\] ","date":"2024-02-01","objectID":"/transformer/:3:4","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"参考论文 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. “Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention.” Arxiv.Org. https://arxiv.org/abs/2006.16236v3. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762. ","date":"2024-02-01","objectID":"/transformer/:4:0","tags":["transformer"],"title":"深入理解transformer","uri":"/transformer/"},{"categories":null,"content":"Whole hugo blog in plain text!","date":"2024-02-01","objectID":"/positional_embedding/","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"背景 ","date":"2024-02-01","objectID":"/positional_embedding/:1:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"主要内容 旋转位置编码在大模型中应用广泛 google PaLM meta llama 内容 旋转位置编码的来龙去脉 基于实数域上的旋转变换来推演 pytorch 代码实现 tricks 重新看解绝对位置编码 \\(\\begin{aligned} P_{i,2t} \u0026= sin(i/10000^{2t/D}) \\\\ P_{i,2t+1} \u0026= cos(i/10000^{2t/D}) \\end{aligned}\\) 参考： Enhanced Transformer with Rotary Position Embedding (Su et al. 2022) Attention Is All You Need (Vaswani et al. 2023) hugging face llama ","date":"2024-02-01","objectID":"/positional_embedding/:1:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"回顾transformer encoder: 寻找算子\\(\\mathcal{T}\\) 低阶语义向量序列转化为高阶的语义向量序列 \\(\\mathcal{T}\\begin{pmatrix} X_1\\\\ X_2\\\\ \\vdots\\\\ X_N \\end{pmatrix} \\rightarrow\\begin{pmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_N \\end{pmatrix}\\) \\(Y=\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) Attention \\(\\mathcal{A}\\) Feedforward \\(\\mathcal{F}\\) Attention \\(\\begin{aligned}Q_{i} \u0026= X_{i} W_{Q} \\\\ K_{i} \u0026= X_{i} W_{K}\\\\ V_{i} \u0026= X_{i} W_{V}\\\\ Y_{i} \u0026= \\sum_{j=1}^{N}sim(Q_i,K_{j}) V_j\\\\ sim(Q_{i},K_j) = \u0026= \\frac{exp(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{D}})} {\\sum_{j=1}^N exp(\\frac{Q_iK_j^{T}}{\\sqrt{D}})}\\\\ \\end{aligned}\\) ","date":"2024-02-01","objectID":"/positional_embedding/:1:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"为什么需要位置编码 因为transformer结构本身是和位置无关的 \\(Y=\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) 高阶语义向量不仅仅是由周围token的语义向量组合表达而成 还需要加上每个token所处的位置 下面的cls token得到的语义向量是完全一样的。 \u003ccls\u003e 从 北京 到 上海 的 火车票 \u003ccls\u003e 从 上海 到 北京 的 火车票 其他的网络结构天然有序列的位置信息 RNN/CNN ","date":"2024-02-01","objectID":"/positional_embedding/:1:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"如何加入位置编码 \\(\\mathcal{T}(X)=\\mathcal{F}(\\mathcal{A}(X))\\) \\(\\mathcal{F}\\) 是位置无关的 可以修改 \\(X\\) 或者 \\(\\mathcal{A}\\) ","date":"2024-02-01","objectID":"/positional_embedding/:2:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"直接修改输入 在\\(X_i \\rightarrow Q_i, K_i, V_i\\) 之前，直接加入位置的embedding \\(X_i^{’}=X_i+P_i\\) learned embedding 优点简单，bert/GPT 外推困难，对于超过序列最大长度的位置 自定义绝对位置编码 二维函数 f(position, dimension) 要求 函数随着position,dimension增长应该是有界的 足够的区分度，对每个position,对应的行向量应该是不同的 例子 \\(\\begin{aligned} P_{i,2t} \u0026= sin(k/10000^{2t/D}) \u0026\u0026\\\\ P_{i,2t+1} \u0026= cos(k/10000^{2t/D})\u0026\u0026\\\\ \\end{aligned}\\) \\(t\\in[0,1,\\ldots,D/2-1], i\\ge0\\) 问题: 语义应该是和相对位置有关的，而不是绝对位置 ","date":"2024-02-01","objectID":"/positional_embedding/:2:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"修改Attention Attention \\(\\begin{aligned}Q_{i} \u0026= X_{i} W_{Q} \\\\ K_{i} \u0026= X_{i} W_{K}\\\\ V_{i} \u0026= X_{i} W_{V}\\\\ Y_{i} \u0026= \\sum_{j=1}^{N}sim(Q_i,K_{j}) V_j\\\\ sim(Q_{i},K_j) \u0026= \\frac{exp(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{D}})} {\\sum_{j=1}^N exp(\\frac{Q_iK_j^{T}}{\\sqrt{D}})}\\\\ \\end{aligned}\\) 想法 可以从相似性入手，i和j之间的语义的相似性应该包含相对的距离信息 希望相似性计算只依赖向量还有相对距离,而不依赖于其绝对的位置。 \\(Q_{i}K_j^T=g(X_{i},X_j,i-j)\\) ","date":"2024-02-01","objectID":"/positional_embedding/:2:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"回顾矩阵的知识 关于行向量和矩阵 定义线性算子 \\(\\mathcal{A}\\) 可以作用到行向量 \\(\\mathcal{A}(X_i) = X_{i} A\\) 也可以作用到矩阵 \\(\\mathcal{A}(X) = XA\\) 右乘矩阵等于对每个行向量逐个施加行变换 线性算子是对矩阵乘法的一种物理理解 旋转变换 \\(R(\\theta)= \\begin{pmatrix} cos\\theta\u0026 sin\\theta\\\\ -sin\\theta\u0026 cos\\theta \\end{pmatrix}\\) 缩放变换 \\(R(\\lambda_1,\\lambda_2)=\\begin{pmatrix} \\lambda_1 \u0026 \\\\ \u0026 \\lambda_2 \\\\ \\end{pmatrix}\\) 用对角阵在正交的子空间上施加不同的行变换 假设有两个方阵A,B，设 \\(X= (X^1, X^2)\\), 那么 \\((X^1,X^2)\\begin{pmatrix} A \u0026 0 \\\\ 0 \u0026 B \\end{pmatrix} = (X^1A, X^2 B)\\) 注： pytorch/tensorflow 中的矩阵相关代码都是按照行向量来组织的 在ROPE 论文是按照列向量来撰写的，表现为是用矩阵左乘以一个列向量 本文中出现的向量全部用行向量来表达，和代码一致 关于旋转矩阵 在二维子空间的旋转矩阵 \\(R(\\theta)= \\begin{pmatrix} cos\\theta\u0026 sin\\theta\\\\ -sin\\theta\u0026 cos\\theta \\end{pmatrix}\\) 物理意义 \\(XR(\\theta)\\) 对\\(X\\) 逆时针旋转\\(\\theta\\) 证明 \\(X=\\rho(cos\\phi, sin\\phi)\\) \\(\\begin{aligned} \u0026XR(\\theta)\\\\ =\u0026\\rho(cos \\phi, sin \\phi) \\begin{pmatrix} cos\\theta\u0026 sin\\theta\\\\ -sin\\theta\u0026 cos\\theta \\end{pmatrix} \\\\ =\u0026 \\rho( cos\\phi cos\\theta - sin\\phi sin\\theta, cos\\phi sin\\theta + sin\\phi cos\\theta )\\\\ =\u0026 \\rho(cos(\\phi+\\theta), sin(\\phi+\\theta)) \\end{aligned}\\) 性质 \\(R(\\theta)^T=R(-\\theta)\\) \\(R(\\theta_1)R(\\theta_2)=R(\\theta_1+\\theta_{2})\\) 在高维空间中旋转 假设空间是偶数维的，把原始的空间切分成为一个正交的二维子空间，在上面做独立的旋转。 定义 \\(\\Theta=(\\theta_{1},\\theta_2,\\ldots,\\theta_{D/2})\\) \\(R(\\Theta)=\\begin{pmatrix} cos\\,\\theta_{1} \u0026 sin\\,\\theta_1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u00260\\\\ -sin\\,\\theta_{1} \u0026 cos\\,\\theta_1 \u0026 0 \u0026 0 \u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 cos\\,\\theta_{2} \u0026 sin\\,\\theta_2 \u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 -sin\\,\\theta_{2} \u0026 cos\\,\\theta_2\u0026 0 \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 \\ldots \u00260 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026\\ldots \u0026 cos\\,\\theta_{D/2} \u0026 sin\\,\\theta_{D/2} \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026\\ldots \u0026 -sin\\,\\theta_{D/2} \u0026 cos\\,\\theta_{D/2} \\end{pmatrix}\\) \\(R(\\Theta)=\\begin{pmatrix} R(\\theta_{1}) \u0026 0 \u00260 \u0026 0\\\\ 0 \u0026 R(\\theta_2) \u0026 0 \u00260 \\\\ 0 \u0026 0 \u0026\\ldots \u00260 \\\\ 0 \u0026 0 \u0026 0 \u0026R(\\theta_{D/2})\\\\ \\end{pmatrix}\\) 性质 在独立的二维子空间上做不同角度的旋转 \\(XR(\\Theta)=(X^1, X^2) \\begin{pmatrix} R(\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(\\theta_2) \\end{pmatrix}=(X^1R(\\theta_1), X^2R(\\theta_2))\\) \\(R(\\Theta)=\\widehat{R}(\\theta_1)\\widehat{R}(\\theta_2)\\ldots\\widehat{R}(\\theta_{D/2})\\) 逐个在不同的子空间上做旋转 定义 \\(\\widehat{R}(\\theta)= \\begin{pmatrix} R(\\theta) \u0026 0 \\\\ 0 \u0026 1 \\\\ \\end{pmatrix}\\) \\(R(\\Theta)=\\begin{pmatrix} R(\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(\\theta_2) \\end{pmatrix}=\\begin{pmatrix} R(\\theta_{1}) \u0026 0 \\\\ 0 \u0026 1 \\\\ \\end{pmatrix}\\begin{pmatrix} 1 \u0026 0 \\\\ 0 \u0026 R(\\theta_2) \\end{pmatrix}=\\widehat{R}(\\theta_1)\\widehat{R}(\\theta_2)\\) ","date":"2024-02-01","objectID":"/positional_embedding/:2:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"旋转位置编码 ","date":"2024-02-01","objectID":"/positional_embedding/:3:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"motivation 假设\\(Q_{i}, K_j\\) 都是二维的向量，\\(i, j\\) 是它们对应的position， 这里\\(\\eta_{i},\\eta_{j}\\) 是\\(Q_i, K_j\\) 弧度表示. 点积只和模长和夹角有关 \\(Q_iK_j^T=\\|Q_i\\|\\|K_j\\| cos(\\eta_{j}-\\eta_i)\\), 如何在这里融入位置的信息？基于位置乘倍数旋转之后做点击 做法： 我们把两个向量各自旋转\\(i\\theta,j\\theta\\) 后再来计算点积 其中\\(\\theta\\) 是一个单位角度， 新的向量的内积带上了位置信息，且他们的内积只和\\(Q_i,Q_j,i-j\\) 相关 因为: 模长没有变，只是夹角变了，夹角增加了 \\((j-i)\\theta\\). \\(Q_iR(i\\theta)(K_jR(j\\theta))^T=\\|Q_i\\|\\|K_j\\| cos(\\eta_{j}-\\eta_{i}+(j-i)\\theta)\\) ","date":"2024-02-01","objectID":"/positional_embedding/:3:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"二维空间中的一个解 基于旋转矩阵的一个解 \\begin{equation*} \\begin{split} Q_{i}\u0026= X_{i} W_{Q} R(i\\theta) \\\\ K_{j}\u0026= X_j W_{K} R(j\\theta)\\\\ Q_{i}K_j^T \u0026=X_{i}W_QR(i\\theta)R(j\\theta)^{T}W_K^{T}X_{j}^T\\\\ \u0026=X_{i}W_QR(i\\theta)R(-j\\theta)W_K^{T}X_{j}^T\\\\ \u0026=X_{i}W_QR((i-j)\\theta)W_K^{T}X_{j}^T\\\\ \u0026 =g(X_i,X_j,i-j)\\\\ \\end{split} \\end{equation*} 为什么是在投影之后旋转，不在投影之前转？ \\begin{equation*} \\begin{split} Q_{i}\u0026= X_{i} R(i\\theta) W_{Q} \\\\ K_{j}\u0026= X_j R(j\\theta) W_{K} \\\\ Q_{i}K_j^T \u0026=X_{i}R(i\\theta)W_QW_KR(j\\theta)^{T}X_{j}^T\\\\ \u0026=?\\\\ \\end{split} \\end{equation*} ","date":"2024-02-01","objectID":"/positional_embedding/:3:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"推广到高维空间 整个空间分割成\\(D/2\\) 个子空间，在各个子空间上分别按照一个位置相关的角度旋转 定义 \\(R(i\\Theta)\\) \\(X_{i}R(i\\Theta)\\) 表示对\\(X_{i}\\) 在各个子空间分别做角度为\\(i\\theta_1,i\\theta_2,\\ldots,i\\theta_{D/2}\\) 的旋转. \\(\\Theta=(\\theta_{1},\\theta_2,\\ldots,\\theta_{D/2})\\) \\(R(i \\Theta)=\\begin{pmatrix} cos\\,i\\theta_{1} \u0026 sin\\,i\\theta_1 \u0026 0 \u0026 0 \\\\ -sin\\,i\\theta_{1} \u0026 cos\\,i\\theta_1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 cos\\,i\\theta_{2} \u0026 sin\\,i\\theta_2 \\\\ 0 \u0026 0 \u0026 -sin\\,i\\theta_{2} \u0026 cos\\,i\\theta_2 \\\\ \\end{pmatrix}=\\begin{pmatrix} R(i\\theta_{1}) \u0026 0 \\\\ 0 \u0026 R(i\\theta_2) \\end{pmatrix}\\) ROPE在高维空间 \\begin{equation*} \\begin{split} Q_{i}\u0026 = X_{i} W_{Q} R(i\\Theta) \\\\ K_{j}\u0026 = X_j W_{K} R(j\\Theta)\\\\ Q_{i}K_j^T \u0026=X_{i}W_QR(i\\Theta)R(j\\Theta)^{T}W_K^{T}X_{j}^{T}\\\\ \u0026=X_{i}W_QR(i\\Theta)R(-j\\Theta)W_K^{T}X_{j}^{T}\\\\ \u0026=X_{i}W_QR((i-j)\\Theta)W_K^{T}X_{j}^{T}\\\\ \u0026=g(X_i,X_j,i-j)\\\\ \\end{split} \\end{equation*} 其中 \\begin{equation*} \\begin{split} R(i\\Theta)R(j\\Theta)^{T} \u0026= \\widehat{R}(i\\theta_1)\\widehat{R}(i\\theta_2)\\ldots\\widehat{R}(i\\theta_{D/2})\\widehat{R}(j\\theta_{D/2})^{T}\\ldots \\widehat{R}(j\\theta_{2})^{T} \\widehat{R}(j\\theta_{1})^{T} \\\\ \u0026= (\\widehat{R}(i\\theta_1)\\widehat{R}(j\\theta_1)^T)(\\widehat{R}(i\\theta_2)\\widehat{R}(j\\theta_2)^T)\\ldots(\\widehat{R}(i\\theta_{D/2}\\widehat{R}(j\\theta_{D/2})^T)\\\\ \u0026= \\widehat{R}((i-j)\\theta_1)\\widehat{R}((i-j)\\theta_2)\\ldots \\widehat{R}((i-j)\\theta_{D/2})\\\\ \u0026= R((i-j)\\Theta)\\\\ \\end{split} \\end{equation*} ","date":"2024-02-01","objectID":"/positional_embedding/:3:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"整体看下 空间是\\(D\\) 维度，\\(d=D/2\\) 有\\(d\\) 个正交的二维子空间 \\(\\mathcal{X}_1, \\mathcal{X}_2, \\dots, \\mathcal{X}_{d}\\) 每个子空间\\(\\mathcal{X}_{k}\\) 有一个旋转角度基准 \\(\\theta_{k}\\), 一个基准旋转矩阵 \\(R(\\theta_{k})\\) 合并后的基准角度序列和旋转序列是 \\(\\Theta, R(\\Theta)\\) 每个子空间对应于三角函数中的一个周期 \\(2\\pi/\\theta_{k}\\) 对于每个位置\\(i\\), 角度序列和旋转序列是 \\(i\\Theta, R(i\\Theta)\\) \\(\\Theta\\) \\(\\theta_1\\) \\(\\theta_2\\) \\(\\theta_3\\) \\(\\ldots\\) \\(\\theta_{d}\\) \\(R(\\Theta)\\) \\(R(\\theta_1)\\) \\(R(\\theta_{2})\\) \\(R(\\theta_3)\\) \\(\\ldots\\) \\(R(\\theta_{d})\\) \\(i\\Theta\\) \\(i\\theta_1\\) \\(i\\theta_{2}\\) \\(i\\theta_3\\) \\(\\ldots\\) \\(i\\theta_{d}\\) 具体化 \\(\\theta_{k}\\) 是超参数 \\(\\theta_{k}=10000^{-2(k-1)/D}, k\\in[1,2,\\ldots,D/2]\\)，记\\(B=10000^{1/d}\\) \\(\\theta_{k}=1/B^{k-1}\\) 是一个几何级数 周期随着维度\\(k\\) 逐渐增大 \\(\\Theta\\) \\(1\\) \\(1/B\\) \\(1/B^{2}\\) \\(\\ldots\\) \\(1/B^{d-1}\\) \\(T\\) \\(2\\pi\\) \\(2B\\pi\\) \\(2B^{2}\\pi\\) \\(\\ldots\\) \\(2B^{d-1}\\pi\\) 随着位置的增大，位置编码是否会重复？ 如果存在位置$i$和 0 位置的编码撞车了， \\(\\Theta\\) 序列在 \\(2\\pi\\) 周期整数倍上撞车 \\(\\Theta\\) 全部都是 $2π$的整数倍 对dimension的每个\\(k\\), 存在着一个整数 \\(I_k\\), 使得\\(i\\theta_{k}=2\\pi I_k\\) 不可能，\\(\\theta_k=1/10000^{k/d}\\) 和 \\(2\\pi\\) 不会扯上关系 RoPE 可能的另外一个优势 在多个block 前向传递的过程中position的信息不会丢失 每个block都会先做QKV的投影，然后QK投影之后会做位置旋转变换 ","date":"2024-02-01","objectID":"/positional_embedding/:3:4","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"再看下绝对位置编码 \\(\\begin{aligned} P_{i,2t} \u0026= sin(i/10000^{2t/D}) \u0026\u0026\\\\ P_{i,2t+1} \u0026= cos(i/10000^{2t/D})\u0026\u0026\\\\ \\end{aligned}\\) \\(t\\in[0,1,\\ldots,D/2-1], i\\ge0\\) 如果记\\(d=D/2,B=1/10000^{1/d}\\)， 那么\\(\\theta_{k}=1/B^{k-1}, k\\in[1,2,\\ldots,d]\\) structure 有\\(d\\) 个正交的二维子空间 \\(\\mathcal{X}_1, \\mathcal{X}_2, \\dots, \\mathcal{X}_{d}\\) 每个子空间\\(\\mathcal{X}_{k}\\) 有一个基础角度 \\(\\theta_{k}\\)， 两个基底 \\(sin\\theta_{k}, cos\\theta_{k}\\) ，记作\\(\\text{Tri}(\\theta_k)=(sin(\\theta_k), cos(\\theta_k))\\) 合并后的基准角度序列和基底序列是 \\(\\Theta, \\text{Tri} (\\Theta)\\) 由 \\(\\theta_{k}\\) 来决定各个子空间的不同 子空间内部由sin,cos 来区分 对于每个位置\\(i\\), 基准角度序列和基底序列是 \\(i\\Theta, \\text{Tri}(i\\Theta)\\) \\(\\Theta\\) \\(\\theta_1\\) \\(\\theta_{2}\\) \\(\\theta_3\\) \\(\\ldots\\) \\(\\theta_{d}\\) \\(\\text{Tri}(\\Theta)\\) \\(\\text{Tri}(\\theta_1)\\) \\(\\text{Tri}(\\theta_{2})\\) \\(\\text{Tri}(\\theta_3)\\) \\(\\ldots\\) \\(\\text{Tri}(\\theta_{d})\\) \\(i\\Theta\\) \\(i\\theta_1\\) \\(i\\theta_{2}\\) \\(i\\theta_3\\) \\(\\ldots\\) \\(i\\theta_{d}\\) 具体化 \\(\\theta_{k}=1/B^{k-1}\\) 是一个几何级数序列 周期随着维度\\(k\\) 逐渐增大 \\(\\Theta\\) \\(0\\) \\(1/B\\) \\(1/B^{2}\\) \\(\\ldots\\) \\(1/B^{d}\\) \\(T\\) \\(2\\pi\\) \\(2B\\pi\\) \\(2B^{2}\\pi\\) \\(\\ldots\\) \\(2B^{d}\\pi\\) 如果我们记录 \\(i=x\\) \\(\\{sin(\\theta_k x), cos(\\theta_{k} x)\\}_{k=1}^{D}\\) 很像对位置函数\\(f(x)\\) 的一个fourier展开 同理，随着位置的增大，位置编码不会重复 ","date":"2024-02-01","objectID":"/positional_embedding/:3:5","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"代码实现 ","date":"2024-02-01","objectID":"/positional_embedding/:4:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"trick1: 避开旋转矩阵的相乘 why？ 我们需要对每个\\(Q_{i}\\) 乘以不同的旋转矩阵，也就是 \\(QR=\\begin{pmatrix} Q_1 R(1\\Theta)\\\\ Q_2 R(2\\Theta)\\\\ \\ldots \\\\ Q_N R(N\\Theta)\\\\ \\end{pmatrix}\\) 而每个\\(R(i\\Theta)\\) 是一个稀疏矩阵，直接matmul代价太大 how? 假设是二维空间，把\\(Q\\) 拆分成两个列向量\\(U,V\\), 记录 \\(cos=\\begin{pmatrix}cos1\\theta \\\\ cos 2\\theta\\\\ \\ldots,\\\\ cos N\\theta \\end{pmatrix}, sin=\\begin{pmatrix}sin 1\\theta \\\\ sin 2\\theta\\\\ \\ldots,\\\\ sin N\\theta \\end{pmatrix}\\) 那么 \\(\\begin{aligned} QR\u0026=\\begin{pmatrix} u_1 cos 1\\theta-v_1 sin 1\\theta, u_1 sin 1\\theta + v_1 cos 1\\theta\\\\ u_2 cos 2\\theta-v_2 sin 2\\theta, u_2 sin 2\\theta + v_2 cos 2\\theta\\\\ \\ldots\\\\ u_N cos N\\theta-v_N sin N\\theta, u_N sin N \\theta + v_N cos N\\theta\\\\ \\end{pmatrix}\\\\ \u0026=(U * cos - V* sin, U*sin+V*cos) \\\\ \u0026= (U,V)cos +(-V, U) sin \\end{aligned}\\) 同样的，在高维空间，我们可以把\\(Q\\) 拆分成\\(D/2\\) 个列向量\\(U_1,V_1,U_2,V_2,\\ldots,U_{D/2},V_{D/2}\\) ","date":"2024-02-01","objectID":"/positional_embedding/:4:1","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"trick2: 将整个空间分成两部分 不需要做严格紧密相连的二维子空间序列 第一个部分放的是每个子空间的第一维度， 第二部分放置的是每个子空间的第二维度 (x1,y1) 是一个子空间，(x2, y2)是一个子空间，(x3, y3)是一个子空间 before： [(x1,y1), (x2,y2), (x3,y3)] after： [(x1,x2,x3), (y1, y2, y3)] ","date":"2024-02-01","objectID":"/positional_embedding/:4:2","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"code import torch import torch.nn as nn import math from torch.nn import functional as F class Rotator: \"\"\"根据hidden_dim，和position_ids 生成对应的旋转位置编码, 和论文中定义略有不同，一个个二维的子空间被 分割到了前后两部分，分别进行旋转，然后拼接起来 \"\"\" def __init__(self, D, position_ids): \"\"\" position_ids: [seq_len], D 和单个头的hidden_dim对应 \"\"\" base = 10000 d = D / 2 B = base ** (1/d) theta_base = 1.0 / (B ** (torch.arange(0, d))) # 几何级数序列 thetas = position_ids.outer(theta_base) # [seq_len, D/2] full_thetas = torch.cat((thetas, thetas), dim=-1) # [seq_len, D] self.cos = full_thetas.cos() self.sin = full_thetas.sin() def rotate(self, x): \"\"\" trick1 x: [bs, num_attention_heads, seq_len, D] q: [bs, num_attention_heads, seq_len, D] cos: [seq_len, D] [x,y] @ [[cos, sin], [-sin, cos]] = [x*cos-y*sin, ycos+x*sin] =[x,y]*cos+[-y, x]*sin \"\"\" return x * self.cos + Rotator.reverse_half(x) * self.sin @staticmethod def reverse_half(q): \"\"\" q: [bs, num_attention_heads, seq_len, D] trick2 \"\"\" x = q[..., : q.shape[-1] // 2] y = q[..., q.shape[-1] // 2:] return torch.cat((-y, x), dim=-1) class SelfAttentionWithRoPE(nn.Module): def __init__(self, config): super().__init__() self.H = config[\"n_head\"] self.F = config[\"hidden_dim\"] # F self.D = self.F // self.H # D # 一次把qkv 全部映射完成，对应W_Q, W_K, W_V self.qkv_proj = nn.Linear(self.F, 3 * self.F) # 最后的投影，对应于 $W_O$ self.out_proj = nn.Linear(self.F, self.F) def forward(self, x, position_ids): # position_ids: [seq_len] B, N, _ = x.size() q, k, v = self.qkv_proj(x).split(self.F, dim=-1) # matmul 只能在最后两个维度相乘，需要对NxD的矩阵相乘，做1,2维度的交换 k = k.view(B, N, self.H, self.D).transpose(1, 2) q = q.view(B, N, self.H, self.D).transpose(1, 2) v = v.view(B, N, self.H, self.D).transpose(1, 2) # 旋转位置编码 rotator = Rotator(self.D, position_ids) q = rotator.rotate(q) k = rotator.rotate(k) # 计算相似性 att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) att = F.softmax(att, dim=-1) y = att @ v # 多头拼接 y = y.transpose(1, 2).contiguous().view(B, N, self.F) y = self.out_proj(y) return y config = {\"n_head\": 2, \"hidden_dim\": 16, \"batch_size\": 3, \"seq_len\": 5} attn = SelfAttentionWithRoPE(config) x = torch.rand(config[\"batch_size\"], config[\"seq_len\"], config[\"hidden_dim\"]) position_ids = torch.arange(config[\"seq_len\"]) y = attn(x, position_ids) ","date":"2024-02-01","objectID":"/positional_embedding/:4:3","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"总结 RoPE的motivation： 希望相似性只依赖于向量本身和其相对位置的距离 通过对\\(Q_i,K_i\\) 施加 $R(iΘ)$变换做到 \\begin{equation*} \\begin{split} Q_{i}\u0026 = X_{i} W_{Q} R(i\\Theta) \\\\ K_{j}\u0026 = X_j W_{K} R(j\\Theta)\\\\ Q_{i}K_j^T \u0026=X_{i}W_QR(i\\Theta)R(j\\Theta)^{T}W_K^{T}X_{j}^{T}\\\\ \u0026=X_{i}W_QR(i\\Theta)R(-j\\Theta)W_K^{T}X_{j}^{T}\\\\ \u0026=X_{i}W_QR((i-j)\\Theta)W_K^{T}X_{j}^{T}\\\\ \u0026=g(X_i,X_j,i-j)\\\\ \\end{split} \\end{equation*} RoPE是什么？ 把原空间切分成为一个个正交的二维子空间，在上面做独立的旋转。 RoPE 结构 有\\(d\\) 个正交的二维子空间 \\(\\mathcal{X}_1, \\mathcal{X}_2, \\dots, \\mathcal{X}_{d}\\) 每个子空间\\(\\mathcal{X}_{k}\\) 对应一个基础角度和基础矩阵 \\(\\theta_{k}, R(\\theta_{k})\\) 对于每个位置\\(i\\), 对应一个角度序列和矩阵序列 \\(i\\Theta, R(i\\Theta)\\) \\(\\Theta\\) \\(\\theta_1\\) \\(\\theta_2\\) \\(\\theta_3\\) \\(\\ldots\\) \\(\\theta_{d}\\) \\(R(\\Theta)\\) \\(R(\\theta_1)\\) \\(R(\\theta_{2})\\) \\(R(\\theta_3)\\) \\(\\ldots\\) \\(R(\\theta_{d})\\) \\(i\\Theta\\) \\(i\\theta_1\\) \\(i\\theta_{2}\\) \\(i\\theta_3\\) \\(\\ldots\\) \\(i\\theta_{d}\\) 绝对位置编码和RoPE 有相似的结构 ","date":"2024-02-01","objectID":"/positional_embedding/:4:4","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"参考论文 Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv. https://arxiv.org/abs/2104.09864. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762. ","date":"2024-02-01","objectID":"/positional_embedding/:5:0","tags":["transformer","rope"],"title":"旋转位置编码","uri":"/positional_embedding/"},{"categories":null,"content":"关于我 你好，我是连义江，本人博士毕业于北京大学数学科学学院，前百度凤巢主任架构师。 喜欢AI和数学。闲暇喜欢弹弹电吉他。 在这里，你会看到我对技术和生活的一些思考。 感谢你的访问！ ","date":"2024-02-02","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"}]