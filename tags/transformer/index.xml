<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>transformer - Tag - 连义江的博客</title><link>https://skylyj.github.io/tags/transformer/</link><description>transformer - Tag - 连义江的博客</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 01 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://skylyj.github.io/tags/transformer/" rel="self" type="application/rss+xml"/><item><title>深入理解transformer</title><link>https://skylyj.github.io/transformer/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><author>连义江</author><guid>https://skylyj.github.io/transformer/</guid><description>#+BEGIN_line3
transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子</description></item></channel></rss>