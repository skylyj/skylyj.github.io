<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>org | 连博讲AI</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.122.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link href=/tags/org/index.xml rel=alternate type=application/rss+xml title=连博讲AI><link href=/tags/org/index.xml rel=feed type=application/rss+xml title=连博讲AI><link rel=canonical href=https://skylyj.github.io/tags/org/><meta property="og:title" content="org"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://skylyj.github.io/tags/org/"><meta itemprop=name content="org"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="org"><meta name=twitter:description content><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class="ma0 avenir bg-near-white"><header><div class="pb3-m pb6-l bg-black"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">连博讲AI</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about-transformer/ title="transformer入门 page">transformer入门</a></li></ul><div class=ananke-socials></div></div></div></nav><div class="tc-l pv3 ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">org</h1></div></div></header><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"><p>Below you will find pages that utilize the taxonomy term “org”</p></div></article><div class="mw8 center"><section class="flex-ns flex-wrap justify-around mt5"><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">连博讲AI</span><h1 class="f3 near-black"><a href=/about-transformer/ class="link black dim">transformer入门</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">transformer背景 主要内容 矩阵知识 why 矩阵和行向量 矩阵相乘和算子作用 transformer中的\(QK^{T}V\) 代码 注 encoder-decoder 低阶到高阶语义向量的转换 核心的问题 transformer背景 主要内容 参考 (引用 106576) Attention Is All You Need (引用 975) Fast Autoregressive Transformers with Linear Attention mingpt by karpathy 主要内容 transformer 的设计推演 transformer 的代码讲解 transformer的参数和运算量 linear attention 矩阵知识 why 原文直接从整个矩阵作用出发 \[\begin{aligned}\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\dfrac{QK^T}{\sqrt{d_k}})V \\ \mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\ldots,\mathrm{head}_h)W^{O} \\ \mathrm{head}_i=\mathrm{Attention}(QW_i^Q, KW^{K}_i,VW^V_i) \end{aligned}\] 从行向量的角度更容易理解 矩阵和行向量 矩阵 \(X\in R^{N\times F}\) \(X=\begin{pmatrix} X_{11}, X_{12},\ldots, X_{1F} \\ X_{21}, X_{22},\ldots, X_{2F} \\ \vdots\\ X_{N1}, X_{N2},\ldots, X_{NF} \end{pmatrix}\) 行向量 \(X_{i}=\begin{pmatrix} X_{i1}, X_{i2},\ldots, X_{iF}\end{pmatrix}, X_i \in R^{1\times F}\) 分块矩阵 \(X=\begin{pmatrix} X_1\\ X_2\\ \vdots\\ X_N \end{pmatrix}\) 例子</div><a href=/about-transformer/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://skylyj.github.io/>&copy; 连博讲AI 2024</a><div><div class=ananke-socials></div></div></div></footer></body></html>